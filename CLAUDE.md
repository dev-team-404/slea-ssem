# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project

**slea-ssem**: AI-driven learning platform for S.LSI employees. Two-round adaptive testing with RAG-based dynamic question generation, LLM auto-scoring, and ranking system.

## Tech Stack

- **Backend**: FastAPI (Python 3.11+)
- **Database**: PostgreSQL + Alembic (migrations)
- **Package Manager**: uv
- **Testing**: pytest
- **Code Quality**: ruff, black, mypy (strict), pylint

## Development

### Quick Start (4 Commands)

```bash
./tools/dev.sh up              # Start dev server (localhost:8000)
./tools/dev.sh down            # Stop dev server (free port)
./tools/dev.sh test            # Run tests
./tools/dev.sh format          # Format + lint code
```

### Common Commands

```bash
./tools/dev.sh cli             # Start interactive CLI
./tools/dev.sh shell           # Enter project shell
./tools/dev.sh help            # Show all commands

tox -e py311                   # Test on Python 3.11
tox -e style                   # Full format/lint pipeline
```

### Server Management with Custom Port

```bash
PORT=8100 ./tools/dev.sh up    # Start on port 8100
PORT=8100 ./tools/dev.sh down  # Stop port 8100 server
```

**Notes**:

- Default port: 8000 (can override with `PORT` env var)
- `down` automatically finds and kills the server on specified port
- `down` works with or without `lsof` command available

### Package Management

**When installing new packages during development**:

```bash
uv add <package-name>          # Add to [project] dependencies
uv add --dev <package-name>    # Add to dev dependencies
```

This automatically updates `pyproject.toml` and `uv.lock`. **Do NOT manually edit pyproject.toml for dependencies** â€” always use `uv add`.

## Testing

```bash
pytest                         # Run all tests
pytest -k <name>              # Run specific test
pytest -v                      # Verbose output
tox -e py310 py311 py312      # Test multiple versions
```

## Code Quality (Before Commit)

- [ ] `./tools/dev.sh format` passes (ruff, black, mypy, pylint)
- [ ] `./tools/dev.sh test` passes
- [ ] Type hints on all functions (mypy strict mode)
- [ ] Docstrings on public functions
- [ ] Line length â‰¤ 120 chars

## Git Workflow

```bash
./tools/commit.sh              # Interactive commit (Conventional Commits)
./tools/release.sh patch|minor|major  # Release & tag
```

**Commit types**: feat, fix, chore, refactor, test, docs
**Branch format**: `feature/name`, `fix/name`, `hotfix/name`

## Architecture

```text
Frontend â†” Backend (FastAPI) â†” PostgreSQL + LLM/RAG

Key Components:
- User Auth: Azure AD â†’ Session management
- Tests: 2-round adaptive (difficulty adjusts based on score)
- Scoring: MC (exact match) + Short answer (LLM-based)
- Ranking: Global ranking + percentile + category breakdown
```

### Key Files

| Path | Purpose |
|------|---------|
| `src/backend/main.py` | FastAPI app entry (TBD) |
| `src/backend/api/` | Route handlers (TBD) |
| `src/backend/models/` | SQLAlchemy models (TBD) |
| `tests/` | pytest test suite |
| `docs/` | User scenarios, setup guides |
| `alembic/` | Database migrations |

## Data Schema (MVP 1.0)

**Key Entities**:

- `users`: id, ad_id (email), nickname (UNIQUE)
- `users_profile`: user_id (FK), level, career, interests
- `test_sessions`: user_id, round (1-2), status
- `test_questions`: session_id (FK), content, difficulty_level, category
- `test_responses`: session_id, question_id, user_answer, is_correct, score
- `test_results`: user_id, grade, total_score, rank, category_breakdown

## Conventions

**Variable Naming**: snake_case (Python)
**Constants**: UPPER_SNAKE_CASE
**Functions**: verb_noun (e.g., `get_user`, `create_session`)
**Classes**: PascalCase
**Async**: Prefix with `a_` if async function

## Troubleshooting

**Type errors?** Run `tox -e mypy` (strict mode enforced)
**Import errors?** Run `tox -e ruff` then `./tools/dev.sh format`
**DB issues?** Check `alembic/versions/` for migrations, then `./tools/dev.sh up`

### Rich Console Markup Issues

**Problem**: When printing usage strings with square brackets like `[level]`, `[years]`, Rich Console interprets them as markup tags and removes them.

```python
# âŒ Wrong: Will output "Usage: cmd [--option]" (brackets removed)
console.print("Usage: cmd [level] [years] [--option VALUE]")

# âœ… Correct: Use markup=False parameter
console.print("Usage: cmd [level] [years] [--option VALUE]", markup=False)

# âœ… Also correct: Escape with double brackets
console.print("Usage: cmd [[level]] [[years]] [--option VALUE]")
```

**When to use**:

- Use `markup=False` when printing usage/help text with square brackets (cleaner)
- Use `[[...]]` when you need markup enabled elsewhere but want literal brackets

**Cache after changes**: Run `./tools/dev.sh clean` before testing CLI changes, as Python caches compiled modules.

## Further Reading

- **User Scenarios**: `docs/user_scenarios_mvp1.md`
- **Setup Template**: `docs/PROJECT_SETUP_PROMPT.md`
- **Contributing**: See git flow above + code quality rules

---

## REQ-Based Development Workflow (for MVP 1.0)

**When to use**: Each user request follows format: `"REQ-X-Y ê¸°ëŠ¥ êµ¬í˜„í•´"` (implement REQ X-Y)

### Command Format

```
User: "REQ-B-A2-Edit-1 ê¸°ëŠ¥ êµ¬í˜„í•´"
Assistant: (Automatically follows 4-phase workflow below)
```

### Phase 1ï¸âƒ£: SPECIFICATION (Parse & Pause for Review)

```
- Extract REQ ID, ìš”êµ¬ì‚¬í•­, ìš°ì„ ìˆœìœ„, Acceptance Criteria from feature_requirement_mvp1.md
- Summarize: intent, constraints, performance goals
- Define: Location (module path), Signature (types, I/O, side effects),
  Behavior (logic, validation), Dependencies, Non-functional (perf/security)
- ğŸ›‘ PAUSE: Present spec, ask "Approved? Continue to Phase 2?"
```

### Phase 2ï¸âƒ£: TEST DESIGN (TDD Before Code)

```
- Create: tests/<domain>/test_<feature>.py
- Design 4-5 test cases:
  âœ“ Happy path (valid inputs)
  âœ“ Input validation errors
  âœ“ Edge cases (DB, timeout, concurrency)
  âœ“ Acceptance criteria verification
- Include REQ ID in docstrings: # REQ: REQ-X-Y-Edit-1
- ğŸ›‘ PAUSE: Present test list, ask "Tests approved? Continue to Phase 3?"
```

### Phase 3ï¸âƒ£: IMPLEMENTATION (Code to Spec)

```
- Write minimal code satisfying spec + tests
- Follow SOLID + conventions from above
- Run: tox -e style && pytest tests/<domain>/test_<feature>.py
- ğŸ›‘ STOP if validation fails; report errors
```

### Phase 4ï¸âƒ£: SUMMARY (Report & Commit)

```
- Modified files + rationale
- Test results (all pass)
- Traceability: REQ â†’ Spec â†’ Tests â†’ Code
- **Create progress file**: docs/progress/REQ-X-Y.md with full Phase 1-4 documentation
  * Include: Requirements, Implementation locations, Test results, Git commit
  * Add REQ traceability table (implementation â†” test coverage)
- **Update progress tracking**: docs/DEV-PROGRESS.md
  * Find REQ row in developer section
  * Change Phase: 0 â†’ 4
  * Change Status: â³ Backlog â†’ âœ… Done
  * Update Notes: Add commit SHA (e.g., "Commit: f5412e9")
- Create git commit:
  * Format: "chore: Update progress tracking for REQ-X-Y completion"
  * Include: progress file creation + DEV-PROGRESS.md update
  * Tag with ğŸ¤– Claude Code marker
```

**Key Principle**: Phase 1-2 pause for review = prevent rework. Spec must be approved before coding.
**Progress Tracking**: Always complete Phase 4 progress files to maintain audit trail & team visibility.

---

## CLI Feature Requirement Workflow

**When to use**: CLI ê¸°ëŠ¥ ì¶”ê°€/ê°œì„ í•  ë•Œ, ëª¨ë“  featureëŠ” requirement ë¨¼ì € ì •ì˜

### Step 1: Requirement ì‘ì„±

**íŒŒì¼**: `docs/CLI-FEATURE-REQUIREMENTS.md`ì— ë‹¤ìŒ í¬ë§·ìœ¼ë¡œ ì¶”ê°€

**í¬ë§·**: `REQ-CLI-[DOMAIN]-[NUMBER]`

- Domain: auth, survey, profile, questions, session, export, ...
- Number: ë„ë©”ì¸ ë‚´ ìˆœë²ˆ (1, 2, 3, ...)

**í…œí”Œë¦¿**:

```markdown
### REQ-CLI-[DOMAIN]-[NUMBER]: [ê¸°ëŠ¥ëª…]

**Description**:
í•œ ë¬¸ì¥ ìš”ì•½

ìƒì„¸ ì„¤ëª… (2-3ì¤„)

**ì‚¬ìš© ì˜ˆ**:
```bash
> command [args]
âœ“ success output
```

**ê¸°ëŒ€ ì¶œë ¥**:

- ì„±ê³µ: ë©”ì‹œì§€ + ë°ì´í„°
- ì‹¤íŒ¨: ì—ëŸ¬ ë©”ì‹œì§€

**ì—ëŸ¬ ì¼€ì´ìŠ¤**:

- Not authenticated â†’ "Please login first"
- API error â†’ ìƒì„¸ ì—ëŸ¬ ë©”ì‹œì§€
- Invalid input â†’ Usage ê°€ì´ë“œ

**Acceptance Criteria**:

- [ ] ëª…ë ¹ì–´ ì •í™• ì‘ë™
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ì™„ë²½
- [ ] ë„ì›€ë§ ëª…í™•

**Priority**: M/H/L
**Dependencies**: [API / Module]
**Status**: â³ Backlog

```

### Step 2-5: REQ-Based Workflow ì ìš©

**Phase 1-4ëŠ” ê¸°ì¡´ CLAUDE.mdì˜ REQ-Based Development Workflowê³¼ ë™ì¼**

```

REQ-CLI-AUTH-1 ê¸°ëŠ¥ êµ¬í˜„í•´
â†“
Phase 1: ìš”êµ¬ì‚¬í•­ ê²€í†  â†’ Approve?
Phase 2: í…ŒìŠ¤íŠ¸ ì„¤ê³„ â†’ Approve?
Phase 3: êµ¬í˜„ + ê²€ì¦
Phase 4: Commit + Progress tracking

```

### CLI Feature ë°œê²¬ ë° ì¶”ê°€ ë°©ë²•

**ë°©ë²•: ì¦‰ì‹œ ì¶”ê°€ (ê¶Œì¥)**

```

ì‚¬ìš©ì: "CLIì— ì„¸ì…˜ ì €ì¥ ê¸°ëŠ¥ì´ í•„ìš”í•´"
â†“

1. docs/CLI-FEATURE-REQUIREMENTS.mdì— REQ-CLI-SESSION-1 ì¶”ê°€
2. Requirement ì •ì˜ (5ë¶„)
3. "REQ-CLI-SESSION-1 ê¸°ëŠ¥ êµ¬í˜„í•´" ìš”ì²­
4. Claudeê°€ 4ë‹¨ê³„ Workflow ì ìš©

```

### CLI Requirement ê´€ë¦¬

**ì¡°ì§**:
- `docs/CLI-FEATURE-REQUIREMENTS.md`: ëª¨ë“  CLI ìš”êµ¬ì‚¬í•­ í†µí•© ê´€ë¦¬
- `docs/DEV-PROGRESS.md`: CLI ì„¹ì…˜ì— ì§„í–‰ ìƒí™© ì¶”ì 
- `docs/progress/REQ-CLI-*.md`: ê° ê¸°ëŠ¥ë³„ Phase 4 documentation

**ì§„í–‰ ì¶”ì **:
- Phase 4 ì™„ë£Œ ì‹œ:
  1. `docs/CLI-FEATURE-REQUIREMENTS.md`ì—ì„œ Status: âœ… Done ë³€ê²½
  2. `docs/DEV-PROGRESS.md`ì˜ CLI ì„¹ì…˜ì—ì„œ Phase 4ë¡œ ë³€ê²½
  3. Commit SHA ê¸°ë¡

---

**Forcing Function Principle**: 3-4 intuitive commands (dev.sh, commit.sh, tox) reduce learning curve & execution variance. See `docs/PROJECT_SETUP_PROMPT.md` for details.

---

## ğŸ¯ CURRENT STATUS & NEXT TASKS

### âœ… Completed Work (Session: DB Persistence Fix + Answer Schema Population)

**Phase 1: DB Persistence Fix (STEP 1)**
- âœ… Root Cause: LLM max_tokens=1024 was insufficient, agent output truncated at MAX_TOKENS
- âœ… Solution: Increased max_tokens=4096 in src/agent/config.py:31
- âœ… Fix Code Indentation: Fixed for loop indentation in llm_agent.py:933-999
- âœ… Initialize Variables: agent_steps initialized early to prevent unbound errors
- âœ… Commit b9c1ee5: "fix: STEP 1 - Fix DB persistence by increasing LLM max_tokens and fixing agent output parsing"
- âœ… Test Result: agent generate-questions --domain AI â†’ items generated: 1 âœ…

**Phase 2: Answer Schema Population (Option A)**
- âœ… Problem: Answer Schema empty in CLI despite DB save success
- âœ… Root Cause: Tool 5 returned nested answer_schema, Agent needed flattened format
- âœ… Solution: Tool 5 flattens answer_schema fields (correct_keyâ†’correct_answer, etc.)
- âœ… Enhanced Prompt: Agent Prompt instructs Agent to include Tool 5 fields in Final Answer JSON
- âœ… Improved Parsing: llm_agent.py logs answer_schema population success
- âœ… Commit 44620ad: "fix: Option A - Improve Tool 5 return format and Agent Prompt for proper Answer Schema population"
- âœ… Test Result: Answer Schema now fully populated with correct_answer + correct_keywords

**Key Files Modified**:
- src/agent/config.py (max_tokens increase)
- src/agent/llm_agent.py (indentation fix, variable init, enhanced logging)
- src/agent/tools/save_question_tool.py (flattened response format)
- src/agent/prompts/react_prompt.py (enhanced instructions)

### â³ Pending: STEP 2 (Structured Format Refactoring - 1ë²ˆ ë°©ì‹)

**STEP 2 Objective**: Refactor agent output from ReAct text format to LangGraph intermediate_steps structure
- Create converter class: AgentOutputConverter
- Convert ReAct format â†’ intermediate_steps format (tool_calls + ToolMessage pairs)
- Implement SOLID principles (Single Responsibility, Dependency Inversion)
- Target: Proper structured format for downstream consumption

**Files to Modify**:
- src/agent/llm_agent.py: Add AgentOutputConverter class + refactored _parse_agent_output_generate
- src/agent/tools/*.py: May need minor adjustments for structured format

---

**Next High-Priority Tasks** (~2.5 hours total):

### Task 1: REQ-A-Agent-Backend-1 (Mock â†’ Real Agent í†µí•©) â­ HIGH PRIORITY
- **File**: `src/backend/services/question_gen_service.py` (ìˆ˜ì •)
- **Objective**: QuestionGenerationServiceê°€ Mock ëŒ€ì‹  Real Agent í˜¸ì¶œ
- **Duration**: ~1.5ì‹œê°„
- **What to do**:
  1. generate_questions() ë©”ì„œë“œë¥¼ asyncë¡œ ë³€ê²½
  2. create_agent() í˜¸ì¶œ ì¶”ê°€
  3. GenerateQuestionsRequest ìƒì„± ë° ì „ë‹¬
  4. ì´ì „ ë¼ìš´ë“œ ë‹µë³€ (prev_answers) ì¡°íšŒ
  5. Agent ì‘ë‹µì„ DBì— ì €ì¥
- **Acceptance**: Phase 1-4 documentation + ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼
- **Test Location**: `tests/backend/test_question_gen_service_agent.py`
- **Spec Location**: `docs/AGENT-TEST-SCENARIO.md` lines 471-555

### Task 2 (Optional): REQ-A-Agent-Backend-2 (ScoringService í†µí•©)
- **File**: `src/backend/services/scoring_service.py`
- **Objective**: ScoringServiceê°€ Tool 6 í˜¸ì¶œ
- **Duration**: ~1ì‹œê°„ (ì„ íƒì‚¬í•­)
- **Spec Location**: `docs/AGENT-TEST-SCENARIO.md` lines 517-555

---

## ğŸ“š Documentation References (Already Exist)
**Do NOT regenerate these** - they are already complete:
- `docs/TOOL_DEFINITIONS_SUMMARY.md` - Complete tool signatures & details
- `docs/TOOL_QUICK_REFERENCE.md` - Quick examples & validation rules
- `docs/TOOL_DOCUMENTATION_INDEX.md` - Navigation & troubleshooting
- `docs/AGENT-TEST-SCENARIO.md` - Full phase planning (REQ-A-Agent-*)

**Just reference them when implementing!**

---

## ğŸš€ Quick Start After Context Gap

1. Read this section first (2 min)
2. Run: `git log --oneline -10` to see recent commits
3. Start with Task 1 (REQ-A-Agent-Backend-1) in `docs/AGENT-TEST-SCENARIO.md` lines 471-555
4. Use TOOL documentation (don't regenerate - it already exists)
5. Create progress file in `docs/progress/REQ-A-Agent-Backend-1.md` after Phase 4
