"""
Question generation service for generating test questions.

REQ: REQ-B-B2-Gen-1, REQ-B-B2-Gen-2, REQ-B-B2-Gen-3
REQ: REQ-A-Agent-Backend-1 (Real Agent Integration)

Implementation: Async service with Real Agent integration for LLM-based question generation.
"""

import logging
from typing import Any
from uuid import uuid4

from sqlalchemy.orm import Session

from src.agent.llm_agent import GenerateQuestionsRequest, create_agent
from src.backend.models.question import Question
from src.backend.models.test_result import TestResult
from src.backend.models.test_session import TestSession
from src.backend.models.user_profile import UserProfileSurvey
from src.backend.services.adaptive_difficulty_service import AdaptiveDifficultyService

logger = logging.getLogger(__name__)


class QuestionGenerationService:
    """
    Service for generating test questions with Real Agent integration.

    REQ: REQ-B-B2-Gen-1, REQ-B-B2-Gen-2, REQ-B-B2-Gen-3
    REQ: REQ-A-Agent-Backend-1 (Real Agent Integration)

    Implementation:
        - generate_questions: Async method using ItemGenAgent (Google Gemini LLM)
        - generate_questions_adaptive: Round 2+ with adaptive difficulty

    Note: MOCK_QUESTIONS kept for fallback/testing purposes only.
    """

    # Mock question templates for different categories and difficulty levels
    MOCK_QUESTIONS = {
        "LLM": [
            {
                "item_type": "multiple_choice",
                "stem": "LLM(Large Language Model)ì˜ í•µì‹¬ íŠ¹ì„±ìœ¼ë¡œ ê°€ìž¥ ì ì ˆí•œ ê²ƒì€?",
                "choices": [
                    "A: ìž‘ì€ í¬ê¸°ì˜ ì‹ ê²½ë§ìœ¼ë¡œ ë‹¨ìˆœí•œ íŒ¨í„´ë§Œ ì¸ì‹ ê°€ëŠ¥",
                    "B: ìˆ˜ì‹­ì–µ ê°œ ì´ìƒì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ì‹ ê²½ë§ ëª¨ë¸",
                    "C: ìŒì„± ì¸ì‹ë§Œ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ëª¨ë¸",
                    "D: ê·œì¹™ ê¸°ë°˜ì˜ ê³ ì „ì  ìžì—°ì–´ì²˜ë¦¬ ì—”ì§„",
                ],
                "answer_schema": {
                    "correct_key": "B",
                    "explanation": "LLMì€ ìˆ˜ì‹­ì–µì—ì„œ ì¡°ì¡° ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ëŒ€ê·œëª¨ ì‹ ê²½ë§ìœ¼ë¡œ, ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œë¶€í„° ì–¸ì–´ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 4,
            },
            {
                "item_type": "true_false",
                "stem": "íŠ¸ëžœìŠ¤í¬ë¨¸(Transformer)ëŠ” LLMì˜ ê¸°ì´ˆ ì•„í‚¤í…ì²˜ì´ë‹¤.",
                "answer_schema": {
                    "correct_key": "true",
                    "explanation": "íŠ¸ëžœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° í˜„ëŒ€ LLMì˜ í‘œì¤€ ê¸°ì´ˆìž…ë‹ˆë‹¤.",
                },
                "difficulty": 5,
            },
            {
                "item_type": "short_answer",
                "stem": "LLMì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ê¸°ë²• ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤. (í† í° ì˜ˆì¸¡, ê°•í™”í•™ìŠµ ë“±)",
                "answer_schema": {
                    "keywords": ["í† í° ì˜ˆì¸¡", "ê°•í™”í•™ìŠµ", "ìžê¸°ê°ë…í•™ìŠµ", "ì‚¬ì „í•™ìŠµ"],
                    "explanation": "LLMì€ ì£¼ë¡œ ë‹¤ìŒ í† í° ì˜ˆì¸¡(Next Token Prediction)ì„ í†µí•œ ìžê¸°ê°ë…í•™ìŠµìœ¼ë¡œ í›ˆë ¨ë©ë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 6,
            },
            {
                "item_type": "multiple_choice",
                "stem": "ChatGPTì™€ ê°™ì€ ëŒ€í™”í˜• LLMì„ ë§Œë“¤ê¸° ìœ„í•´ í•„ìˆ˜ì ì¸ ê³¼ì •ì€?",
                "choices": [
                    "A: ì‚¬ì „í•™ìŠµ(Pre-training)ë§Œìœ¼ë¡œ ì¶©ë¶„",
                    "B: ì§€ì‹œì–´ íŠœë‹(Instruction Tuning)ê³¼ ê°•í™”í•™ìŠµ(RLHF)",
                    "C: ë‹¨ìˆœ ë°ì´í„° ì¦ê°•(Data Augmentation)ë§Œìœ¼ë¡œ ì¶©ë¶„",
                    "D: ì‚¬ìš©ìž í”¼ë“œë°±ì´ ì—†ì–´ë„ ê°€ëŠ¥",
                ],
                "answer_schema": {
                    "correct_key": "B",
                    "explanation": "ëŒ€í™”í˜• LLMì€ ì‚¬ì „í•™ìŠµ í›„ ì§€ì‹œì–´ íŠœë‹ê³¼ ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ(RLHF)ì„ í†µí•´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 7,
            },
            {
                "item_type": "multiple_choice",
                "stem": "LLMì˜ ê°€ìž¥ í° ì œí•œì‚¬í•­ì€?",
                "choices": [
                    "A: ì†ë„ê°€ ë§¤ìš° ë¹ ë¦„",
                    "B: í• ë£¨ì‹œë„¤ì´ì…˜(ê±°ì§“ ì •ë³´ ìƒì„±) ë°œìƒ ê°€ëŠ¥",
                    "C: í•™ìŠµ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ŽìŒ",
                    "D: íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ",
                ],
                "answer_schema": {
                    "correct_key": "B",
                    "explanation": "LLMì€ í†µê³„ì  íŒ¨í„´ í•™ìŠµì—ë§Œ ì˜ì¡´í•˜ì—¬ ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ì •ë³´ë¥¼ ë§ˆì¹˜ ì‚¬ì‹¤ì¸ ê²ƒì²˜ëŸ¼ ìƒì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 6,
            },
        ],
        "RAG": [
            {
                "item_type": "multiple_choice",
                "stem": "RAG(Retrieval-Augmented Generation)ì˜ í•µì‹¬ ëª©í‘œëŠ”?",
                "choices": [
                    "A: ëª¨ë“  ë‹µë³€ì„ ì‚¬ìš©ìž ì •ì˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰",
                    "B: LLMì— ì™¸ë¶€ ì§€ì‹ì„ ë™ì ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ ë‹µë³€ ì •í™•ë„ í–¥ìƒ",
                    "C: ë„¤íŠ¸ì›Œí¬ ì†ë„ ê°œì„ ",
                    "D: ëª¨ë¸ íŒŒë¼ë¯¸í„° ê°ì†Œ",
                ],
                "answer_schema": {
                    "correct_key": "B",
                    "explanation": "RAGëŠ” ì‚¬ìš©ìž ì •ì˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì— ì œê³µí•¨ìœ¼ë¡œì¨ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 5,
            },
            {
                "item_type": "true_false",
                "stem": "RAG ì‹œìŠ¤í…œì—ì„œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” í•„ìˆ˜ ìš”ì†Œì´ë‹¤.",
                "answer_schema": {
                    "correct_key": "true",
                    "explanation": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ìŠµë‹ˆë‹¤.",
                },
                "difficulty": 5,
            },
            {
                "item_type": "short_answer",
                "stem": "RAGì˜ ë‘ ê°€ì§€ ì£¼ìš” ë‹¨ê³„ë¥¼ ìž‘ì„±í•˜ì‹œì˜¤.",
                "answer_schema": {
                    "keywords": ["ê²€ìƒ‰", "ìƒì„±", "Retrieval", "Generation"],
                    "explanation": "RAGëŠ” (1) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë‹¨ê³„ì™€ (2) ê²€ìƒ‰ëœ ì •ë³´ë¥¼ í™œìš©í•œ ìƒì„± ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.",
                },
                "difficulty": 4,
            },
            {
                "item_type": "multiple_choice",
                "stem": "RAGì—ì„œ ì‚¬ìš©í•˜ëŠ” ìž„ë² ë”© ëª¨ë¸ì˜ ì—­í• ì€?",
                "choices": [
                    "A: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜",
                    "B: ëª¨ë¸ íŒŒë¼ë¯¸í„° ì••ì¶•",
                    "C: ë°ì´í„°ë² ì´ìŠ¤ ì•”í˜¸í™”",
                    "D: API ì‘ë‹µ ì†ë„ í–¥ìƒ",
                ],
                "answer_schema": {
                    "correct_key": "A",
                    "explanation": "ìž„ë² ë”© ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.",
                },
                "difficulty": 6,
            },
            {
                "item_type": "multiple_choice",
                "stem": "RAG ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë°©ë²•ì€?",
                "choices": [
                    "A: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì¦ê°€",
                    "B: ë¬¸ì„œ ì²­í‚¹(Chunking)ê³¼ ë©”íƒ€ë°ì´í„° í™œìš©",
                    "C: ë„¤íŠ¸ì›Œí¬ ì†ë„ ì¦ê°€",
                    "D: ë” í° LLM ëª¨ë¸ ì‚¬ìš©",
                ],
                "answer_schema": {
                    "correct_key": "B",
                    "explanation": "íš¨ê³¼ì ì¸ ë¬¸ì„œ ì²­í‚¹ê³¼ ë©”íƒ€ë°ì´í„° í™œìš©ìœ¼ë¡œ ê²€ìƒ‰ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.",
                },
                "difficulty": 7,
            },
        ],
        "Robotics": [
            {
                "item_type": "multiple_choice",
                "stem": "ë¡œë´‡ ìžë™í™”ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ”?",
                "choices": [
                    "A: ì¦‰ì‹œ ì™„ì „ ìžë™í™” ë„ìž…",
                    "B: í˜„ìž¬ í”„ë¡œì„¸ìŠ¤ ë¶„ì„ ë° ê°œì„ ì  íŒŒì•…",
                    "C: ìµœì‹  ë¡œë´‡ ê¸°ìˆ  êµ¬ë§¤",
                    "D: ìž‘ì—…ìž ìž¬êµìœ¡",
                ],
                "answer_schema": {
                    "correct_key": "B",
                    "explanation": "ë¡œë´‡ ìžë™í™”ëŠ” í˜„ìž¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ê¹Šì´ ìžˆê²Œ ë¶„ì„í•˜ê³  ê°œì„ ì ì„ íŒŒì•…í•œ í›„ ì§„í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 4,
            },
            {
                "item_type": "true_false",
                "stem": "ë¡œë´‡ì€ ëª¨ë“  ì¢…ë¥˜ì˜ ìž‘ì—…ì„ ì¸ê°„ë³´ë‹¤ ë” ìž˜ ìˆ˜í–‰í•  ìˆ˜ ìžˆë‹¤.",
                "answer_schema": {
                    "correct_key": "false",
                    "explanation": "ë¡œë´‡ì€ ë°˜ë³µì ì´ê³  ì •í•´ì§„ ìž‘ì—…ì—ì„œ ìš°ìˆ˜í•˜ì§€ë§Œ, ì°½ì˜ì„±ê³¼ ì ì‘ë ¥ì´ í•„ìš”í•œ ìž‘ì—…ì—ì„œëŠ” ì¸ê°„ì´ ë” íš¨ê³¼ì ìž…ë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 3,
            },
            {
                "item_type": "short_answer",
                "stem": "ë¡œë´‡ê³µí•™ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ì„¼ì„œ 3ê°€ì§€ë¥¼ ë‚˜ì—´í•˜ì‹œì˜¤.",
                "answer_schema": {
                    "keywords": ["ì¹´ë©”ë¼", "ë¼ì´ë”", "ê°€ì†ë„ê³„", "ìžì´ë¡œìŠ¤ì½”í”„", "ê·¼ì ‘ ì„¼ì„œ"],
                    "explanation": "ë¡œë´‡ì€ ì‹œê°(ì¹´ë©”ë¼), ê±°ë¦¬ ì¸¡ì •(ë¼ì´ë”), ì›€ì§ìž„ ê°ì§€(ê°€ì†ë„ê³„/ìžì´ë¡œìŠ¤ì½”í”„) ë“± ë‹¤ì–‘í•œ ì„¼ì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 5,
            },
            {
                "item_type": "multiple_choice",
                "stem": "í˜‘ë™ ë¡œë´‡(Cobot)ì˜ ì£¼ìš” íŠ¹ì§•ì€?",
                "choices": [
                    "A: ë§¤ìš° ë¹ ë¥¸ ìž‘ë™ ì†ë„",
                    "B: ì¸ê°„ê³¼ í•¨ê»˜ ìž‘ì—…í•  ìˆ˜ ìžˆëŠ” ì•ˆì „ ê¸°ëŠ¥",
                    "C: ë§¤ìš° ë³µìž¡í•œ ì œì–´ ì‹œìŠ¤í…œ",
                    "D: ì¼íšŒìš© ë¡œë´‡",
                ],
                "answer_schema": {
                    "correct_key": "B",
                    "explanation": "í˜‘ë™ ë¡œë´‡ì€ ì¸ê°„ ê·¼ì²˜ì—ì„œ ì•ˆì „í•˜ê²Œ ìž‘ì—…í•  ìˆ˜ ìžˆë„ë¡ ì„¤ê³„ëœ ë¡œë´‡ìž…ë‹ˆë‹¤.",
                },
                "difficulty": 5,
            },
            {
                "item_type": "multiple_choice",
                "stem": "ë¡œë´‡ ë¹„ì „(Robot Vision) ì‹œìŠ¤í…œì´ ê°œì„ ë˜ë©´ì„œ ê°€ëŠ¥í•´ì§„ ì‘ìš© ë¶„ì•¼ëŠ”?",
                "choices": [
                    "A: ìŒì•… ìž‘ê³¡ë§Œ ê°€ëŠ¥",
                    "B: ì •ë°€í•œ í’ˆì§ˆ ê²€ì‚¬, ë¶€í’ˆ ë¶„ë¥˜, ë³µìž¡í•œ ì¡°ë¦½ ìž‘ì—…",
                    "C: í…ìŠ¤íŠ¸ ì½ê¸°ë§Œ ê°€ëŠ¥",
                    "D: ê³„ì‚°ë§Œ ê°€ëŠ¥",
                ],
                "answer_schema": {
                    "correct_key": "B",
                    "explanation": "í–¥ìƒëœ ë¨¸ì‹  ë¹„ì „ ê¸°ìˆ ë¡œ ë¡œë´‡ì´ ë³µìž¡í•œ ì‹œê°ì  ìž‘ì—…ì„ ì •í™•í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìžˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.",  # noqa: E501
                },
                "difficulty": 6,
            },
        ],
    }

    def __init__(self, session: Session) -> None:
        """
        Initialize QuestionGenerationService with database session.

        Args:
            session: SQLAlchemy database session

        """
        self.session = session

    async def generate_questions(
        self,
        user_id: int,
        survey_id: str,
        round_num: int = 1,
        question_count: int = 5,
        question_types: list[str] | None = None,
        domain: str = "AI",
    ) -> dict[str, Any]:
        """
        Generate questions using Real Agent (async).

        REQ: REQ-B-B2-Gen-1, REQ-B-B2-Gen-2, REQ-B-B2-Gen-3
        REQ: REQ-A-Agent-Backend-1 (Real Agent Integration)

        Workflow:
        1. Validate survey exists and retrieve context
        2. Create TestSession with in_progress status
        3. Retrieve previous round answers (for adaptive difficulty)
        4. Call Real Agent (ItemGenAgent) via GenerateQuestionsRequest
        5. Save generated items to DB as Question records
        6. Return backwards-compatible dict response

        Args:
            user_id: User ID
            survey_id: UserProfileSurvey ID to get interests
            round_num: Round number (1 or 2, default 1)
            question_count: Number of questions to generate (default 5, test 2)
            question_types: List of question types to generate (e.g., ["multiple_choice"])
            domain: Question domain/topic (e.g., "AI", "food", default "AI")

        Returns:
            Dictionary with:
                - session_id (str): TestSession UUID
                - questions (list): List of question dictionaries with all fields

        Raises:
            Exception: If survey not found

        """
        try:
            # Step 1: Validate survey and get context
            survey = self.session.query(UserProfileSurvey).filter_by(user_id=user_id, id=survey_id).first()
            if not survey:
                raise Exception(f"Survey with id {survey_id} not found for user {user_id}.")

            logger.debug(f"âœ“ Survey found: interests={survey.interests}")

            # Step 2: Create TestSession
            session_id = str(uuid4())
            test_session = TestSession(
                id=session_id,
                user_id=user_id,
                survey_id=survey_id,
                round=round_num,
                status="in_progress",
            )
            self.session.add(test_session)
            self.session.commit()
            logger.debug(f"âœ“ TestSession created: session_id={session_id}")

            # Step 3: Retrieve previous answers (for adaptive difficulty)
            prev_answers = None
            if round_num > 1:
                prev_answers = self._get_previous_answers(user_id, round_num - 1)
                logger.debug(f"âœ“ Previous answers retrieved: count={len(prev_answers) if prev_answers else 0}")

            # Step 4: Call Real Agent
            logger.debug("ðŸ“¡ Creating Agent and calling generate_questions...")
            agent = await create_agent()
            logger.debug("âœ“ Agent created successfully")

            agent_request = GenerateQuestionsRequest(
                session_id=session_id,
                survey_id=survey_id,
                round_idx=round_num,
                prev_answers=prev_answers,
                question_count=question_count,
                question_types=question_types,
                domain=domain,
            )
            logger.debug(
                f"âœ“ GenerateQuestionsRequest created: session_id={session_id}, count={question_count}, types={question_types}"
            )

            agent_response = await agent.generate_questions(agent_request)
            logger.debug(
                f"Agent response received: {len(agent_response.items)} items, tokens={agent_response.total_tokens}"
            )

            # Step 5: Save generated items to DB
            questions_list = []
            for item in agent_response.items:
                # Handle both Pydantic model and dict for answer_schema
                answer_schema_value = (
                    item.answer_schema.model_dump() if hasattr(item.answer_schema, "model_dump") else item.answer_schema
                )
                question = Question(
                    id=item.id,
                    session_id=session_id,
                    item_type=item.type,
                    stem=item.stem,
                    choices=item.choices,
                    answer_schema=answer_schema_value,
                    difficulty=item.difficulty,
                    category=item.category,
                    round=round_num,
                )
                self.session.add(question)
                questions_list.append(question)
                logger.debug(f"Saved question: id={item.id}, type={item.type}")

            self.session.commit()

            # Step 6: Format and return response (backwards compatible dict format)
            response = {
                "session_id": session_id,
                "questions": [
                    {
                        "id": q.id,
                        "item_type": q.item_type,
                        "stem": q.stem,
                        "choices": q.choices,
                        "answer_schema": q.answer_schema,
                        "difficulty": q.difficulty,
                        "category": q.category,
                    }
                    for q in questions_list
                ],
            }
            logger.info(f"âœ… Generated {len(questions_list)} questions (tokens: {agent_response.total_tokens})")
            return response

        except Exception as e:
            logger.error(f"âŒ Question generation failed: {e}")
            # Return error response with empty questions (graceful degradation)
            return {
                "session_id": f"error_{uuid4().hex[:8]}",
                "questions": [],
                "error": str(e),
            }

    def _get_previous_answers(self, user_id: int, round_num: int) -> list[dict] | None:
        """
        Retrieve previous round answers for adaptive difficulty.

        REQ: REQ-A-Agent-Backend-1 (Context for Agent)

        Args:
            user_id: User ID
            round_num: Round number to retrieve answers from

        Returns:
            List of previous answers dict or None if not found

        """
        try:
            # Get the latest TestResult for the round
            test_result = (
                self.session.query(TestResult)
                .join(TestSession, TestSession.id == TestResult.session_id)
                .filter(TestSession.user_id == user_id, TestResult.round == round_num)
                .order_by(TestResult.created_at.desc())
                .first()
            )

            if not test_result:
                logger.debug(f"No previous answers found for round {round_num}")
                return None

            # Get questions and answers from the previous session
            prev_session = self.session.query(TestSession).filter_by(id=test_result.session_id).first()
            if not prev_session:
                return None

            prev_questions = self.session.query(Question).filter_by(session_id=prev_session.id).all()
            if not prev_questions:
                return None

            # Build prev_answers list from session data
            # This will be used by Agent for context
            prev_answers = [
                {
                    "question_id": q.id,
                    "category": q.category,
                    "difficulty": q.difficulty,
                    "item_type": q.type,
                }
                for q in prev_questions
            ]

            logger.debug(f"âœ“ Retrieved {len(prev_answers)} previous answers for adaptive context")
            return prev_answers

        except Exception as e:
            logger.warning(f"Failed to retrieve previous answers: {e}")
            return None

    def generate_questions_adaptive(
        self,
        user_id: int,
        session_id: str,
        round_num: int = 2,
    ) -> dict[str, Any]:
        """
        Generate Round 2+ questions with adaptive difficulty.

        REQ: REQ-B-B2-Adapt-1, REQ-B-B2-Adapt-2, REQ-B-B2-Adapt-3

        Analyzes previous round results and generates questions with:
        - Adjusted difficulty based on score
        - Prioritized weak categories (â‰¥50%)

        Args:
            user_id: User ID
            session_id: Previous TestSession ID (Round N-1)
            round_num: Target round number (2, 3, etc.)

        Returns:
            Dictionary with:
                - session_id (str): New TestSession UUID for this round
                - questions (list): List of 5 adaptive questions
                - adaptive_params (dict): Difficulty tier, weak categories

        Raises:
            ValueError: If previous round results not found

        """
        # Get previous round result
        prev_round = round_num - 1
        prev_result = (
            self.session.query(TestResult)
            .join(TestSession, TestSession.id == TestResult.session_id)
            .filter(TestSession.user_id == user_id, TestResult.round == prev_round)
            .order_by(TestResult.created_at.desc())
            .first()
        )

        if not prev_result:
            raise ValueError(
                f"Round {prev_round} results not found for user {user_id}. "
                "Cannot generate adaptive Round {round_num}."
            )

        # Get adaptive difficulty parameters
        adaptive_service = AdaptiveDifficultyService(self.session)
        params = adaptive_service.get_adaptive_generation_params(prev_result.session_id)

        # Get the survey_id from the previous session
        prev_session = self.session.query(TestSession).filter_by(id=prev_result.session_id).first()
        if not prev_session:
            raise ValueError(f"Previous TestSession {prev_result.session_id} not found")

        # Create new test session for this round
        new_session_id = str(uuid4())
        test_session = TestSession(
            id=new_session_id,
            user_id=user_id,
            survey_id=prev_session.survey_id,  # Use same survey from previous session
            round=round_num,
            status="in_progress",
        )
        self.session.add(test_session)
        self.session.commit()

        # Get weak categories to prioritize
        priority_ratio = params["priority_ratio"]
        adjusted_difficulty = params["adjusted_difficulty"]

        questions_list = []

        # Get all available categories
        all_categories = list(self.MOCK_QUESTIONS.keys())

        # Build question selection strategy
        questions_allocated = {}

        # Step 1: Allocate from weak categories (if any)
        weak_total = 0
        for weak_cat in priority_ratio:
            count = int(priority_ratio[weak_cat])
            questions_allocated[weak_cat] = count
            weak_total += count

        # Step 2: Fill remaining slots with other categories (from all available)
        remaining_needed = 5 - weak_total
        other_categories = [c for c in all_categories if c not in questions_allocated]

        if remaining_needed > 0:
            if other_categories:
                # Distribute remaining among other categories fairly
                per_category = remaining_needed // len(other_categories)
                remainder = remaining_needed % len(other_categories)

                for idx, cat in enumerate(other_categories):
                    count = per_category + (1 if idx < remainder else 0)
                    if count > 0:
                        questions_allocated[cat] = count
            else:
                # No other categories, add back to weak categories
                # Distribute remaining among weak categories
                weak_cats = list(questions_allocated.keys())
                if weak_cats:
                    per_category = remaining_needed // len(weak_cats)
                    remainder = remaining_needed % len(weak_cats)
                    for idx, cat in enumerate(weak_cats):
                        questions_allocated[cat] += per_category + (1 if idx < remainder else 0)

        # Step 3: Select questions from each category
        question_idx = 0
        for category, count in questions_allocated.items():
            mock_questions = self.MOCK_QUESTIONS.get(category, [])

            for _ in range(count):
                if not mock_questions:
                    continue

                # Select question (use index to vary)
                mock_q = mock_questions[question_idx % len(mock_questions)]
                question_idx += 1

                # Adjust difficulty based on adaptive parameters
                # Round difficulty to nearest integer for mock data
                adjusted_diff_int = int(round(adjusted_difficulty))
                # Clamp to 1-10
                final_difficulty = max(1, min(10, adjusted_diff_int))

                # Create Question record
                question = Question(
                    id=str(uuid4()),
                    session_id=new_session_id,
                    item_type=mock_q["item_type"],
                    stem=mock_q["stem"],
                    choices=mock_q.get("choices"),
                    answer_schema=mock_q["answer_schema"],
                    difficulty=final_difficulty,
                    category=category,
                    round=round_num,
                )
                self.session.add(question)
                questions_list.append(question)

        self.session.commit()

        # Format response
        return {
            "session_id": new_session_id,
            "questions": [
                {
                    "id": q.id,
                    "item_type": q.item_type,
                    "stem": q.stem,
                    "choices": q.choices,
                    "answer_schema": q.answer_schema,
                    "difficulty": q.difficulty,
                    "category": q.category,
                }
                for q in questions_list
            ],
            "adaptive_params": params,
        }
