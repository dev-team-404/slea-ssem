"""
Item-Gen-Agent: LangChain ReAct Í∏∞Î∞ò ÏûêÏú® AI ÏóêÏù¥Ï†ÑÌä∏.

REQ: REQ-A-ItemGen

Í∞úÏöî:
    LangChainÏùò ÏµúÏã† Agent Ìå®ÌÑ¥ (ReAct)ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏûêÎèôÏúºÎ°ú ÎèÑÍµ¨Î•º ÏÑ†ÌÉù¬∑ÌôúÏö©ÌïòÎäî
    AI ÏóêÏù¥Ï†ÑÌä∏ÏûÖÎãàÎã§. Mode 1 (Î¨∏Ìï≠ ÏÉùÏÑ±)Í≥º Mode 2 (ÏûêÎèô Ï±ÑÏ†ê) Îëê Í∞ÄÏßÄ Î™®ÎìúÎ•º ÏßÄÏõêÌï©ÎãàÎã§.

Ï∞∏Í≥†:
    - LangChain Í≥µÏãù Î¨∏ÏÑú: https://python.langchain.com/docs/concepts/agents
    - create_react_agent: https://python.langchain.com/api_reference/langchain/agents/
    - ÏµúÏã† API Î≤ÑÏ†Ñ: LangChain 0.3.x+

ÌíàÏßà Í∏∞Ï§Ä:
    - ÌåÄ ÎèôÎ£å Ï∞∏Í≥† ÏΩîÎìú (ÎÜíÏùÄ ÏàòÏ§ÄÏùò Î¨∏ÏÑúÌôî)
    - Í≥µÏãù Î¨∏ÏÑú ÏòàÏãú Í∏∞Î∞ò Íµ¨ÌòÑ
    - ÌÉÄÏûÖ ÌûåÌä∏ & ÏóêÎü¨ Ï≤òÎ¶¨ Î™ÖÏãú
"""

import json
import logging
import re
import uuid
from datetime import UTC, datetime

from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel, Field

from src.agent.config import AGENT_CONFIG, create_llm
from src.agent.fastmcp_server import TOOLS
from src.agent.prompts.react_prompt import get_react_prompt
from src.agent.round_id_generator import RoundIDGenerator

logger = logging.getLogger(__name__)

# Round ID generator instance (singleton pattern)
_round_id_gen = RoundIDGenerator()


# ============================================================================
# Helper Functions for Robust JSON Parsing
# ============================================================================


def parse_json_robust(json_str: str, max_attempts: int = 5) -> dict | list:
    """
    Robust JSON parsing with multiple cleanup strategies.

    This function attempts to parse JSON using various cleanup strategies when
    the initial parse fails. This handles edge cases like unescaped newlines,
    trailing commas, Python True/False, etc.

    Args:
        json_str: Raw JSON string from LLM response
        max_attempts: Maximum number of cleanup strategies to try (default 5)

    Returns:
        Parsed JSON object or list

    Raises:
        json.JSONDecodeError: If all cleanup strategies fail
    """
    if not json_str or not isinstance(json_str, str):
        raise ValueError("json_str must be a non-empty string")

    # Strategy list: (name, cleanup_function)
    cleanup_strategies = [
        # Strategy 1: No cleanup - try as-is
        ("no_cleanup", lambda s: s),

        # Strategy 2: Fix Python literals (True/False/None)
        ("fix_python_literals", lambda s: re.sub(r"\b(True|False)\b", lambda m: m.group(1).lower(), re.sub(r"\bNone\b", "null", s))),

        # Strategy 3: Remove trailing commas
        ("fix_trailing_commas", lambda s: re.sub(r",(\s*[}\]])", r"\1", s)),

        # Strategy 4: Fix escaped quotes and backslashes
        ("fix_escapes", lambda s: re.sub(r"\\(?!\\|/|[btnfr])", "\\\\", s)),

        # Strategy 5: Remove control characters
        ("remove_control_chars", lambda s: s.encode('utf-8', 'ignore').decode('utf-8')),
    ]

    last_error = None
    for attempt_num, (strategy_name, cleanup_fn) in enumerate(cleanup_strategies, 1):
        try:
            cleaned_json = cleanup_fn(json_str)
            result = json.loads(cleaned_json)

            if attempt_num > 1:
                logger.info(
                    f"‚úÖ JSON parsing succeeded with strategy '{strategy_name}' "
                    f"(attempt {attempt_num}/{len(cleanup_strategies)})"
                )

            return result

        except json.JSONDecodeError as e:
            last_error = e
            logger.debug(
                f"   Attempt {attempt_num}/{len(cleanup_strategies)} "
                f"(strategy: {strategy_name}) failed at char {e.pos}: {e.msg}"
            )
            continue

        except Exception as e:
            logger.debug(f"   Attempt {attempt_num} ({strategy_name}) error: {e}")
            continue

    # All strategies failed
    logger.error(
        f"‚ùå JSON parsing failed after {len(cleanup_strategies)} cleanup strategies. "
        f"Last error: {last_error} at char {last_error.pos if last_error else 'unknown'}"
    )
    raise last_error or ValueError("JSON parsing failed with unknown error")


def normalize_answer_schema(answer_schema_raw: str | dict | None) -> str:
    """
    Normalize answer_schema to ensure it's always a string.

    Handles multiple formats:
    1. Tool 5 format: {"type": "exact_match", "keywords": null, "correct_answer": "..."}
    2. Mock format: {"correct_key": "B", "explanation": "..."}
    3. LLM format: {"answer_type": "exact_match"}
    4. String format: "exact_match"

    Args:
        answer_schema_raw: Raw answer_schema value (could be str or dict)

    Returns:
        Normalized answer_schema as string: "exact_match" | "keyword_match" | "semantic_match"
    """
    if isinstance(answer_schema_raw, dict):
        # Case 1: Tool 5 format - has 'type' field
        if "type" in answer_schema_raw:
            schema_type = answer_schema_raw.get("type")
            if isinstance(schema_type, str):
                return schema_type

        # Case 2: Mock/LLM format - has correct_key or answer_type
        # Mock questions use "correct_key" but it's still exact_match
        if "correct_key" in answer_schema_raw or "explanation" in answer_schema_raw:
            return "exact_match"

        # Case 3: Keywords present - likely keyword_match
        if "keywords" in answer_schema_raw:
            return "keyword_match"

        # Case 4: Unknown dict - try 'type' or 'answer_type'
        for key in ["type", "answer_type", "schema_type"]:
            if key in answer_schema_raw:
                val = answer_schema_raw.get(key)
                if isinstance(val, str):
                    return val

    if isinstance(answer_schema_raw, str):
        return answer_schema_raw

    # Default fallback
    logger.warning(
        f"‚ö†Ô∏è  answer_schema has unexpected type: {type(answer_schema_raw).__name__}. "
        f"Using default 'exact_match'"
    )
    return "exact_match"


# ============================================================================
# Pydantic Schemas (ÏûÖÏ∂úÎ†• Îç∞Ïù¥ÌÑ∞ Í≥ÑÏïΩ)
# ============================================================================


class GenerateQuestionsRequest(BaseModel):
    """Î¨∏Ìï≠ ÏÉùÏÑ± ÏöîÏ≤≠ (REQ: POST /api/v1/items/generate)."""

    session_id: str = Field(..., description="ÌÖåÏä§Ìä∏ ÏÑ∏ÏÖò ID (Backend ServiceÍ∞Ä ÏÉùÏÑ±)")
    survey_id: str = Field(..., description="ÏÑ§Î¨∏ ID")
    round_idx: int = Field(..., ge=1, description="ÎùºÏö¥Îìú Î≤àÌò∏ (1-based)")
    prev_answers: list[dict] | None = Field(default=None, description="Ïù¥Ï†Ñ ÎùºÏö¥Îìú ÎãµÎ≥Ä (Ï†ÅÏùëÌòï ÌÖåÏä§Ìä∏Ïö©)")
    question_count: int = Field(default=5, ge=1, le=20, description="ÏÉùÏÑ±Ìï† Î¨∏Ìï≠ Í∞úÏàò (Í∏∞Î≥∏Í∞í: 5, ÌÖåÏä§Ìä∏: 2)")
    question_types: list[str] | None = Field(
        default=None, description="ÏÉùÏÑ±Ìï† Î¨∏Ìï≠ Ïú†Ìòï (multiple_choice | true_false | short_answer), NoneÏù¥Î©¥ Î™®Îëê ÏÉùÏÑ±"
    )
    domain: str = Field(default="AI", description="Î¨∏Ìï≠ ÎèÑÎ©îÏù∏/Ï£ºÏ†ú (Ïòà: AI, food, science)")


class AnswerSchema(BaseModel):
    """ÎãµÎ≥Ä Í≤ÄÏ¶ù Ïä§ÌÇ§Îßà."""

    type: str = Field(..., description="ÎãµÎ≥Ä Ïú†Ìòï (exact_match | keyword_match | semantic_match)")
    keywords: list[str] | None = Field(default=None, description="Ï†ïÎãµ ÌÇ§ÏõåÎìú (Ï£ºÍ¥ÄÏãùÏö©)")
    correct_answer: str | None = Field(default=None, description="Ï†ïÎãµ (Í∞ùÍ¥ÄÏãù/OXÏö©)")


class GeneratedItem(BaseModel):
    """ÏÉùÏÑ±Îêú Î¨∏Ìï≠ ÏïÑÏù¥ÌÖú."""

    id: str = Field(..., description="Î¨∏Ìï≠ ID (UUID)")
    type: str = Field(..., description="Î¨∏Ìï≠ Ïú†Ìòï (multiple_choice | true_false | short_answer)")
    stem: str = Field(..., description="Î¨∏Ìï≠ ÎÇ¥Ïö©")
    choices: list[str] | None = Field(default=None, description="Í∞ùÍ¥ÄÏãù ÏÑ†ÌÉùÏßÄ")
    answer_schema: AnswerSchema = Field(..., description="ÎãµÎ≥Ä Í≤ÄÏ¶ù Ïä§ÌÇ§Îßà")
    difficulty: int = Field(..., ge=1, le=10, description="ÎÇúÏù¥ÎèÑ (1~10)")
    category: str = Field(..., description="Î¨∏Ìï≠ Ïπ¥ÌÖåÍ≥†Î¶¨")
    validation_score: float = Field(default=0.0, ge=0, le=1, description="Í≤ÄÏ¶ù Ï†êÏàò (Tool 4) - ÎÇ¥Î∂Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞")
    saved_at: str | None = Field(default=None, description="Ï†ÄÏû• ÏãúÍ∞Ñ - ÎÇ¥Î∂Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞")


class GenerateQuestionsResponse(BaseModel):
    """Î¨∏Ìï≠ ÏÉùÏÑ± ÏùëÎãµ (REQ: POST /api/v1/items/generate)."""

    round_id: str = Field(..., description="ÏÉùÏÑ±Îêú ÎùºÏö¥Îìú ID")
    items: list[GeneratedItem] = Field(..., description="ÏÉùÏÑ±Îêú Î¨∏Ìï≠ Î™©Î°ù")
    time_limit_seconds: int = Field(default=1200, description="ÏãúÍ∞Ñ Ï†úÌïú (Ï¥à, Í∏∞Î≥∏ 20Î∂Ñ)")
    agent_steps: int = Field(default=0, description="ÏóêÏù¥Ï†ÑÌä∏ Î∞òÎ≥µ ÌöüÏàò - ÎÇ¥Î∂Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞")
    failed_count: int = Field(default=0, description="Ïã§Ìå®Ìïú Î¨∏Ìï≠ Í∞úÏàò - ÎÇ¥Î∂Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞")
    total_tokens: int = Field(default=0, description="LLM ÏùëÎãµ Ï¥ù ÌÜ†ÌÅ∞ Ïàò (input + output)")
    error_message: str | None = Field(default=None, description="ÏóêÎü¨ Î©îÏãúÏßÄ")


class ScoreAnswerRequest(BaseModel):
    """
    ÏûêÎèô Ï±ÑÏ†ê ÏöîÏ≤≠ (Îã®Ïùº Ï≤òÎ¶¨, Phase 1 - Tool 6 ÏãúÍ∑∏ÎãàÏ≤ò Ï§ÄÏàò).

    Tool 6 ÏãúÍ∑∏ÎãàÏ≤ò: score_and_explain(session_id, user_id, question_id, question_type, user_answer, ...)

    ÌïÑÎìú ÏÑ§Í≥Ñ:
    - Tool 6 ÌïÑÏàò: session_id, user_id, question_id, question_type (Î™®Îëê optionalÎ°ú ÏãúÏûë Í∞ÄÎä•, ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ï†úÍ≥µ)
    - ÌïÑÏàò (API): user_answer
    - Tool 6 ÏÑ†ÌÉù: correct_answer, correct_keywords, difficulty, category
    - ÎÇ¥Î∂ÄÏö©: round_id (Ìò∏ÌôòÏÑ±), item_id (Ìò∏ÌôòÏÑ±), response_time_ms
    """

    # ÌïÑÏàò ÌååÎùºÎØ∏ÌÑ∞
    user_answer: str = Field(..., description="ÏùëÏãúÏûêÏùò ÎãµÎ≥Ä")

    # Tool 6 ÌååÎùºÎØ∏ÌÑ∞ (ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Ìò∏Ï∂úÌï† Îïå Ï†úÍ≥µ)
    session_id: str | None = Field(default=None, description="ÌÖåÏä§Ìä∏ ÏÑ∏ÏÖò ID")
    user_id: str | None = Field(default=None, description="ÏÇ¨Ïö©Ïûê ID")
    question_id: str | None = Field(default=None, description="Î¨∏Ìï≠ ID")
    question_type: str | None = Field(default=None, description="Î¨∏Ìï≠ Ïú†Ìòï (multiple_choice|true_false|short_answer)")

    # Tool 6 ÏÑ†ÌÉù ÌååÎùºÎØ∏ÌÑ∞
    correct_answer: str | None = Field(default=None, description="Ï†ïÎãµ (Í∞ùÍ¥ÄÏãù/Ï∞∏Í±∞ÏßìÏö©)")
    correct_keywords: list[str] | None = Field(default=None, description="Ï†ïÎãµ ÌÇ§ÏõåÎìú (Ï£ºÍ¥ÄÏãùÏö©)")
    difficulty: int | None = Field(default=None, description="Î¨∏Ìï≠ ÎÇúÏù¥ÎèÑ (1-10)")
    category: str | None = Field(default=None, description="Î¨∏Ìï≠ Ïπ¥ÌÖåÍ≥†Î¶¨")

    # ÎÇ¥Î∂ÄÏö© Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ (Ìò∏ÌôòÏÑ±)
    round_id: str | None = Field(default=None, description="ÎùºÏö¥Îìú ID (ÎÇ¥Î∂ÄÏö©)")
    item_id: str | None = Field(default=None, description="Î¨∏Ìï≠ ID (ÎÇ¥Î∂ÄÏö©, round_idÏôÄ Ìï®Íªò ÏÇ¨Ïö©)")
    response_time_ms: int = Field(default=0, ge=0, description="ÏùëÎãµ ÏãúÍ∞Ñ (Î∞ÄÎ¶¨Ï¥à, ÎÇ¥Î∂ÄÏö©)")


class ScoreAnswerResponse(BaseModel):
    """ÏûêÎèô Ï±ÑÏ†ê ÏùëÎãµ (Îã®Ïùº Ï≤òÎ¶¨, Phase 1 - Tool 6 Î∞òÌôòÍ∞í Ï§ÄÏàò)."""

    # ÌïÑÏàò ÌïÑÎìú
    score: float = Field(..., ge=0, le=100, description="Ï†êÏàò (0~100)")
    explanation: str = Field(..., description="Ï†ïÎãµ Ìï¥ÏÑ§")
    graded_at: str = Field(..., description="Ï±ÑÏ†ê ÏãúÍ∞Ñ")

    # Tool 6 Î∞òÌôòÍ∞í Î∞è API Ìò∏Ìôò ÌïÑÎìú
    # is_correctÏôÄ correctÎäî Í∞ôÏùÄ ÏùòÎØ∏ (Tool 6Îäî is_correct ÏÇ¨Ïö©)
    is_correct: bool = Field(default=False, description="Ï†ïÎãµ Ïó¨Î∂Ä (Tool 6 Ìò∏Ìôò)")
    correct: bool = Field(default=False, description="Ï†ïÎãµ Ïó¨Î∂Ä (API Ìò∏Ìôò, is_correctÏôÄ ÎèôÏùº)")
    feedback: str | None = Field(default=None, description="Î∂ÄÎ∂Ñ Ï†ïÎãµ ÌîºÎìúÎ∞±")
    keyword_matches: list[str] = Field(default_factory=list, description="ÌÇ§ÏõåÎìú Îß§Ïπ≠ (Tool 6)")
    extracted_keywords: list[str] = Field(default_factory=list, description="Ï∂îÏ∂úÎêú ÌÇ§ÏõåÎìú (API Ìò∏Ìôò)")

    # ÎÇ¥Î∂Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    item_id: str | None = Field(default=None, description="Î¨∏Ìï≠ ID (ÎÇ¥Î∂ÄÏö©, Î∞∞Ïπò ÏùëÎãµÏö©)")


# ============================================================================
# Batch Scoring Models (Phase 2)
# ============================================================================


class UserAnswer(BaseModel):
    """ÏÇ¨Ïö©Ïûê ÎãµÎ≥Ä (Î∞∞Ïπò)."""

    item_id: str = Field(..., description="Î¨∏Ìï≠ ID")
    user_answer: str = Field(..., description="ÏÇ¨Ïö©Ïûê ÎãµÎ≥Ä")
    response_time_ms: int = Field(default=0, ge=0, description="ÏùëÎãµ ÏãúÍ∞Ñ (Î∞ÄÎ¶¨Ï¥à)")


class SubmitAnswersRequest(BaseModel):
    """Î∞∞Ïπò Ï±ÑÏ†ê ÏöîÏ≤≠ (REQ: POST /api/v1/scoring/submit-answers)."""

    round_id: str = Field(..., description="ÎùºÏö¥Îìú ID")
    answers: list[UserAnswer] = Field(..., description="ÏÇ¨Ïö©Ïûê ÎãµÎ≥Ä Î∞∞Ïπò (1-50Í∞ú)")


class ItemScore(BaseModel):
    """Ï±ÑÏ†êÎêú Î¨∏Ìï≠ (Î∞∞Ïπò ÏùëÎãµ)."""

    item_id: str = Field(..., description="Î¨∏Ìï≠ ID")
    correct: bool = Field(..., description="Ï†ïÎãµ Ïó¨Î∂Ä")
    score: float = Field(..., ge=0, le=100, description="Ï†êÏàò (0~100)")
    extracted_keywords: list[str] = Field(default_factory=list, description="Ï∂îÏ∂úÎêú ÌÇ§ÏõåÎìú (Ï£ºÍ¥ÄÏãù)")
    feedback: str | None = Field(default=None, description="Î∂ÄÎ∂Ñ Ï†ïÎãµ ÌîºÎìúÎ∞±")


class RoundStats(BaseModel):
    """ÎùºÏö¥Îìú ÌÜµÍ≥Ñ."""

    avg_response_time: float = Field(..., ge=0, description="ÌèâÍ∑† ÏùëÎãµ ÏãúÍ∞Ñ (Î∞ÄÎ¶¨Ï¥à)")
    correct_count: int = Field(..., ge=0, description="Ï†ïÎãµ Í∞úÏàò")
    total_count: int = Field(..., ge=1, description="Ï†ÑÏ≤¥ Î¨∏Ìï≠ Í∞úÏàò")


class SubmitAnswersResponse(BaseModel):
    """Î∞∞Ïπò Ï±ÑÏ†ê ÏùëÎãµ (REQ: POST /api/v1/scoring/submit-answers)."""

    round_id: str = Field(..., description="ÎùºÏö¥Îìú ID")
    per_item: list[ItemScore] = Field(..., description="Î¨∏Ìï≠Î≥Ñ Ï±ÑÏ†ê Í≤∞Í≥º")
    round_score: float = Field(..., ge=0, le=100, description="ÎùºÏö¥Îìú Ï¥ùÏ†ê")
    round_stats: RoundStats = Field(..., description="ÎùºÏö¥Îìú ÌÜµÍ≥Ñ")


# ============================================================================
# ItemGenAgent Main Class
# ============================================================================


class ItemGenAgent:
    """
    LangChain AgentExecutor Í∏∞Î∞ò Item-Gen-Agent.

    ÏÑ§Î™Ö:
        - LangChainÏùò create_tool_calling_agent() API ÏÇ¨Ïö©
        - AgentExecutorÎ°ú ÎèÑÍµ¨ Ìò∏Ï∂ú Î∞è ÏóêÎü¨ Ï≤òÎ¶¨ Í¥ÄÎ¶¨
        - Tool Calling Î∞©Ïãù (ÏµúÏã† LLM Î™®Îç∏ ÏµúÏ†ÅÌôî)
        - Íµ¨Ï°∞ÌôîÎêú ÏûÖÏ∂úÎ†• (Pydantic)
        - ÏÉÅÏÑ∏Ìïú Î°úÍπÖ (ÎîîÎ≤ÑÍπÖ)

    ÏÇ¨Ïö© ÏòàÏãú:
        ```python
        # ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±
        agent = ItemGenAgent()

        # Mode 1: Î¨∏Ìï≠ ÏÉùÏÑ±
        request = GenerateQuestionsRequest(
            survey_id="survey_123",
            round_idx=1,
            prev_answers=None
        )
        response = await agent.generate_questions(request)

        # Mode 2: ÏûêÎèô Ï±ÑÏ†ê
        score_request = ScoreAnswerRequest(
            round_id="round_123",
            item_id="item_456",
            user_answer="The answer is..."
        )
        score_response = await agent.score_and_explain(score_request)
        ```

    Ï∞∏Í≥†:
        - LangChain Í≥µÏãù: https://python.langchain.com/docs/concepts/agents
        - create_tool_calling_agent: Tool Calling Ìå®ÌÑ¥ Íµ¨ÌòÑ (ÏµúÏã† LLM ÏµúÏ†ÅÌôî)
        - AgentExecutor: max_iterations, early_stopping_method, ÏóêÎü¨ Ï≤òÎ¶¨
    """

    def __init__(self) -> None:
        """
        Initialize ItemGenAgent with LangGraph create_react_agent.

        Îã®Í≥Ñ:
            1. LLM ÏÉùÏÑ± (Google Gemini)
            2. ÌîÑÎ°¨ÌîÑÌä∏ Î°úÎìú
            3. FastMCP ÎèÑÍµ¨ Îì±Î°ù
            4. create_react_agent()Î°ú ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ± (ÏµúÏã† Tool Calling ÏßÄÏõê)

        ÏóêÎü¨ Ï≤òÎ¶¨:
            - GEMINI_API_KEY ÏóÜÏùå: ValueError
            - LLM Ï¥àÍ∏∞Ìôî Ïã§Ìå®: Î°úÍ∑∏ + Ïû¨ÏãúÎèÑ
        """
        logger.info("ItemGenAgent Ï¥àÍ∏∞Ìôî Ï§ë...")

        try:
            # 1. LLM ÏÉùÏÑ±
            self.llm = create_llm()
            logger.info("‚úì LLM (Google Gemini) ÏÉùÏÑ± ÏôÑÎ£å")

            # 2. ÌîÑÎ°¨ÌîÑÌä∏ Î°úÎìú
            self.prompt = get_react_prompt()
            logger.info("‚úì ReAct ÌîÑÎ°¨ÌîÑÌä∏ Î°úÎìú ÏôÑÎ£å")

            # 3. ÎèÑÍµ¨ Î™©Î°ù (6Í∞ú)
            self.tools = TOOLS
            logger.info(f"‚úì {len(self.tools)}Í∞ú ÎèÑÍµ¨ Îì±Î°ù ÏôÑÎ£å: {[t.name for t in self.tools]}")

            # 4. create_react_agent() - LangGraph ÏµúÏã† Tool Calling ÏßÄÏõê
            # LangGraphÏùò create_react_agentÎäî ÏµúÏã† LLMÏùò Tool Calling Í∏∞Îä•ÏùÑ ÏûêÎèôÏúºÎ°ú ÌôúÏö©Ìï©ÎãàÎã§.
            # ReAct Ìå®ÌÑ¥: Thought ‚Üí Action ‚Üí ObservationÏùÑ Î∞òÎ≥µÌïòÎ©∞ Î≥µÏû°Ìïú ÏûëÏóÖÏùÑ ÏàòÌñâÌï©ÎãàÎã§.
            # AGENT_CONFIGÏùò max_iterations, early_stopping_method, handle_parsing_errorsÎäî
            # create_react_agentÏùò ÎûòÌçºÎ°ú ÌôúÏö©ÎêòÍ±∞ÎÇò, CompiledStateGraph Ïã§Ìñâ Ïãú configÎ°ú Ï†ÑÎã¨Îê©ÎãàÎã§.
            self.executor = create_react_agent(
                model=self.llm,
                tools=self.tools,
                prompt=self.prompt,
                debug=AGENT_CONFIG.get("verbose", False),
                version="v2",  # ÏµúÏã† LangGraph v2 API ÏÇ¨Ïö©
            )
            logger.info("‚úì ReAct ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ± ÏôÑÎ£å (Tool Calling ÏµúÏ†ÅÌôî v2)")

            logger.info("‚úÖ ItemGenAgent Ï¥àÍ∏∞Ìôî ÏÑ±Í≥µ")

        except Exception as e:
            logger.error(f"‚ùå ItemGenAgent Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            raise

    def _extract_tool_results(self, result: dict, tool_name: str) -> list[tuple[str, str]]:
        """
        Extract tool results from agent output in both formats.

        LangGraph create_react_agent returns:
            {"messages": [AIMessage, ToolMessage, ...], ...}

        While AgentExecutor returns:
            {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...], ...}

        This helper extracts tool calls in both formats to support both actual LangGraph
        output and test mocks.

        Args:
            result: Agent output dict
            tool_name: Name of the tool to extract (e.g., "save_generated_question", "score_and_explain")

        Returns:
            List of (tool_name, tool_output_str) tuples matching the given tool_name

        Raises:
            Exception: Logs errors but continues processing for robustness

        LangChain Message Structures:
        - AIMessage: {"type": "ai", "content": "...", "tool_calls": [ToolCall(...), ...]}
        - ToolMessage: {"type": "tool", "content": "...", "tool_call_id": "call_123", "name": "tool_name"}
        - ToolCall: {id: "call_123", name: "tool_name", args: {...}}

        """
        tool_results = []
        logger.info(f"\nüîß _extract_tool_results: Looking for tool_name='{tool_name}'")

        # Format 1: AgentExecutor intermediate_steps (for backward compatibility with tests)
        intermediate_steps = result.get("intermediate_steps", [])
        if intermediate_steps:
            logger.info("‚úì Format 1: intermediate_steps detected (test mock)")
            logger.info(f"  Total steps: {len(intermediate_steps)}")
            for i, (step_tool_name, tool_output_str) in enumerate(intermediate_steps):
                logger.info(f"  [{i}] step_tool_name='{step_tool_name}', matches_target={step_tool_name == tool_name}")
                if step_tool_name == tool_name:
                    tool_results.append((step_tool_name, tool_output_str))
                    logger.info(f"       ‚úì MATCHED - output_type={type(tool_output_str).__name__}")
            logger.info(f"  Result: {len(tool_results)} matching tools found\n")
            return tool_results

        # Format 2: LangGraph messages format (actual LangGraph output)
        messages = result.get("messages", [])
        if not messages:
            logger.warning("No intermediate_steps or messages found in agent result\n")
            return []

        logger.info("‚úì Format 2: messages detected (LangGraph output)")
        logger.info(f"  Total messages: {len(messages)}\n")

        # Step 1: Build a map of tool_call_id ‚Üí ToolMessage for quick lookup
        logger.info("Step 1Ô∏è‚É£: Scanning for ToolMessages...")
        tool_messages_by_id: dict[str, ToolMessage] = {}
        for i, message in enumerate(messages):
            msg_type = type(message).__name__
            if isinstance(message, ToolMessage):
                tool_call_id = message.tool_call_id
                name = getattr(message, "name", "?")
                content_preview = str(getattr(message, "content", ""))[:100]
                logger.info(f"  [{i}] ToolMessage found:")
                logger.info(f"       tool_call_id={tool_call_id}, name={name}")
                logger.info(f"       content_preview={content_preview}...")
                if tool_call_id:
                    tool_messages_by_id[tool_call_id] = message
                    logger.info("       ‚úì Added to map")
                else:
                    logger.info("       ‚ö†Ô∏è  No tool_call_id!")
            elif msg_type == "AIMessage":
                logger.info(f"  [{i}] AIMessage (checking for tool_calls...)")
            else:
                logger.info(f"  [{i}] {msg_type}")

        logger.info(f"\nToolMessage map summary: {len(tool_messages_by_id)} items\n")

        # Step 2: Iterate through AIMessages to find matching tool calls
        logger.info("Step 2Ô∏è‚É£: Scanning AIMessages for tool_calls...")
        ai_message_count = 0
        for i, message in enumerate(messages):
            if isinstance(message, AIMessage):
                ai_message_count += 1
                logger.info(f"  [{i}] AIMessage #{ai_message_count}")

                # AIMessage has tool_calls list with ToolCall objects
                tool_calls = getattr(message, "tool_calls", [])
                logger.info(f"       tool_calls: {len(tool_calls)} found")

                if not tool_calls:
                    logger.info("       ‚ö†Ô∏è  No tool_calls in this message")
                    continue

                for j, tool_call in enumerate(tool_calls):
                    try:
                        # ToolCall is an object with .id and .name attributes
                        call_id = tool_call.id if hasattr(tool_call, "id") else tool_call.get("id")
                        call_name = tool_call.name if hasattr(tool_call, "name") else tool_call.get("name")

                        logger.info(f"         [{j}] tool_call: name='{call_name}', id={call_id}")
                        logger.info(f"              target_tool_name='{tool_name}', matches={call_name == tool_name}")

                        # Check if this tool call matches what we're looking for
                        if call_name == tool_name:
                            logger.info("              ‚úì NAME MATCHED!")
                            # Find the corresponding ToolMessage
                            if call_id in tool_messages_by_id:
                                tool_msg = tool_messages_by_id[call_id]
                                content = tool_msg.content if hasattr(tool_msg, "content") else str(tool_msg)
                                tool_results.append((tool_name, content))
                                content_preview = str(content)[:100]
                                logger.info(f"              ‚úì FOUND ToolMessage with id={call_id}")
                                logger.info(f"              content_preview={content_preview}...")
                            else:
                                logger.warning(
                                    f"              ‚úó NO ToolMessage found for id={call_id}!\n"
                                    f"                 Available IDs in map: {list(tool_messages_by_id.keys())}"
                                )
                        else:
                            logger.info(f"              ‚úó Name mismatch (looking for '{tool_name}')")

                    except (AttributeError, KeyError, TypeError) as e:
                        logger.error(f"         [{j}] ERROR extracting tool_call properties: {e}")
                        continue

        logger.info(f"\nStep 2Ô∏è‚É£ Result: {len(tool_results)} matching tools found\n")
        return tool_results

    async def generate_questions(self, request: GenerateQuestionsRequest) -> GenerateQuestionsResponse:
        """
        Mode 1: Generate questions (Tool 1-5 auto-select).

        REQ: REQ-A-Mode1-Pipeline

        Îã®Í≥Ñ:
            1. ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ Ï°∞Ìöå (Tool 1)
            2. ÌÖúÌîåÎ¶ø Í≤ÄÏÉâ (Tool 2) - ÏÑ†ÌÉùÏÇ¨Ìï≠
            3. ÎÇúÏù¥ÎèÑÎ≥Ñ ÌÇ§ÏõåÎìú Ï°∞Ìöå (Tool 3)
            4. LLMÏúºÎ°ú Î¨∏Ìï≠ ÏÉùÏÑ±
            5. Í∞Å Î¨∏Ìï≠ Í≤ÄÏ¶ù (Tool 4)
            6. Í≤ÄÏ¶ù ÌÜµÍ≥º Î¨∏Ìï≠ Ï†ÄÏû• (Tool 5)

        Args:
            request: GenerateQuestionsRequest

        Returns:
            GenerateQuestionsResponse

        ÏóêÎü¨ Ï≤òÎ¶¨:
            - Tool Ìò∏Ï∂ú Ïã§Ìå®: ÏûêÎèô Ïû¨ÏãúÎèÑ (ÏµúÎåÄ 3Ìöå)
            - ÏóêÏù¥Ï†ÑÌä∏ ÏµúÎåÄ Î∞òÎ≥µ: Î∂ÄÎ∂Ñ Í≤∞Í≥º Î∞òÌôò
            - LLM Ïò§Î•ò: ÏóêÎü¨ Î©îÏãúÏßÄ Ìè¨Ìï®

        Ï∞∏Í≥†:
            - AgentExecutor: ÎèÑÍµ¨ Ìò∏Ï∂ú Î∞è Tool Calling Î£®ÌîÑ ÏûêÎèô Í¥ÄÎ¶¨
            - intermediate_steps: ÏóêÏù¥Ï†ÑÌä∏Ïùò Í∞Å ÎèÑÍµ¨ Ìò∏Ï∂ú Ï∂îÏ†Å

        """
        logger.info(f"üìù Î¨∏Ìï≠ ÏÉùÏÑ± ÏãúÏûë: survey_id={request.survey_id}, round_idx={request.round_idx}")

        try:
            # ÎùºÏö¥Îìú ID ÏÉùÏÑ± (REQ-A-RoundID)
            # survey_idÎ•º session_idÎ°ú ÏÇ¨Ïö©ÌïòÏó¨ ÎùºÏö¥Îìú ID ÏÉùÏÑ±
            round_id = _round_id_gen.generate(session_id=request.survey_id, round_number=request.round_idx)

            # ÏóêÏù¥Ï†ÑÌä∏ ÏûÖÎ†• Íµ¨ÏÑ±
            question_types_str = (
                ", ".join(request.question_types)
                if request.question_types
                else "multiple_choice, true_false, short_answer"
            )
            agent_input = f"""
Generate high-quality exam questions for the following survey.
Session ID: {request.session_id}
Survey ID: {request.survey_id}
Round: {request.round_idx}
Domain: {request.domain}
Previous Answers: {json.dumps(request.prev_answers) if request.prev_answers else "None (First round)"}
Question Count: {request.question_count}
Question Types: {question_types_str}

Follow these steps:
1. Get survey context and user profile (Tool 1)
2. Search question templates for similar items (Tool 2) if available
3. Get keywords for adaptive difficulty (Tool 3)
4. Generate new questions with appropriate difficulty (focused on {request.domain} domain)
5. Validate each question (Tool 4)
6. Save validated questions (Tool 5) with session_id={request.session_id} and round_id={round_id}

Important:
- Generate EXACTLY {request.question_count} questions with the specified types
- All questions should be related to {request.domain} domain/topic
- Generate questions with appropriate answer_schema (exact_match, keyword_match, or semantic_match)
- Each question must include: id, type, stem, choices (if MC), answer_schema, difficulty, category
- Return all saved questions with validation scores
- When calling Tool 5, ALWAYS pass session_id={request.session_id} to save questions with correct session reference
"""

            # ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ (Tool Calling Î£®ÌîÑ)
            # LangGraph v2 create_react_agentÎäî messages ÌòïÏãùÏúºÎ°ú ÏûÖÎ†• Î∞õÏùå
            # - messages: ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ (HumanMessage, AIMessage, ToolMessage Îì±)
            # - AgentÍ∞Ä ÏûêÎèôÏúºÎ°ú Thought ‚Üí Action ‚Üí Observation Î∞òÎ≥µ
            #
            # ÏÑ±Îä• ÏµúÏ†ÅÌôî (Ìñ•ÌõÑ Í∞úÏÑ†ÏÇ¨Ìï≠):
            # 1. Tool Î≥ëÎ†¨ Ïã§Ìñâ: ÎèÖÎ¶ΩÏ†ÅÏù∏ ToolÎì§ (validate + save)ÏùÄ asyncio.gatherÎ°ú Î≥ëÎ†¨Ìôî Í∞ÄÎä•
            #    - ReAct Ìå®ÌÑ¥Ïùò Ï†úÏïΩ: Í∞Å Tool Í≤∞Í≥º ‚Üí Îã§Ïùå Thought Í≤∞Ï†ï
            #    - Í∞ÄÎä•Ìïú Í≤ΩÏö∞: Í∞ôÏùÄ ÏßàÎ¨∏Ïùò validate/save Îã®Í≥ÑÎ•º Î≥ëÎ†¨Ìôî
            # 2. Tool ÎπÑÎèôÍ∏∞Ìôî: Î™®Îì† ToolÏùÑ async Ìï®ÏàòÎ°ú Î≥ÄÍ≤Ω (ÌòÑÏû¨Îäî ÎèôÍ∏∞)
            # 3. Ï∫êÏã±: ÏûêÏ£º Ìò∏Ï∂úÎêòÎäî Tool (get_difficulty_keywords)Ïóê Ï∫êÏã± Ï†ÅÏö©
            result = await self.executor.ainvoke({"messages": [HumanMessage(content=agent_input)]})

            logger.info("‚úÖ ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ ÏôÑÎ£å")

            # Í≤∞Í≥º ÌååÏã±
            response = self._parse_agent_output_generate(result, round_id)
            logger.info(f"‚úÖ Î¨∏Ìï≠ ÏÉùÏÑ± ÏÑ±Í≥µ: {len(response.items)}Í∞ú ÏÉùÏÑ±")

            return response

        except Exception as e:
            logger.error(f"‚ùå Î¨∏Ìï≠ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return GenerateQuestionsResponse(
                round_id=f"round_error_{uuid.uuid4().hex[:8]}",
                items=[],
                time_limit_seconds=1200,
                agent_steps=0,
                failed_count=0,
                error_message=str(e),
            )

    async def score_and_explain(self, request: ScoreAnswerRequest) -> ScoreAnswerResponse:
        """
        Mode 2: Auto-grade answers (Tool 6).

        REQ: REQ-A-Mode2-Pipeline

        Îã®Í≥Ñ:
            1. Tool 6 Ìò∏Ï∂ú (ÏûêÎèô Ï±ÑÏ†ê & Ìï¥ÏÑ§ ÏÉùÏÑ±)

        Args:
            request: ScoreAnswerRequest

        Returns:
            ScoreAnswerResponse

        ÏóêÎü¨ Ï≤òÎ¶¨:
            - Tool 6 Ìò∏Ï∂ú Ïã§Ìå®: Ïû¨ÏãúÎèÑ 3Ìöå
            - LLM Ïò§Î•ò: Í∏∞Î≥∏ Ï†êÏàò 0 Î∞òÌôò

        Ï∞∏Í≥†:
            - Tool 6: Í∞ùÍ¥ÄÏãù/OX (Ï†ïÌôï Îß§Ïπ≠) vs Ï£ºÍ¥ÄÏãù (LLM ÌèâÍ∞Ä)
            - Ï±ÑÏ†ê Í∏∞Ï§Ä: >= 80 ‚Üí Ï†ïÎãµ, 70~79 ‚Üí Î∂ÄÎ∂Ñ Ï†ïÎãµ, < 70 ‚Üí Ïò§Îãµ

        """
        logger.info(
            f"üìã ÏûêÎèô Ï±ÑÏ†ê ÏãúÏûë: round_id={request.round_id}, item_id={request.item_id}, user_id={request.user_id}"
        )

        try:
            # Tool 6 ÌïÑÏàò ÌååÎùºÎØ∏ÌÑ∞ ÌôïÏù∏ Î∞è Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï
            session_id = request.session_id or "unknown_session"
            user_id = request.user_id or "unknown_user"
            question_id = request.question_id or request.item_id
            question_type = request.question_type or "short_answer"

            # ÏóêÏù¥Ï†ÑÌä∏ ÏûÖÎ†• Íµ¨ÏÑ± (Tool 6 ÌïÑÏàò ÌååÎùºÎØ∏ÌÑ∞ Î™ÖÏãú)
            agent_input = f"""
Score and explain the following answer using Tool 6 (score_and_explain):

=== TEST SESSION CONTEXT ===
Session ID: {session_id}
User ID: {user_id}
Question ID: {question_id}
Question Type: {question_type}

=== ANSWER DETAILS ===
Round ID: {request.round_id}
User Answer: {request.user_answer}
Response Time (ms): {request.response_time_ms}"""

            # Ï∂îÍ∞Ä Ï†ïÎ≥¥ (ÏÑ†ÌÉùÏÇ¨Ìï≠)
            if request.correct_answer:
                agent_input += f"\nCorrect Answer: {request.correct_answer}"
            if request.correct_keywords:
                agent_input += f"\nCorrect Keywords: {', '.join(request.correct_keywords)}"
            if request.difficulty:
                agent_input += f"\nDifficulty Level: {request.difficulty}"
            if request.category:
                agent_input += f"\nQuestion Category: {request.category}"

            agent_input += f"""

=== TASK ===
Call Tool 6 (score_and_explain) with the following parameters:
- session_id: {session_id}
- user_id: {user_id}
- question_id: {question_id}
- question_type: {question_type}
- user_answer: [User's response]
- correct_answer: [if available]
- correct_keywords: [if applicable]
- difficulty: [if available]
- category: [if available]

Tool 6 will return: is_correct (boolean), score (0-100), explanation, keyword_matches, feedback, graded_at
"""

            # ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ
            result = await self.executor.ainvoke({"input": agent_input})

            logger.info("‚úÖ Ï±ÑÏ†ê ÏôÑÎ£å")

            # Í≤∞Í≥º ÌååÏã±
            response = self._parse_agent_output_score(result, request.item_id)
            logger.info(f"‚úÖ Ï±ÑÏ†ê ÏÑ±Í≥µ: score={response.score}, correct={response.correct}")

            return response

        except Exception as e:
            logger.error(f"‚ùå Ï±ÑÏ†ê Ïã§Ìå®: {e}")
            # Í∏∞Î≥∏Í∞í Î∞òÌôò
            return ScoreAnswerResponse(
                item_id=request.item_id,
                correct=False,
                score=0.0,
                explanation=f"Ï±ÑÏ†ê Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )

    async def submit_answers(self, request: SubmitAnswersRequest) -> SubmitAnswersResponse:
        """
        Mode 2 Batch: Auto-grade multiple answers in one round (Tool 6).

        REQ: REQ-B-ItemGen-Batch

        Îã®Í≥Ñ:
            1. Í∞Å ÎãµÎ≥ÄÏóê ÎåÄÌï¥ Tool 6 Ìò∏Ï∂ú (ÏûêÎèô Ï±ÑÏ†ê)
            2. Ï±ÑÏ†ê Í≤∞Í≥º ÏàòÏßë
            3. ÎùºÏö¥Îìú ÌÜµÍ≥Ñ Í≥ÑÏÇ∞ (ÌèâÍ∑† ÏùëÎãµÏãúÍ∞Ñ, Ï†ïÎãµÎ•† Îì±)
            4. Î∞∞Ïπò ÏùëÎãµ Î∞òÌôò

        Args:
            request: SubmitAnswersRequest

        Returns:
            SubmitAnswersResponse

        ÏóêÎü¨ Ï≤òÎ¶¨:
            - Tool 6 Ìò∏Ï∂ú Ïã§Ìå®: Í∞úÎ≥Ñ Ìï≠Î™©Î≥Ñ Ïû¨ÏãúÎèÑ
            - ÌÜµÍ≥Ñ Í≥ÑÏÇ∞ Ïò§Î•ò: ÏïàÏ†ÑÌïú Í∏∞Î≥∏Í∞í Ï†úÍ≥µ

        """
        logger.info(f"üìù Î∞∞Ïπò Ï±ÑÏ†ê ÏãúÏûë: round_id={request.round_id}, items={len(request.answers)}")

        try:
            per_item: list[ItemScore] = []
            response_times: list[int] = []

            # Round IDÏóêÏÑú session_id Ï∂îÏ∂ú (RoundIDGenerator Ìè¨Îß∑: session_id_round_number_timestamp)
            try:
                parsed_round = _round_id_gen.parse(request.round_id)
                session_id = parsed_round.session_id
                logger.debug(f"Extracted session_id from round_id: {session_id}")
            except Exception as e:
                # Round ID ÌååÏã± Ïã§Ìå® Ïãú round_id Ï†ÑÏ≤¥Î•º session_idÎ°ú ÏÇ¨Ïö©
                session_id = request.round_id
                logger.warning(f"Failed to parse round_id: {e}, using full round_id as session_id")

            # 1. Í∞Å ÎãµÎ≥ÄÏùÑ ÏàúÏ∞® Ï±ÑÏ†ê (Î≥ëÎ†¨ÌôîÎäî Phase 3)
            for answer in request.answers:
                try:
                    # Îã®Ïùº Ï±ÑÏ†ê Î©îÏÑúÎìú ÌôúÏö© (Tool 6 ÌïÑÏàò ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Ìï®)
                    single_request = ScoreAnswerRequest(
                        # Tool 6 ÌïÑÏàò ÌååÎùºÎØ∏ÌÑ∞
                        session_id=session_id,
                        user_id=None,  # Î∞∞Ïπò ÏöîÏ≤≠ÏóêÏÑúÎäî Ï†úÍ≥µÎêòÏßÄ ÏïäÏùå - Tool 6ÏóêÏÑú "unknown_user" Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
                        question_id=answer.item_id,  # item_idÎ•º question_idÎ°ú ÏÇ¨Ïö©
                        question_type="short_answer",  # Î∞∞Ïπò Ï±ÑÏ†êÏóêÏÑú Í∏∞Î≥∏Í∞í - Ïã§Ï†ú ÌÉÄÏûÖÏùÄ ÏßàÎ¨∏ DBÏóêÏÑú Ï°∞Ìöå ÌïÑÏöî
                        # ÏÇ¨Ïö©Ïûê ÎãµÎ≥Ä
                        user_answer=answer.user_answer,
                        # ÎÇ¥Î∂Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                        round_id=request.round_id,
                        item_id=answer.item_id,
                        response_time_ms=answer.response_time_ms,
                    )

                    result = await self.score_and_explain(single_request)

                    # Î∞∞Ïπò ÏùëÎãµ Ìè¨Îß∑ÏúºÎ°ú Î≥ÄÌôò
                    item_score = ItemScore(
                        item_id=result.item_id,
                        correct=result.correct,
                        score=result.score,
                        extracted_keywords=result.extracted_keywords,
                        feedback=result.feedback,
                    )
                    per_item.append(item_score)
                    response_times.append(answer.response_time_ms)

                    logger.info(f"‚úì Î¨∏Ìï≠ Ï±ÑÏ†ê ÏôÑÎ£å: {answer.item_id}, score={result.score}, correct={result.correct}")

                except Exception as e:
                    logger.error(f"‚ùå Î¨∏Ìï≠ Ï±ÑÏ†ê Ïã§Ìå®: {answer.item_id}, {str(e)}")
                    # Ïã§Ìå®Ìïú Ìï≠Î™©ÎèÑ Í≤∞Í≥ºÏóê Ìè¨Ìï® (score=0)
                    item_score = ItemScore(
                        item_id=answer.item_id,
                        correct=False,
                        score=0.0,
                        feedback=f"Ï±ÑÏ†ê Ïò§Î•ò: {str(e)}",
                    )
                    per_item.append(item_score)
                    response_times.append(answer.response_time_ms)

            # 2. ÎùºÏö¥Îìú ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
            correct_count = sum(1 for item in per_item if item.correct)
            total_count = len(per_item)
            round_score = sum(item.score for item in per_item) / total_count if total_count > 0 else 0.0
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0.0

            round_stats = RoundStats(
                avg_response_time=avg_response_time,
                correct_count=correct_count,
                total_count=total_count,
            )

            # 3. Î∞∞Ïπò ÏùëÎãµ ÏÉùÏÑ±
            response = SubmitAnswersResponse(
                round_id=request.round_id,
                per_item=per_item,
                round_score=round_score,
                round_stats=round_stats,
            )

            logger.info(
                f"‚úÖ Î∞∞Ïπò Ï±ÑÏ†ê ÏôÑÎ£å: "
                f"round_score={response.round_score:.1f}, "
                f"correct={correct_count}/{total_count}, "
                f"avg_time={avg_response_time:.0f}ms"
            )

            return response

        except Exception as e:
            logger.error(f"‚ùå Î∞∞Ïπò Ï±ÑÏ†ê Ï§ë ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {e}")
            # Î∂ÄÎ∂Ñ Í≤∞Í≥ºÎùºÎèÑ Î∞òÌôòÌïòÏßÄ ÏïäÍ≥† Ï†ÑÏ≤¥ Ïã§Ìå® ÌëúÏãú
            return SubmitAnswersResponse(
                round_id=request.round_id,
                per_item=[],
                round_score=0.0,
                round_stats=RoundStats(
                    avg_response_time=0.0,
                    correct_count=0,
                    total_count=len(request.answers),
                ),
            )

    def _parse_agent_output_generate(self, result: dict, round_id: str) -> GenerateQuestionsResponse:
        """
        Parse agent output for question generation (REQ-A-LangChain).

        Args:
            result: Agent output (supports both AgentExecutor and LangGraph formats)
            round_id: ÎùºÏö¥Îìú ID

        Returns:
            GenerateQuestionsResponse

        Î°úÏßÅ:
            1. _extract_tool_results()Î°ú ÎèÑÍµ¨ Ìò∏Ï∂ú Ï∂îÏ∂ú (intermediate_steps ÎòêÎäî messages Î™®Îëê ÏßÄÏõê)
            2. nameÏù¥ "save_generated_question"Ïù∏ Ìò∏Ï∂úÏóêÏÑú question Îç∞Ïù¥ÌÑ∞ ÌååÏã±
            3. Í∞Å questionÏùÑ GeneratedItemÏúºÎ°ú Î≥ÄÌôò
            4. ÏÑ±Í≥µ/Ïã§Ìå® Í∞úÏàò ÏßëÍ≥Ñ

        Ï∞∏Í≥†:
            - AgentExecutor Ï∂úÎ†•: {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...]}
            - LangGraph Ï∂úÎ†•: {"messages": [...message objects...]}
            - Îëê Ìè¨Îß∑ Î™®Îëê _extract_tool_results()Î°ú ÏùºÍ¥ÄÎêòÍ≤å Ï≤òÎ¶¨

        """
        logger.info(f"Î¨∏Ìï≠ ÏÉùÏÑ± Í≤∞Í≥º ÌååÏã± Ï§ë... round_id={round_id}")

        # Extract total_tokens from LangGraph messages
        total_tokens = 0
        if "messages" in result:
            for msg in result.get("messages", []):
                if hasattr(msg, "response_metadata") and msg.response_metadata:
                    usage_metadata = msg.response_metadata.get("usage_metadata", {})
                    if "total_tokens" in usage_metadata:
                        total_tokens = usage_metadata["total_tokens"]
                        break

        try:
            # DEBUG: Agent output Íµ¨Ï°∞ Î∂ÑÏÑù
            logger.info("=" * 80)
            logger.info("üîç AGENT OUTPUT STRUCTURE ANALYSIS")
            logger.info("=" * 80)

            # 1. Result dict ÌÇ§ ÌôïÏù∏
            result_keys = list(result.keys())
            logger.info(f"Result ÏµúÏÉÅÏúÑ ÌÇ§: {result_keys}")

            # 2. intermediate_steps ÌôïÏù∏
            intermediate_steps = result.get("intermediate_steps", [])
            if intermediate_steps:
                logger.info(f"\n‚úì intermediate_steps Î∞úÍ≤¨: {len(intermediate_steps)}Í∞ú")
                for i, (tool_name, tool_output) in enumerate(intermediate_steps):
                    output_preview = str(tool_output)[:150]
                    logger.info(f"  [{i}] tool_name={tool_name}, output_type={type(tool_output).__name__}")
                    logger.info(f"      output_preview={output_preview}...")
            else:
                logger.info("\n‚úó intermediate_steps ÏóÜÏùå")

            # 3. messages ÌôïÏù∏
            messages = result.get("messages", [])
            if messages:
                logger.info(f"\n‚úì messages Î∞úÍ≤¨: {len(messages)}Í∞ú")
                for i, msg in enumerate(messages):
                    msg_type = type(msg).__name__
                    msg_attrs = {
                        "type": getattr(msg, "type", "N/A"),
                        "has_content": hasattr(msg, "content"),
                        "has_tool_calls": hasattr(msg, "tool_calls"),
                        "has_tool_call_id": hasattr(msg, "tool_call_id"),
                    }
                    logger.info(f"  [{i}] type={msg_type}, attrs={msg_attrs}")

                    # AIMessageÏùò Í≤ΩÏö∞ tool_calls ÌôïÏù∏
                    if msg_type == "AIMessage" and hasattr(msg, "tool_calls"):
                        tool_calls = getattr(msg, "tool_calls", [])
                        if tool_calls:
                            logger.info(f"      tool_calls: {len(tool_calls)}Í∞ú")
                            for j, tc in enumerate(tool_calls):
                                tc_name = getattr(tc, "name", tc.get("name") if isinstance(tc, dict) else "?")
                                tc_id = getattr(tc, "id", tc.get("id") if isinstance(tc, dict) else "?")
                                logger.info(f"        [{j}] name={tc_name}, id={tc_id}")
                        # DEBUG: AIMessageÏùò content Ï†ÑÏ≤¥ Ï∂úÎ†•
                        ai_content = getattr(msg, "content", "")
                        if ai_content:
                            content_preview = str(ai_content)[:500]
                            logger.info("      AIMessage.content (first 500 chars):")
                            logger.info(f"        {content_preview}...")

                    # ToolMessageÏùò Í≤ΩÏö∞ ÎÇ¥Ïö© ÌôïÏù∏
                    if msg_type == "ToolMessage":
                        content = getattr(msg, "content", "?")
                        tool_call_id = getattr(msg, "tool_call_id", "?")
                        name = getattr(msg, "name", "?")
                        logger.info(f"      tool_call_id={tool_call_id}, name={name}")
                        logger.info(f"      content_preview={str(content)[:200]}...")
            else:
                logger.info("\n‚úó messages ÏóÜÏùå")

            logger.info("=" * 80)

            # 0. ReAct ÌÖçÏä§Ìä∏ ÌòïÏãù: Final Answer JSON ÌååÏã± ÏãúÎèÑ
            logger.info("\nüîç Attempting to parse Final Answer JSON from AIMessage...")
            items: list[GeneratedItem] = []
            failed_count = 0
            error_messages: list[str] = []
            agent_steps = 0  # Initialize agent_steps early

            # AIMessageÏóêÏÑú Final Answer JSON Ï∂îÏ∂ú
            for message in result.get("messages", []):
                if isinstance(message, AIMessage):
                    content = getattr(message, "content", "")

                    # Final Answer: Ìå®ÌÑ¥ Ï∞æÍ∏∞
                    if "Final Answer:" in content:
                        logger.info("‚úì Found 'Final Answer:' in AIMessage content")

                        try:
                            # Final Answer Îí§Ïùò JSON Ï∂îÏ∂ú
                            json_start = content.find("Final Answer:") + len("Final Answer:")
                            json_str = content[json_start:].strip()

                            # ```json ... ``` ÎßàÌÅ¨Îã§Ïö¥ Ï†úÍ±∞
                            if "```json" in json_str:
                                json_str = json_str.split("```json")[1].split("```")[0].strip()
                            elif "```" in json_str:
                                json_str = json_str.split("```")[1].split("```")[0].strip()

                            # Unescape Ï≤òÎ¶¨: AgentÍ∞Ä escaped quotesÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏùå
                            # Replace escaped quotes (order matters: handle single quotes first)
                            # Remove backslash before single quotes (not valid in JSON strings)
                            json_str = json_str.replace("\\'", "'")
                            # Replace escaped double quotes with regular quotes
                            json_str = json_str.replace('\\"', '"')

                            # Convert Python None to JSON null
                            import re

                            json_str = re.sub(r"\bNone\b", "null", json_str)

                            logger.info(f"üìã Extracted JSON (first 300 chars): {json_str[:300]}...")

                            # JSON ÌååÏã± (robust parser ÏÇ¨Ïö©)
                            try:
                                questions_data = parse_json_robust(json_str)
                            except json.JSONDecodeError as e:
                                logger.warning(f"‚ùå Failed to parse Final Answer JSON after all cleanup strategies: {e}")
                                error_messages.append(f"Final Answer JSON parsing failed: {str(e)}")
                                continue

                            if not isinstance(questions_data, list):
                                questions_data = [questions_data]

                            logger.info(f"‚úÖ Parsed {len(questions_data)} question(s) from Final Answer JSON")

                            # Í∞Å questionÏùÑ GeneratedItemÏúºÎ°ú Î≥ÄÌôò
                            for q in questions_data:
                                try:
                                    # Determine question type for answer_schema structure
                                    question_type = q.get("type", "multiple_choice")

                                    # answer_schema Íµ¨ÏÑ± (type-aware)
                                    # Tool 5 Î∞òÌôòÍ∞íÏóêÏÑú flattened ÌïÑÎìú ÏÇ¨Ïö© (correct_answer, correct_keywords)
                                    # Normalize answer_schema to handle both string and dict formats
                                    raw_answer_schema = q.get("answer_schema")
                                    normalized_schema_type = normalize_answer_schema(raw_answer_schema)

                                    # Extract correct_answer from multiple possible fields
                                    # Priority: correct_answer > correct_key > (from raw_answer_schema dict)
                                    correct_answer_value = (
                                        q.get("correct_answer") or
                                        q.get("correct_key") or
                                        (raw_answer_schema.get("correct_answer") if isinstance(raw_answer_schema, dict) else None) or
                                        (raw_answer_schema.get("correct_key") if isinstance(raw_answer_schema, dict) else None)
                                    )

                                    if question_type == "short_answer":
                                        # Short answer: include keywords only
                                        answer_schema = AnswerSchema(
                                            type=normalized_schema_type,
                                            keywords=q.get("correct_keywords"),
                                            correct_answer=None,  # Not used for short answer
                                        )
                                    else:
                                        # MC/TF: include correct_answer only
                                        answer_schema = AnswerSchema(
                                            type=normalized_schema_type,
                                            keywords=None,  # Not used for MC/TF
                                            correct_answer=correct_answer_value,
                                        )
                                    logger.info(
                                        f"  ‚úì answer_schema populated: type={answer_schema.type}, keywords={answer_schema.keywords is not None}, correct_answer={answer_schema.correct_answer is not None}"
                                    )

                                    item = GeneratedItem(
                                        id=q.get("question_id", f"q_{uuid.uuid4().hex[:8]}"),
                                        type=question_type,
                                        stem=q.get("stem", ""),
                                        choices=q.get("choices"),
                                        answer_schema=answer_schema,
                                        difficulty=q.get("difficulty", 5),
                                        category=q.get("category", "AI"),
                                        validation_score=q.get("validation_score", 0.0),
                                        saved_at=datetime.now(UTC).isoformat(),
                                    )
                                    items.append(item)
                                    logger.info(f"  ‚úì Created GeneratedItem: {item.id} ({item.stem[:50]}...)")

                                except Exception as e:
                                    logger.error(f"  ‚úó Failed to create GeneratedItem: {e}")
                                    failed_count += 1
                                    error_messages.append(f"GeneratedItem creation error: {str(e)}")
                                    continue

                            # Final Answer JSONÏù¥ ÌååÏã±ÎêòÎ©¥ ÎèÑÍµ¨ Ï∂îÏ∂ú Ïä§ÌÇµ
                            if items:
                                logger.info(f"\n‚úÖ Successfully parsed {len(items)} items from Final Answer JSON")
                                logger.info("Skipping tool results extraction (using Final Answer format)")
                                # Update agent_steps when Final Answer JSON is found
                                agent_steps = max(
                                    agent_steps,
                                    len(result.get("intermediate_steps", [])) or len(result.get("messages", [])),
                                )
                                break

                        except json.JSONDecodeError as e:
                            logger.warning(f"‚ùå Failed to parse Final Answer JSON: {e}")
                            error_messages.append(f"Final Answer JSON decode error: {str(e)}")
                            continue
                        except Exception as e:
                            logger.warning(f"‚ùå Error processing Final Answer: {e}")
                            error_messages.append(f"Final Answer processing error: {str(e)}")
                            continue

            # 1. Final Answer JSONÏù¥ ÏóÜÏúºÎ©¥ save_generated_question ÎèÑÍµ¨ Í≤∞Í≥º Ï∂îÏ∂ú
            if not items:
                logger.info("\nüìä Extracting save_generated_question tool results (LangGraph format)...")
                tool_results = self._extract_tool_results(result, "save_generated_question")
                agent_steps = max(
                    agent_steps, len(result.get("intermediate_steps", [])) or len(result.get("messages", []))
                )
                logger.info(f"‚úì ÎèÑÍµ¨ Ìò∏Ï∂ú {agent_steps}Í∞ú Î∞úÍ≤¨, save_generated_question {len(tool_results)}Í∞ú")

                # DEBUG: Ï∂îÏ∂úÎêú tool_results ÏÉÅÏÑ∏ Ï∂úÎ†•
                if tool_results:
                    logger.info("\nüìã Extracted tool results:")
                    for i, (tool_name, tool_output_str) in enumerate(tool_results):
                        logger.info(f"  [{i}] tool_name={tool_name}")
                        logger.info(f"      output_type={type(tool_output_str).__name__}")
                        output_preview = str(tool_output_str)[:300]
                        logger.info(f"      output_preview={output_preview}...")
                else:
                    logger.warning("‚ö†Ô∏è  No tool results extracted!")

                # Tool results ÌååÏã±ÏúºÎ°ú items Íµ¨ÏÑ±
                for tool_name, tool_output_str in tool_results:
                    if tool_name != "save_generated_question":
                        continue

                    if not tool_output_str:
                        failed_count += 1
                        continue

                    # JSON ÌååÏã±
                    try:
                        tool_output = (
                            json.loads(tool_output_str) if isinstance(tool_output_str, str) else tool_output_str
                        )
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON ÌååÏã± Ïã§Ìå®: {str(tool_output_str)[:100]}")
                        failed_count += 1
                        error_messages.append(f"JSON decode error: {str(e)}")
                        continue

                    # success ÌîåÎûòÍ∑∏ ÌôïÏù∏
                    has_error = "error" in tool_output
                    is_success = tool_output.get("success", not has_error)

                    if not is_success or has_error:
                        failed_count += 1
                        if "error" in tool_output:
                            error_messages.append(tool_output["error"])
                        continue

                    # GeneratedItem Í∞ùÏ≤¥ ÏÉùÏÑ±
                    try:
                        # Determine question type for answer_schema structure
                        item_type = tool_output.get("item_type", "multiple_choice")

                        # answer_schema Íµ¨ÏÑ± (Tool 5Í∞Ä Ï†úÍ≥µÌïòÍ±∞ÎÇò Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©)
                        schema_from_tool = tool_output.get("answer_schema", {})
                        if isinstance(schema_from_tool, dict):
                            # Tool 5ÏóêÏÑú Î∞òÌôòÌïú answer_schema ÏÇ¨Ïö©
                            # Type-aware construction: include only relevant fields for question type
                            if item_type == "short_answer":
                                # Short answer: include keywords only
                                answer_schema = AnswerSchema(
                                    type=schema_from_tool.get("type", "keyword_match"),
                                    keywords=schema_from_tool.get("correct_keywords")
                                    or schema_from_tool.get("keywords"),
                                    correct_answer=None,  # Not used for short answer
                                )
                            else:
                                # MC/TF: include correct_answer/correct_key only
                                answer_schema = AnswerSchema(
                                    type=schema_from_tool.get("type", "exact_match"),
                                    keywords=None,  # Not used for MC/TF
                                    correct_answer=schema_from_tool.get("correct_key")
                                    or schema_from_tool.get("correct_answer"),
                                )
                        else:
                            # Fallback to tool_output fields with type awareness
                            if item_type == "short_answer":
                                answer_schema = AnswerSchema(
                                    type=tool_output.get("answer_type", "keyword_match"),
                                    keywords=tool_output.get("correct_keywords"),
                                    correct_answer=None,
                                )
                            else:
                                answer_schema = AnswerSchema(
                                    type=tool_output.get("answer_type", "exact_match"),
                                    keywords=None,
                                    correct_answer=tool_output.get("correct_answer"),
                                )

                        item = GeneratedItem(
                            id=tool_output.get("question_id", f"q_{uuid.uuid4().hex[:8]}"),
                            type=tool_output.get("item_type", "multiple_choice"),
                            stem=tool_output.get("stem", ""),
                            choices=tool_output.get("choices"),
                            answer_schema=answer_schema,
                            difficulty=tool_output.get("difficulty", 5),
                            category=tool_output.get("category", "general"),
                            validation_score=tool_output.get("validation_score", 0.0),
                            saved_at=tool_output.get("saved_at", datetime.now(UTC).isoformat()),
                        )
                        items.append(item)
                        logger.info(f"‚úì Î¨∏Ìï≠ ÌååÏã± ÏÑ±Í≥µ: {item.id}, stem={item.stem[:50] if item.stem else 'N/A'}")

                    except Exception as e:
                        logger.error(f"GeneratedItem ÏÉùÏÑ± Ïã§Ìå®: {e}")
                        failed_count += 1
                        error_messages.append(str(e))
                        continue

            # 3. ÏùëÎãµ ÏÉùÏÑ±
            error_msg = " | ".join(error_messages) if error_messages else None

            response = GenerateQuestionsResponse(
                round_id=round_id,
                items=items,
                time_limit_seconds=1200,  # Í∏∞Î≥∏ 20Î∂Ñ
                agent_steps=agent_steps,
                failed_count=failed_count,
                total_tokens=total_tokens,
                error_message=error_msg,
            )

            logger.info(f"‚úÖ ÌååÏã± ÏôÑÎ£å: ÏÑ±Í≥µ={len(items)}, Ïã§Ìå®={failed_count}, agent_steps={agent_steps}")
            return response

        except Exception as e:
            logger.error(f"‚ùå ÌååÏã± Ï§ë ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {e}")
            return GenerateQuestionsResponse(
                round_id=round_id,
                items=[],
                time_limit_seconds=1200,
                agent_steps=0,
                failed_count=0,
                total_tokens=total_tokens,
                error_message=f"Parsing error: {str(e)}",
            )

    def _parse_agent_output_score(self, result: dict, item_id: str) -> ScoreAnswerResponse:
        """
        Parse agent output for auto-grading (REQ-A-LangChain).

        Args:
            result: Agent output (supports both AgentExecutor and LangGraph formats)
            item_id: Î¨∏Ìï≠ ID

        Returns:
            ScoreAnswerResponse

        Î°úÏßÅ:
            1. _extract_tool_results()Î°ú score_and_explain Ìò∏Ï∂ú Ï∂îÏ∂ú (intermediate_steps ÎòêÎäî messages Î™®Îëê ÏßÄÏõê)
            2. Tool Ï∂úÎ†•ÏùÑ JSONÏúºÎ°ú ÌååÏã±
            3. is_correct, score, explanation, feedback, keyword_matches Ï∂îÏ∂ú
            4. ScoreAnswerResponseÎ°ú Î≥ÄÌôò

        Ï∞∏Í≥†:
            - AgentExecutor Ï∂úÎ†•: {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...]}
            - LangGraph Ï∂úÎ†•: {"messages": [...message objects...]}
            - Tool 6 (score_and_explain) Ï∂úÎ†• Íµ¨Ï°∞:
              {
                "is_correct": bool,  # Tool 6 Ìò∏Ìôò
                "score": float (0-100),
                "explanation": str,
                "keyword_matches": list[str] (optional),  # Tool 6 Ìò∏Ìôò
                "feedback": str (optional)
              }

        """
        logger.info(f"Ï±ÑÏ†ê Í≤∞Í≥º ÌååÏã± Ï§ë... item_id={item_id}")

        try:
            # 1. score_and_explain ÎèÑÍµ¨ Í≤∞Í≥º Ï∂îÏ∂ú (Ìè¨Îß∑ Î¨¥Í¥Ä)
            tool_results = self._extract_tool_results(result, "score_and_explain")
            if not tool_results:
                logger.warning("score_and_explain ÎèÑÍµ¨ Ìò∏Ï∂úÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation="score_and_explain tool not executed",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 2. Ï≤´ Î≤àÏß∏ score_and_explain Í≤∞Í≥º ÏÇ¨Ïö©
            _, score_tool_output = tool_results[0]
            if not score_tool_output:
                logger.warning("score_and_explain ÎèÑÍµ¨ Ï∂úÎ†•Ïù¥ ÎπÑÏñ¥ÏûàÏùå")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation="score_and_explain output is empty",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 3. JSON ÌååÏã±
            try:
                tool_output = json.loads(score_tool_output) if isinstance(score_tool_output, str) else score_tool_output
            except json.JSONDecodeError as e:
                logger.warning(f"JSON ÌååÏã± Ïã§Ìå®: {str(score_tool_output)[:100]}")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation=f"JSON decode error: {str(e)}",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 4. ScoreAnswerResponse ÏÉùÏÑ± (Tool 6 Ìò∏Ìôò)
            # Tool 6Îäî "is_correct"Î•º Î∞òÌôòÌïòÎ©∞, "keyword_matches"Î°ú ÌÇ§ÏõåÎìú Î¶¨Ïä§Ìä∏ Ï†úÍ≥µ
            is_correct_from_tool = tool_output.get("is_correct", tool_output.get("correct", False))
            keyword_matches = tool_output.get("keyword_matches", tool_output.get("extracted_keywords", []))

            response = ScoreAnswerResponse(
                item_id=item_id,
                is_correct=is_correct_from_tool,  # Tool 6 Ìò∏Ìôò
                correct=is_correct_from_tool,  # API Ìò∏Ìôò (Í∞ôÏùÄ Í∞í)
                score=float(tool_output.get("score", 0)),
                explanation=tool_output.get("explanation", ""),
                feedback=tool_output.get("feedback"),
                keyword_matches=keyword_matches,  # Tool 6 Ìò∏Ìôò
                extracted_keywords=keyword_matches,  # API Ìò∏Ìôò (Í∞ôÏùÄ Í∞í)
                graded_at=tool_output.get("graded_at", datetime.now(UTC).isoformat()),
            )

            logger.info(f"‚úÖ Ï±ÑÏ†ê ÌååÏã± ÏôÑÎ£å: correct={response.correct}, score={response.score}")
            return response

        except Exception as e:
            logger.error(f"‚ùå Ï±ÑÏ†ê ÌååÏã± Ï§ë Ïò§Î•ò: {e}")
            return ScoreAnswerResponse(
                item_id=item_id,
                is_correct=False,
                correct=False,
                score=0.0,
                explanation=f"Parsing error: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )


# ============================================================================
# Factory Function
# ============================================================================


async def create_agent() -> ItemGenAgent:
    """
    Create ItemGenAgent factory function.

    Returns:
        ItemGenAgent: Ï¥àÍ∏∞ÌôîÎêú ÏóêÏù¥Ï†ÑÌä∏

    ÏÇ¨Ïö©:
        ```python
        agent = await create_agent()
        ```

    """
    return ItemGenAgent()
