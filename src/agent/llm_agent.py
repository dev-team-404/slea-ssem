"""
Item-Gen-Agent: LangChain ReAct ê¸°ë°˜ ììœ¨ AI ì—ì´ì „íŠ¸.

REQ: REQ-A-ItemGen

ê°œìš”:
    LangChainì˜ ìµœì‹  Agent íŒ¨í„´ (ReAct)ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒÂ·í™œìš©í•˜ëŠ”
    AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. Mode 1 (ë¬¸í•­ ìƒì„±)ê³¼ Mode 2 (ìë™ ì±„ì ) ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

ì°¸ê³ :
    - LangChain ê³µì‹ ë¬¸ì„œ: https://python.langchain.com/docs/concepts/agents
    - create_react_agent: https://python.langchain.com/api_reference/langchain/agents/
    - ìµœì‹  API ë²„ì „: LangChain 0.3.x+

í’ˆì§ˆ ê¸°ì¤€:
    - íŒ€ ë™ë£Œ ì°¸ê³  ì½”ë“œ (ë†’ì€ ìˆ˜ì¤€ì˜ ë¬¸ì„œí™”)
    - ê³µì‹ ë¬¸ì„œ ì˜ˆì‹œ ê¸°ë°˜ êµ¬í˜„
    - íƒ€ì… íŒíŠ¸ & ì—ëŸ¬ ì²˜ë¦¬ ëª…ì‹œ
"""

import json
import logging
from datetime import UTC, datetime

from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel, Field

from src.agent.config import AGENT_CONFIG, create_llm
from src.agent.fastmcp_server import TOOLS
from src.agent.prompts.react_prompt import get_react_prompt

logger = logging.getLogger(__name__)


# ============================================================================
# Pydantic Schemas (ì…ì¶œë ¥ ë°ì´í„° ê³„ì•½)
# ============================================================================


class GenerateQuestionsRequest(BaseModel):
    """ë¬¸í•­ ìƒì„± ìš”ì²­."""

    user_id: str = Field(..., description="ì‚¬ìš©ì ID (UUID)")
    difficulty: int = Field(..., ge=1, le=10, description="ë‚œì´ë„ 1~10")
    interests: list[str] = Field(default_factory=list, description="ê´€ì‹¬ë¶„ì•¼ (ì˜ˆ: ['LLM', 'RAG'])")
    num_questions: int = Field(default=5, ge=1, le=10, description="ìƒì„±í•  ë¬¸í•­ ê°œìˆ˜")
    test_session_id: str | None = Field(default=None, description="í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ID (Round ID ìƒì„±ìš©)")


class GeneratedQuestion(BaseModel):
    """ìƒì„±ëœ ë¬¸í•­."""

    question_id: str = Field(..., description="ë¬¸í•­ ID (UUID)")
    stem: str = Field(..., description="ë¬¸í•­ ë‚´ìš©")
    item_type: str = Field(..., description="ë¬¸í•­ ìœ í˜•")
    choices: list[str] | None = Field(default=None, description="ê°ê´€ì‹ ì„ íƒì§€")
    correct_answer: str = Field(..., description="ì •ë‹µ")
    difficulty: int = Field(..., description="ë‚œì´ë„")
    category: str = Field(..., description="ì¹´í…Œê³ ë¦¬")
    validation_score: float = Field(..., ge=0, le=1, description="ê²€ì¦ ì ìˆ˜ (Tool 4)")
    saved_at: str = Field(..., description="ì €ì¥ ì‹œê°„")


class GenerateQuestionsResponse(BaseModel):
    """ë¬¸í•­ ìƒì„± ì‘ë‹µ."""

    success: bool = Field(..., description="ì„±ê³µ ì—¬ë¶€")
    questions: list[GeneratedQuestion] = Field(default_factory=list, description="ìƒì„±ëœ ë¬¸í•­ ë¦¬ìŠ¤íŠ¸")
    total_generated: int = Field(..., description="ìƒì„±ëœ ë¬¸í•­ ì´ ê°œìˆ˜")
    failed_count: int = Field(..., description="ì‹¤íŒ¨í•œ ë¬¸í•­ ê°œìˆ˜")
    agent_steps: int = Field(..., description="ì—ì´ì „íŠ¸ ë°˜ë³µ íšŸìˆ˜ (ReAct ë‹¨ê³„)")
    error_message: str | None = Field(default=None, description="ì—ëŸ¬ ë©”ì‹œì§€")


class ScoreAnswerRequest(BaseModel):
    """ìë™ ì±„ì  ìš”ì²­."""

    session_id: str = Field(..., description="ì‹œí—˜ ì„¸ì…˜ ID")
    user_id: str = Field(..., description="ì‘ì‹œì ID")
    question_id: str = Field(..., description="ë¬¸í•­ ID")
    question_type: str = Field(..., description="ë¬¸í•­ ìœ í˜•")
    user_answer: str = Field(..., description="ì‘ì‹œìì˜ ë‹µë³€")
    correct_answer: str | None = Field(default=None, description="ì •ë‹µ (ê°ê´€ì‹/OXìš©)")
    correct_keywords: list[str] | None = Field(default=None, description="ì •ë‹µ í‚¤ì›Œë“œ (ì£¼ê´€ì‹ìš©)")
    difficulty: int | None = Field(default=None, description="ë‚œì´ë„")
    category: str | None = Field(default=None, description="ì¹´í…Œê³ ë¦¬")


class ScoreAnswerResponse(BaseModel):
    """ìë™ ì±„ì  ì‘ë‹µ."""

    attempt_id: str = Field(..., description="ì±„ì  ID (UUID)")
    question_id: str = Field(..., description="ë¬¸í•­ ID")
    is_correct: bool = Field(..., description="ì •ë‹µ ì—¬ë¶€")
    score: int = Field(..., ge=0, le=100, description="ì ìˆ˜ 0~100")
    explanation: str = Field(..., description="ì •ë‹µ í•´ì„¤")
    feedback: str | None = Field(default=None, description="ë¶€ë¶„ ì •ë‹µ í”¼ë“œë°±")
    keyword_matches: list[str] = Field(default_factory=list, description="ë§¤ì¹­ëœ í‚¤ì›Œë“œ (ì£¼ê´€ì‹)")
    graded_at: str = Field(..., description="ì±„ì  ì‹œê°„")


# ============================================================================
# ItemGenAgent Main Class
# ============================================================================


class ItemGenAgent:
    """
    LangChain ReAct ê¸°ë°˜ Item-Gen-Agent.

    ì„¤ëª…:
        - LangChainì˜ ìµœì‹  create_react_agent() API ì‚¬ìš©
        - FastMCP ë„êµ¬ (Tool 1-6) ìë™ ì„ íƒ & ì‹¤í–‰
        - ReAct íŒ¨í„´: Thought â†’ Action â†’ Observation â†’ Reflection
        - êµ¬ì¡°í™”ëœ ì…ì¶œë ¥ (Pydantic)
        - ìƒì„¸í•œ ë¡œê¹… (ë””ë²„ê¹…)

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = ItemGenAgent()

        # Mode 1: ë¬¸í•­ ìƒì„±
        request = GenerateQuestionsRequest(
            user_id="123",
            difficulty=5,
            interests=["LLM", "RAG"]
        )
        response = await agent.generate_questions(request)

        # Mode 2: ìë™ ì±„ì 
        score_request = ScoreAnswerRequest(
            session_id="sess_123",
            user_id="user_123",
            question_id="q_456",
            question_type="short_answer",
            user_answer="The answer is..."
        )
        score_response = await agent.score_and_explain(score_request)
        ```

    ì°¸ê³ :
        - LangChain ê³µì‹: https://python.langchain.com/docs/concepts/agents
        - create_react_agent: Thought/Action/Observation íŒ¨í„´ ìë™ êµ¬í˜„
        - AgentExecutor: ë„êµ¬ í˜¸ì¶œ ë° ì—ëŸ¬ ì²˜ë¦¬ ê´€ë¦¬
    """

    def __init__(self) -> None:
        """
        Initialize ItemGenAgent.

        ë‹¨ê³„:
            1. LLM ìƒì„± (Google Gemini)
            2. ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            3. FastMCP ë„êµ¬ ë“±ë¡
            4. create_react_agent()ë¡œ ì—ì´ì „íŠ¸ ìƒì„± (LangGraph CompiledStateGraph)

        ì—ëŸ¬ ì²˜ë¦¬:
            - GEMINI_API_KEY ì—†ìŒ: ValueError
            - LLM ì´ˆê¸°í™” ì‹¤íŒ¨: ë¡œê·¸ + ì¬ì‹œë„
        """
        logger.info("ItemGenAgent ì´ˆê¸°í™” ì¤‘...")

        try:
            # 1. LLM ìƒì„±
            self.llm = create_llm()
            logger.info("âœ“ LLM (Google Gemini) ìƒì„± ì™„ë£Œ")

            # 2. ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            self.prompt = get_react_prompt()
            logger.info("âœ“ ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")

            # 3. ë„êµ¬ ëª©ë¡ (6ê°œ)
            self.tools = TOOLS
            logger.info(f"âœ“ {len(self.tools)}ê°œ ë„êµ¬ ë“±ë¡ ì™„ë£Œ: {[t.name for t in self.tools]}")

            # 4. create_react_agent() - LangGraph ìµœì‹  API (v0.2.x+)
            # ë°˜í™˜: CompiledStateGraph (LangGraph v2 í˜¸í™˜)
            # ì°¸ê³ : LangGraphì˜ create_react_agentëŠ” LangChainì˜ deprecated initialize_agentë¥¼ ëŒ€ì²´
            self.agent = create_react_agent(
                model=self.llm,
                tools=self.tools,
                prompt=self.prompt,
                debug=AGENT_CONFIG.get("debug", False),
            )
            logger.info("âœ“ ReAct ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ (LangGraph CompiledStateGraph)")

            logger.info("âœ… ItemGenAgent ì´ˆê¸°í™” ì„±ê³µ")

        except Exception as e:
            logger.error(f"âŒ ItemGenAgent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def generate_questions(self, request: GenerateQuestionsRequest) -> GenerateQuestionsResponse:
        """
        Mode 1: Generate questions (Tool 1-5 auto-select).

        REQ: REQ-A-Mode1-Pipeline

        ë‹¨ê³„:
            1. ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (Tool 1)
            2. í…œí”Œë¦¿ ê²€ìƒ‰ (Tool 2) - ì„ íƒì‚¬í•­
            3. ë‚œì´ë„ë³„ í‚¤ì›Œë“œ ì¡°íšŒ (Tool 3)
            4. LLMìœ¼ë¡œ ë¬¸í•­ ìƒì„±
            5. ê° ë¬¸í•­ ê²€ì¦ (Tool 4)
            6. ê²€ì¦ í†µê³¼ ë¬¸í•­ ì €ì¥ (Tool 5)

        Args:
            request: GenerateQuestionsRequest

        Returns:
            GenerateQuestionsResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool í˜¸ì¶œ ì‹¤íŒ¨: ìë™ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
            - ì—ì´ì „íŠ¸ ìµœëŒ€ ë°˜ë³µ: ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜
            - LLM ì˜¤ë¥˜: ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨

        ì°¸ê³ :
            - AgentExecutor: ë„êµ¬ í˜¸ì¶œ ë° ReAct ë£¨í”„ ìë™ ê´€ë¦¬
            - Thought/Action/Observation: ì—ì´ì „íŠ¸ ë¡œê·¸ì—ì„œ ì¶”ì  ê°€ëŠ¥

        """
        logger.info(f"ğŸ“ ë¬¸í•­ ìƒì„± ì‹œì‘: user_id={request.user_id}, difficulty={request.difficulty}")

        try:
            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„±
            agent_input = f"""
Generate {request.num_questions} high-quality questions for user {request.user_id}.
Difficulty level: {request.difficulty} (1~10)
User interests: {", ".join(request.interests) if request.interests else "general"}
Test session ID: {request.test_session_id or "new_session"}

Follow these steps:
1. Get user profile (Tool 1)
2. Search templates for interests (Tool 2) if interests are provided
3. Get difficulty keywords (Tool 3)
4. Generate and validate each question (Tool 4)
5. Save validated questions (Tool 5)

Return exactly {request.num_questions} questions with validation scores.
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ReAct ë£¨í”„)
            # CompiledStateGraph (LangGraph)ê°€ ë‹¤ìŒì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰:
            # - Thought: ì—ì´ì „íŠ¸ ì¶”ë¡ 
            # - Action: ë„êµ¬ ì„ íƒ
            # - Observation: ë„êµ¬ ê²°ê³¼
            # - ë°˜ë³µ ë˜ëŠ” ì¢…ë£Œ
            result = await self.agent.ainvoke({"messages": [{"role": "user", "content": agent_input}]})

            logger.info(f"âœ… ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {result}")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_generate(result, request.num_questions)
            logger.info(f"âœ… ë¬¸í•­ ìƒì„± ì„±ê³µ: {response.total_generated}ê°œ ìƒì„±, {response.failed_count}ê°œ ì‹¤íŒ¨")

            return response

        except Exception as e:
            logger.error(f"âŒ ë¬¸í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return GenerateQuestionsResponse(
                success=False,
                questions=[],
                total_generated=0,
                failed_count=request.num_questions,
                agent_steps=0,
                error_message=str(e),
            )

    async def score_and_explain(self, request: ScoreAnswerRequest) -> ScoreAnswerResponse:
        """
        Mode 2: Auto-grade answers (Tool 6).

        REQ: REQ-A-Mode2-Pipeline

        ë‹¨ê³„:
            1. Tool 6 í˜¸ì¶œ (ìë™ ì±„ì  & í•´ì„¤ ìƒì„±)

        Args:
            request: ScoreAnswerRequest

        Returns:
            ScoreAnswerResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool 6 í˜¸ì¶œ ì‹¤íŒ¨: ì¬ì‹œë„ 3íšŒ
            - LLM ì˜¤ë¥˜: ê¸°ë³¸ ì ìˆ˜ 0 ë°˜í™˜

        ì°¸ê³ :
            - Tool 6: ê°ê´€ì‹/OX (ì •í™• ë§¤ì¹­) vs ì£¼ê´€ì‹ (LLM í‰ê°€)
            - ì±„ì  ê¸°ì¤€: >= 80 â†’ ì •ë‹µ, 70~79 â†’ ë¶€ë¶„ ì •ë‹µ, < 70 â†’ ì˜¤ë‹µ

        """
        logger.info(f"ğŸ“‹ ìë™ ì±„ì  ì‹œì‘: session_id={request.session_id}, question_id={request.question_id}")

        try:
            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„±
            agent_input = f"""
Score and explain the following answer:

Session ID: {request.session_id}
User ID: {request.user_id}
Question ID: {request.question_id}
Question Type: {request.question_type}
User Answer: {request.user_answer}
Correct Answer: {request.correct_answer or "N/A"}
Correct Keywords: {", ".join(request.correct_keywords) if request.correct_keywords else "N/A"}
Difficulty: {request.difficulty or "unknown"}
Category: {request.category or "general"}

Use Tool 6 (score_and_explain) to:
1. Score the answer (0~100)
2. Generate explanation
3. Provide feedback if needed

Return: is_correct, score, explanation, feedback
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = await self.agent.ainvoke({"messages": [{"role": "user", "content": agent_input}]})

            logger.info(f"âœ… ì±„ì  ì™„ë£Œ: {result}")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_score(result, request.question_id)
            logger.info(f"âœ… ì±„ì  ì„±ê³µ: score={response.score}, is_correct={response.is_correct}")

            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ScoreAnswerResponse(
                attempt_id="error",
                question_id=request.question_id,
                is_correct=False,
                score=0,
                explanation=f"ì±„ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )

    def _parse_agent_output_generate(self, result: dict, num_questions: int) -> GenerateQuestionsResponse:
        """
        Parse agent output for question generation (REQ-A-LangChain).

        Args:
            result: CompiledStateGraph (LangGraph)ì˜ ì¶œë ¥
            num_questions: ìš”ì²­í•œ ë¬¸í•­ ê°œìˆ˜

        Returns:
            GenerateQuestionsResponse

        ë¡œì§:
            1. result["messages"]ì—ì„œ ëª¨ë“  ë©”ì‹œì§€ ì¶”ì¶œ
            2. type="tool"ì¸ ë©”ì‹œì§€ í•„í„°ë§
            3. tool.nameì´ "save_generated_question"ì¸ ë©”ì‹œì§€ì—ì„œ question ë°ì´í„° íŒŒì‹±
            4. ê° questionì„ GeneratedQuestionìœ¼ë¡œ ë³€í™˜
            5. ì„±ê³µ/ì‹¤íŒ¨ ê°œìˆ˜ ì§‘ê³„

        ì°¸ê³ :
            - LangGraph CompiledStateGraph ì¶œë ¥ì€ {"messages": [...]} í˜•ì‹
            - ê° ë©”ì‹œì§€ëŠ” {"type": "tool", "name": "...", "content": "..."} ë˜ëŠ”
              {"role": "user/ai", "content": "..."}
            - Tool ì¶œë ¥ì€ ëŒ€ë¶€ë¶„ JSON ë¬¸ìì—´ í˜•íƒœ

        """
        logger.info("ë¬¸í•­ ìƒì„± ê²°ê³¼ íŒŒì‹± ì¤‘...")

        try:
            # 1. ë©”ì‹œì§€ ì¶”ì¶œ
            messages = result.get("messages", [])
            if not messages:
                logger.warning("ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ")
                return GenerateQuestionsResponse(
                    success=True,
                    questions=[],
                    total_generated=0,
                    failed_count=num_questions,
                    agent_steps=0,
                )

            # 2. Tool ë©”ì‹œì§€ ê°œìˆ˜ (agent_steps)
            tool_messages = [m for m in messages if m.get("type") == "tool"]
            agent_steps = len(tool_messages)
            logger.info(f"Tool ë©”ì‹œì§€ {agent_steps}ê°œ ë°œê²¬")

            # 3. save_generated_question ë„êµ¬ ê²°ê³¼ íŒŒì‹±
            questions: list[GeneratedQuestion] = []
            failed_count = 0
            error_messages: list[str] = []

            for message in messages:
                if message.get("type") != "tool":
                    continue

                tool_name = message.get("name", "")
                if tool_name != "save_generated_question":
                    continue

                content = message.get("content", "")
                if not content:
                    failed_count += 1
                    continue

                # JSON íŒŒì‹±
                try:
                    tool_output = json.loads(content)
                except json.JSONDecodeError as e:
                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {content[:100]}")
                    failed_count += 1
                    error_messages.append(f"JSON decode error: {str(e)}")
                    continue

                # success í”Œë˜ê·¸ í™•ì¸ (ì—†ìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼, error ìˆìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼)
                has_error = "error" in tool_output
                is_success = tool_output.get("success", not has_error)

                if not is_success or has_error:
                    failed_count += 1
                    if "error" in tool_output:
                        error_messages.append(tool_output["error"])
                    continue

                # GeneratedQuestion ê°ì²´ ìƒì„±
                try:
                    question = GeneratedQuestion(
                        question_id=tool_output.get("question_id", f"q_{datetime.now(UTC).timestamp()}"),
                        stem=tool_output.get("stem", ""),
                        item_type=tool_output.get("item_type", "unknown"),
                        choices=tool_output.get("choices"),
                        correct_answer=tool_output.get("correct_answer", ""),
                        difficulty=tool_output.get("difficulty", 5),
                        category=tool_output.get("category", "general"),
                        validation_score=tool_output.get("validation_score", 0.0),
                        saved_at=tool_output.get("saved_at", datetime.now(UTC).isoformat()),
                    )
                    questions.append(question)
                    logger.info(f"âœ“ ë¬¸í•­ íŒŒì‹± ì„±ê³µ: {question.question_id}")

                except Exception as e:
                    logger.error(f"GeneratedQuestion ìƒì„± ì‹¤íŒ¨: {e}")
                    failed_count += 1
                    error_messages.append(str(e))
                    continue

            # 4. ì‘ë‹µ ìƒì„±
            total_generated = len(questions) + failed_count
            error_msg = " | ".join(error_messages) if error_messages else None

            response = GenerateQuestionsResponse(
                success=len(questions) > 0,
                questions=questions,
                total_generated=total_generated,
                failed_count=failed_count,
                agent_steps=agent_steps,
                error_message=error_msg,
            )

            logger.info(f"âœ… íŒŒì‹± ì™„ë£Œ: ì„±ê³µ={len(questions)}, ì‹¤íŒ¨={failed_count}, agent_steps={agent_steps}")
            return response

        except Exception as e:
            logger.error(f"âŒ íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return GenerateQuestionsResponse(
                success=False,
                questions=[],
                total_generated=0,
                failed_count=num_questions,
                agent_steps=0,
                error_message=f"Parsing error: {str(e)}",
            )

    def _parse_agent_output_score(self, result: dict, question_id: str) -> ScoreAnswerResponse:
        """
        Parse agent output for auto-grading (REQ-A-LangChain).

        Args:
            result: CompiledStateGraph (LangGraph)ì˜ ì¶œë ¥
            question_id: ë¬¸í•­ ID

        Returns:
            ScoreAnswerResponse

        ë¡œì§:
            1. result["messages"]ì—ì„œ type="tool", name="score_and_explain" ë©”ì‹œì§€ ì°¾ê¸°
            2. ë©”ì‹œì§€ contentë¥¼ JSONìœ¼ë¡œ íŒŒì‹±
            3. is_correct, score, explanation, feedback, keyword_matches ì¶”ì¶œ
            4. ScoreAnswerResponseë¡œ ë³€í™˜

        ì°¸ê³ :
            - Tool 6 (score_and_explain) ì¶œë ¥ êµ¬ì¡°:
              {
                "attempt_id": "str",
                "is_correct": bool,
                "score": float (0-100),
                "explanation": str,
                "keyword_matches": list[str] (optional),
                "feedback": str (optional),
                "graded_at": str (ISO format)
              }

        """
        logger.info(f"ì±„ì  ê²°ê³¼ íŒŒì‹± ì¤‘... question_id={question_id}")

        try:
            # 1. ë©”ì‹œì§€ ì¶”ì¶œ
            messages = result.get("messages", [])
            if not messages:
                logger.warning("ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ")
                return ScoreAnswerResponse(
                    attempt_id="error",
                    question_id=question_id,
                    is_correct=False,
                    score=0,
                    explanation="No messages found",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 2. score_and_explain ë„êµ¬ ë©”ì‹œì§€ ì°¾ê¸°
            score_message = None
            for message in messages:
                if message.get("type") == "tool" and message.get("name") == "score_and_explain":
                    score_message = message
                    break

            if not score_message:
                logger.warning("score_and_explain ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return ScoreAnswerResponse(
                    attempt_id="error",
                    question_id=question_id,
                    is_correct=False,
                    score=0,
                    explanation="Tool not executed",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 3. JSON íŒŒì‹±
            content = score_message.get("content", "")
            if not content:
                logger.warning("Tool ë©”ì‹œì§€ contentê°€ ë¹„ì–´ìˆìŒ")
                return ScoreAnswerResponse(
                    attempt_id="error",
                    question_id=question_id,
                    is_correct=False,
                    score=0,
                    explanation="Empty tool output",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            try:
                tool_output = json.loads(content)
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {content[:100]}")
                return ScoreAnswerResponse(
                    attempt_id="error",
                    question_id=question_id,
                    is_correct=False,
                    score=0,
                    explanation=f"JSON decode error: {str(e)}",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 4. ScoreAnswerResponse ìƒì„±
            response = ScoreAnswerResponse(
                attempt_id=tool_output.get("attempt_id", f"att_{datetime.now(UTC).timestamp()}"),
                question_id=question_id,
                is_correct=tool_output.get("is_correct", False),
                score=int(tool_output.get("score", 0)),
                explanation=tool_output.get("explanation", ""),
                feedback=tool_output.get("feedback"),
                keyword_matches=tool_output.get("keyword_matches", []),
                graded_at=tool_output.get("graded_at", datetime.now(UTC).isoformat()),
            )

            logger.info(f"âœ… ì±„ì  íŒŒì‹± ì™„ë£Œ: is_correct={response.is_correct}, score={response.score}")
            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return ScoreAnswerResponse(
                attempt_id="error",
                question_id=question_id,
                is_correct=False,
                score=0,
                explanation=f"Parsing error: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )


# ============================================================================
# Factory Function
# ============================================================================


async def create_agent() -> ItemGenAgent:
    """
    Create ItemGenAgent factory function.

    Returns:
        ItemGenAgent: ì´ˆê¸°í™”ëœ ì—ì´ì „íŠ¸

    ì‚¬ìš©:
        ```python
        agent = await create_agent()
        ```

    """
    return ItemGenAgent()
