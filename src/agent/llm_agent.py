"""
Item-Gen-Agent: LangChain ReAct ê¸°ë°˜ ììœ¨ AI ì—ì´ì „íŠ¸.

REQ: REQ-A-ItemGen

ê°œìš”:
    LangChainì˜ ìµœì‹  Agent íŒ¨í„´ (ReAct)ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒÂ·í™œìš©í•˜ëŠ”
    AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. Mode 1 (ë¬¸í•­ ìƒì„±)ê³¼ Mode 2 (ìë™ ì±„ì ) ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

ì°¸ê³ :
    - LangChain ê³µì‹ ë¬¸ì„œ: https://python.langchain.com/docs/concepts/agents
    - create_react_agent: https://python.langchain.com/api_reference/langchain/agents/
    - ìµœì‹  API ë²„ì „: LangChain 0.3.x+

í’ˆì§ˆ ê¸°ì¤€:
    - íŒ€ ë™ë£Œ ì°¸ê³  ì½”ë“œ (ë†’ì€ ìˆ˜ì¤€ì˜ ë¬¸ì„œí™”)
    - ê³µì‹ ë¬¸ì„œ ì˜ˆì‹œ ê¸°ë°˜ êµ¬í˜„
    - íƒ€ì… íŒíŠ¸ & ì—ëŸ¬ ì²˜ë¦¬ ëª…ì‹œ
"""

import logging
from datetime import UTC, datetime

from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel, Field

from src.agent.config import AGENT_CONFIG, create_llm
from src.agent.fastmcp_server import TOOLS
from src.agent.prompts.react_prompt import get_react_prompt

logger = logging.getLogger(__name__)


# ============================================================================
# Pydantic Schemas (ì…ì¶œë ¥ ë°ì´í„° ê³„ì•½)
# ============================================================================


class GenerateQuestionsRequest(BaseModel):
    """ë¬¸í•­ ìƒì„± ìš”ì²­."""

    user_id: str = Field(..., description="ì‚¬ìš©ì ID (UUID)")
    difficulty: int = Field(..., ge=1, le=10, description="ë‚œì´ë„ 1~10")
    interests: list[str] = Field(default_factory=list, description="ê´€ì‹¬ë¶„ì•¼ (ì˜ˆ: ['LLM', 'RAG'])")
    num_questions: int = Field(default=5, ge=1, le=10, description="ìƒì„±í•  ë¬¸í•­ ê°œìˆ˜")
    test_session_id: str | None = Field(default=None, description="í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ID (Round ID ìƒì„±ìš©)")


class GeneratedQuestion(BaseModel):
    """ìƒì„±ëœ ë¬¸í•­."""

    question_id: str = Field(..., description="ë¬¸í•­ ID (UUID)")
    stem: str = Field(..., description="ë¬¸í•­ ë‚´ìš©")
    item_type: str = Field(..., description="ë¬¸í•­ ìœ í˜•")
    choices: list[str] | None = Field(default=None, description="ê°ê´€ì‹ ì„ íƒì§€")
    correct_answer: str = Field(..., description="ì •ë‹µ")
    difficulty: int = Field(..., description="ë‚œì´ë„")
    category: str = Field(..., description="ì¹´í…Œê³ ë¦¬")
    validation_score: float = Field(..., ge=0, le=1, description="ê²€ì¦ ì ìˆ˜ (Tool 4)")
    saved_at: str = Field(..., description="ì €ì¥ ì‹œê°„")


class GenerateQuestionsResponse(BaseModel):
    """ë¬¸í•­ ìƒì„± ì‘ë‹µ."""

    success: bool = Field(..., description="ì„±ê³µ ì—¬ë¶€")
    questions: list[GeneratedQuestion] = Field(default_factory=list, description="ìƒì„±ëœ ë¬¸í•­ ë¦¬ìŠ¤íŠ¸")
    total_generated: int = Field(..., description="ìƒì„±ëœ ë¬¸í•­ ì´ ê°œìˆ˜")
    failed_count: int = Field(..., description="ì‹¤íŒ¨í•œ ë¬¸í•­ ê°œìˆ˜")
    agent_steps: int = Field(..., description="ì—ì´ì „íŠ¸ ë°˜ë³µ íšŸìˆ˜ (ReAct ë‹¨ê³„)")
    error_message: str | None = Field(default=None, description="ì—ëŸ¬ ë©”ì‹œì§€")


class ScoreAnswerRequest(BaseModel):
    """ìë™ ì±„ì  ìš”ì²­."""

    session_id: str = Field(..., description="ì‹œí—˜ ì„¸ì…˜ ID")
    user_id: str = Field(..., description="ì‘ì‹œì ID")
    question_id: str = Field(..., description="ë¬¸í•­ ID")
    question_type: str = Field(..., description="ë¬¸í•­ ìœ í˜•")
    user_answer: str = Field(..., description="ì‘ì‹œìì˜ ë‹µë³€")
    correct_answer: str | None = Field(default=None, description="ì •ë‹µ (ê°ê´€ì‹/OXìš©)")
    correct_keywords: list[str] | None = Field(default=None, description="ì •ë‹µ í‚¤ì›Œë“œ (ì£¼ê´€ì‹ìš©)")
    difficulty: int | None = Field(default=None, description="ë‚œì´ë„")
    category: str | None = Field(default=None, description="ì¹´í…Œê³ ë¦¬")


class ScoreAnswerResponse(BaseModel):
    """ìë™ ì±„ì  ì‘ë‹µ."""

    attempt_id: str = Field(..., description="ì±„ì  ID (UUID)")
    question_id: str = Field(..., description="ë¬¸í•­ ID")
    is_correct: bool = Field(..., description="ì •ë‹µ ì—¬ë¶€")
    score: int = Field(..., ge=0, le=100, description="ì ìˆ˜ 0~100")
    explanation: str = Field(..., description="ì •ë‹µ í•´ì„¤")
    feedback: str | None = Field(default=None, description="ë¶€ë¶„ ì •ë‹µ í”¼ë“œë°±")
    keyword_matches: list[str] = Field(default_factory=list, description="ë§¤ì¹­ëœ í‚¤ì›Œë“œ (ì£¼ê´€ì‹)")
    graded_at: str = Field(..., description="ì±„ì  ì‹œê°„")


# ============================================================================
# ItemGenAgent Main Class
# ============================================================================


class ItemGenAgent:
    """
    LangChain ReAct ê¸°ë°˜ Item-Gen-Agent.

    ì„¤ëª…:
        - LangChainì˜ ìµœì‹  create_react_agent() API ì‚¬ìš©
        - FastMCP ë„êµ¬ (Tool 1-6) ìë™ ì„ íƒ & ì‹¤í–‰
        - ReAct íŒ¨í„´: Thought â†’ Action â†’ Observation â†’ Reflection
        - êµ¬ì¡°í™”ëœ ì…ì¶œë ¥ (Pydantic)
        - ìƒì„¸í•œ ë¡œê¹… (ë””ë²„ê¹…)

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = ItemGenAgent()

        # Mode 1: ë¬¸í•­ ìƒì„±
        request = GenerateQuestionsRequest(
            user_id="123",
            difficulty=5,
            interests=["LLM", "RAG"]
        )
        response = await agent.generate_questions(request)

        # Mode 2: ìë™ ì±„ì 
        score_request = ScoreAnswerRequest(
            session_id="sess_123",
            user_id="user_123",
            question_id="q_456",
            question_type="short_answer",
            user_answer="The answer is..."
        )
        score_response = await agent.score_and_explain(score_request)
        ```

    ì°¸ê³ :
        - LangChain ê³µì‹: https://python.langchain.com/docs/concepts/agents
        - create_react_agent: Thought/Action/Observation íŒ¨í„´ ìë™ êµ¬í˜„
        - AgentExecutor: ë„êµ¬ í˜¸ì¶œ ë° ì—ëŸ¬ ì²˜ë¦¬ ê´€ë¦¬
    """

    def __init__(self) -> None:
        """
        Initialize ItemGenAgent.

        ë‹¨ê³„:
            1. LLM ìƒì„± (Google Gemini)
            2. ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            3. FastMCP ë„êµ¬ ë“±ë¡
            4. create_react_agent()ë¡œ ì—ì´ì „íŠ¸ ìƒì„± (LangGraph CompiledStateGraph)

        ì—ëŸ¬ ì²˜ë¦¬:
            - GEMINI_API_KEY ì—†ìŒ: ValueError
            - LLM ì´ˆê¸°í™” ì‹¤íŒ¨: ë¡œê·¸ + ì¬ì‹œë„
        """
        logger.info("ItemGenAgent ì´ˆê¸°í™” ì¤‘...")

        try:
            # 1. LLM ìƒì„±
            self.llm = create_llm()
            logger.info("âœ“ LLM (Google Gemini) ìƒì„± ì™„ë£Œ")

            # 2. ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            self.prompt = get_react_prompt()
            logger.info("âœ“ ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")

            # 3. ë„êµ¬ ëª©ë¡ (6ê°œ)
            self.tools = TOOLS
            logger.info(f"âœ“ {len(self.tools)}ê°œ ë„êµ¬ ë“±ë¡ ì™„ë£Œ: {[t.name for t in self.tools]}")

            # 4. create_react_agent() - LangGraph ìµœì‹  API (v0.2.x+)
            # ë°˜í™˜: CompiledStateGraph (LangGraph v2 í˜¸í™˜)
            # ì°¸ê³ : LangGraphì˜ create_react_agentëŠ” LangChainì˜ deprecated initialize_agentë¥¼ ëŒ€ì²´
            self.agent = create_react_agent(
                model=self.llm,
                tools=self.tools,
                prompt=self.prompt,
                debug=AGENT_CONFIG.get("debug", False),
            )
            logger.info("âœ“ ReAct ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ (LangGraph CompiledStateGraph)")

            logger.info("âœ… ItemGenAgent ì´ˆê¸°í™” ì„±ê³µ")

        except Exception as e:
            logger.error(f"âŒ ItemGenAgent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def generate_questions(self, request: GenerateQuestionsRequest) -> GenerateQuestionsResponse:
        """
        Mode 1: Generate questions (Tool 1-5 auto-select).

        REQ: REQ-A-Mode1-Pipeline

        ë‹¨ê³„:
            1. ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (Tool 1)
            2. í…œí”Œë¦¿ ê²€ìƒ‰ (Tool 2) - ì„ íƒì‚¬í•­
            3. ë‚œì´ë„ë³„ í‚¤ì›Œë“œ ì¡°íšŒ (Tool 3)
            4. LLMìœ¼ë¡œ ë¬¸í•­ ìƒì„±
            5. ê° ë¬¸í•­ ê²€ì¦ (Tool 4)
            6. ê²€ì¦ í†µê³¼ ë¬¸í•­ ì €ì¥ (Tool 5)

        Args:
            request: GenerateQuestionsRequest

        Returns:
            GenerateQuestionsResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool í˜¸ì¶œ ì‹¤íŒ¨: ìë™ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
            - ì—ì´ì „íŠ¸ ìµœëŒ€ ë°˜ë³µ: ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜
            - LLM ì˜¤ë¥˜: ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨

        ì°¸ê³ :
            - AgentExecutor: ë„êµ¬ í˜¸ì¶œ ë° ReAct ë£¨í”„ ìë™ ê´€ë¦¬
            - Thought/Action/Observation: ì—ì´ì „íŠ¸ ë¡œê·¸ì—ì„œ ì¶”ì  ê°€ëŠ¥

        """
        logger.info(f"ğŸ“ ë¬¸í•­ ìƒì„± ì‹œì‘: user_id={request.user_id}, difficulty={request.difficulty}")

        try:
            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„±
            agent_input = f"""
Generate {request.num_questions} high-quality questions for user {request.user_id}.
Difficulty level: {request.difficulty} (1~10)
User interests: {", ".join(request.interests) if request.interests else "general"}
Test session ID: {request.test_session_id or "new_session"}

Follow these steps:
1. Get user profile (Tool 1)
2. Search templates for interests (Tool 2) if interests are provided
3. Get difficulty keywords (Tool 3)
4. Generate and validate each question (Tool 4)
5. Save validated questions (Tool 5)

Return exactly {request.num_questions} questions with validation scores.
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ReAct ë£¨í”„)
            # CompiledStateGraph (LangGraph)ê°€ ë‹¤ìŒì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰:
            # - Thought: ì—ì´ì „íŠ¸ ì¶”ë¡ 
            # - Action: ë„êµ¬ ì„ íƒ
            # - Observation: ë„êµ¬ ê²°ê³¼
            # - ë°˜ë³µ ë˜ëŠ” ì¢…ë£Œ
            result = await self.agent.ainvoke({"messages": [{"role": "user", "content": agent_input}]})

            logger.info(f"âœ… ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {result}")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_generate(result, request.num_questions)
            logger.info(f"âœ… ë¬¸í•­ ìƒì„± ì„±ê³µ: {response.total_generated}ê°œ ìƒì„±, {response.failed_count}ê°œ ì‹¤íŒ¨")

            return response

        except Exception as e:
            logger.error(f"âŒ ë¬¸í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return GenerateQuestionsResponse(
                success=False,
                questions=[],
                total_generated=0,
                failed_count=request.num_questions,
                agent_steps=0,
                error_message=str(e),
            )

    async def score_and_explain(self, request: ScoreAnswerRequest) -> ScoreAnswerResponse:
        """
        Mode 2: Auto-grade answers (Tool 6).

        REQ: REQ-A-Mode2-Pipeline

        ë‹¨ê³„:
            1. Tool 6 í˜¸ì¶œ (ìë™ ì±„ì  & í•´ì„¤ ìƒì„±)

        Args:
            request: ScoreAnswerRequest

        Returns:
            ScoreAnswerResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool 6 í˜¸ì¶œ ì‹¤íŒ¨: ì¬ì‹œë„ 3íšŒ
            - LLM ì˜¤ë¥˜: ê¸°ë³¸ ì ìˆ˜ 0 ë°˜í™˜

        ì°¸ê³ :
            - Tool 6: ê°ê´€ì‹/OX (ì •í™• ë§¤ì¹­) vs ì£¼ê´€ì‹ (LLM í‰ê°€)
            - ì±„ì  ê¸°ì¤€: >= 80 â†’ ì •ë‹µ, 70~79 â†’ ë¶€ë¶„ ì •ë‹µ, < 70 â†’ ì˜¤ë‹µ

        """
        logger.info(f"ğŸ“‹ ìë™ ì±„ì  ì‹œì‘: session_id={request.session_id}, question_id={request.question_id}")

        try:
            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„±
            agent_input = f"""
Score and explain the following answer:

Session ID: {request.session_id}
User ID: {request.user_id}
Question ID: {request.question_id}
Question Type: {request.question_type}
User Answer: {request.user_answer}
Correct Answer: {request.correct_answer or "N/A"}
Correct Keywords: {", ".join(request.correct_keywords) if request.correct_keywords else "N/A"}
Difficulty: {request.difficulty or "unknown"}
Category: {request.category or "general"}

Use Tool 6 (score_and_explain) to:
1. Score the answer (0~100)
2. Generate explanation
3. Provide feedback if needed

Return: is_correct, score, explanation, feedback
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = await self.agent.ainvoke({"messages": [{"role": "user", "content": agent_input}]})

            logger.info(f"âœ… ì±„ì  ì™„ë£Œ: {result}")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_score(result, request.question_id)
            logger.info(f"âœ… ì±„ì  ì„±ê³µ: score={response.score}, is_correct={response.is_correct}")

            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ScoreAnswerResponse(
                attempt_id="error",
                question_id=request.question_id,
                is_correct=False,
                score=0,
                explanation=f"ì±„ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )

    def _parse_agent_output_generate(self, result: dict, num_questions: int) -> GenerateQuestionsResponse:
        """
        Parse agent output for question generation.

        Args:
            result: CompiledStateGraph (LangGraph)ì˜ ì¶œë ¥
            num_questions: ìš”ì²­í•œ ë¬¸í•­ ê°œìˆ˜

        Returns:
            GenerateQuestionsResponse

        ì°¸ê³ :
            - LangGraph output format: {"messages": [...]}
            - Last message contains agent's final response
            - Parse messages for tool outputs and final answer

        """
        # ì„ì‹œ êµ¬í˜„
        # ì‹¤ì œë¡œëŠ” result["messages"]ì—ì„œ ìµœì¢… ì‘ë‹µ ì¶”ì¶œ ë° íŒŒì‹±
        logger.info("ë¬¸í•­ ìƒì„± ê²°ê³¼ íŒŒì‹± ì¤‘...")

        # Extract messages from LangGraph output
        messages = result.get("messages", [])
        agent_steps = len([m for m in messages if m.get("type") in ["tool", "ai", "human"]])

        return GenerateQuestionsResponse(
            success=True,
            questions=[],  # íŒŒì‹±ëœ ë¬¸í•­ ë¦¬ìŠ¤íŠ¸
            total_generated=0,
            failed_count=0,
            agent_steps=agent_steps,
        )

    def _parse_agent_output_score(self, result: dict, question_id: str) -> ScoreAnswerResponse:
        """
        Parse agent output for auto-grading.

        Args:
            result: AgentExecutorì˜ ì¶œë ¥
            question_id: ë¬¸í•­ ID

        Returns:
            ScoreAnswerResponse

        ì°¸ê³ :
            - result["output"]: ì±„ì  ê²°ê³¼ & í•´ì„¤

        """
        # ì„ì‹œ êµ¬í˜„
        logger.info("ì±„ì  ê²°ê³¼ íŒŒì‹± ì¤‘...")

        return ScoreAnswerResponse(
            attempt_id="temp_id",
            question_id=question_id,
            is_correct=False,
            score=0,
            explanation="ì„¤ëª…",
            graded_at=datetime.now(UTC).isoformat(),
        )


# ============================================================================
# Factory Function
# ============================================================================


async def create_agent() -> ItemGenAgent:
    """
    Create ItemGenAgent factory function.

    Returns:
        ItemGenAgent: ì´ˆê¸°í™”ëœ ì—ì´ì „íŠ¸

    ì‚¬ìš©:
        ```python
        agent = await create_agent()
        ```

    """
    return ItemGenAgent()
