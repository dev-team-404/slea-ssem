"""
Item-Gen-Agent: LangChain ReAct ê¸°ë°˜ ììœ¨ AI ì—ì´ì „íŠ¸.

REQ: REQ-A-ItemGen

ê°œìš”:
    LangChainì˜ ìµœì‹  Agent íŒ¨í„´ (ReAct)ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒÂ·í™œìš©í•˜ëŠ”
    AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. Mode 1 (ë¬¸í•­ ìƒì„±)ê³¼ Mode 2 (ìë™ ì±„ì ) ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

ì°¸ê³ :
    - LangChain ê³µì‹ ë¬¸ì„œ: https://python.langchain.com/docs/concepts/agents
    - create_react_agent: https://python.langchain.com/api_reference/langchain/agents/
    - ìµœì‹  API ë²„ì „: LangChain 0.3.x+

í’ˆì§ˆ ê¸°ì¤€:
    - íŒ€ ë™ë£Œ ì°¸ê³  ì½”ë“œ (ë†’ì€ ìˆ˜ì¤€ì˜ ë¬¸ì„œí™”)
    - ê³µì‹ ë¬¸ì„œ ì˜ˆì‹œ ê¸°ë°˜ êµ¬í˜„
    - íƒ€ì… íŒíŠ¸ & ì—ëŸ¬ ì²˜ë¦¬ ëª…ì‹œ
"""

import json
import logging
import uuid
from datetime import UTC, datetime

from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel, Field

from src.agent.config import AGENT_CONFIG, create_llm
from src.agent.fastmcp_server import TOOLS
from src.agent.prompts.react_prompt import get_react_prompt
from src.agent.round_id_generator import RoundIDGenerator

logger = logging.getLogger(__name__)

# Round ID generator instance (singleton pattern)
_round_id_gen = RoundIDGenerator()


# ============================================================================
# Pydantic Schemas (ì…ì¶œë ¥ ë°ì´í„° ê³„ì•½)
# ============================================================================


class GenerateQuestionsRequest(BaseModel):
    """ë¬¸í•­ ìƒì„± ìš”ì²­ (REQ: POST /api/v1/items/generate)."""

    session_id: str = Field(..., description="í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ID (Backend Serviceê°€ ìƒì„±)")
    survey_id: str = Field(..., description="ì„¤ë¬¸ ID")
    round_idx: int = Field(..., ge=1, description="ë¼ìš´ë“œ ë²ˆí˜¸ (1-based)")
    prev_answers: list[dict] | None = Field(default=None, description="ì´ì „ ë¼ìš´ë“œ ë‹µë³€ (ì ì‘í˜• í…ŒìŠ¤íŠ¸ìš©)")
    question_count: int = Field(default=5, ge=1, le=20, description="ìƒì„±í•  ë¬¸í•­ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5, í…ŒìŠ¤íŠ¸: 2)")
    question_types: list[str] | None = Field(
        default=None, description="ìƒì„±í•  ë¬¸í•­ ìœ í˜• (multiple_choice | true_false | short_answer), Noneì´ë©´ ëª¨ë‘ ìƒì„±"
    )
    domain: str = Field(default="AI", description="ë¬¸í•­ ë„ë©”ì¸/ì£¼ì œ (ì˜ˆ: AI, food, science)")


class AnswerSchema(BaseModel):
    """ë‹µë³€ ê²€ì¦ ìŠ¤í‚¤ë§ˆ."""

    type: str = Field(..., description="ë‹µë³€ ìœ í˜• (exact_match | keyword_match | semantic_match)")
    keywords: list[str] | None = Field(default=None, description="ì •ë‹µ í‚¤ì›Œë“œ (ì£¼ê´€ì‹ìš©)")
    correct_answer: str | None = Field(default=None, description="ì •ë‹µ (ê°ê´€ì‹/OXìš©)")


class GeneratedItem(BaseModel):
    """ìƒì„±ëœ ë¬¸í•­ ì•„ì´í…œ."""

    id: str = Field(..., description="ë¬¸í•­ ID (UUID)")
    type: str = Field(..., description="ë¬¸í•­ ìœ í˜• (multiple_choice | true_false | short_answer)")
    stem: str = Field(..., description="ë¬¸í•­ ë‚´ìš©")
    choices: list[str] | None = Field(default=None, description="ê°ê´€ì‹ ì„ íƒì§€")
    answer_schema: AnswerSchema = Field(..., description="ë‹µë³€ ê²€ì¦ ìŠ¤í‚¤ë§ˆ")
    difficulty: int = Field(..., ge=1, le=10, description="ë‚œì´ë„ (1~10)")
    category: str = Field(..., description="ë¬¸í•­ ì¹´í…Œê³ ë¦¬")
    validation_score: float = Field(default=0.0, ge=0, le=1, description="ê²€ì¦ ì ìˆ˜ (Tool 4) - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    saved_at: str | None = Field(default=None, description="ì €ì¥ ì‹œê°„ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")


class GenerateQuestionsResponse(BaseModel):
    """ë¬¸í•­ ìƒì„± ì‘ë‹µ (REQ: POST /api/v1/items/generate)."""

    round_id: str = Field(..., description="ìƒì„±ëœ ë¼ìš´ë“œ ID")
    items: list[GeneratedItem] = Field(..., description="ìƒì„±ëœ ë¬¸í•­ ëª©ë¡")
    time_limit_seconds: int = Field(default=1200, description="ì‹œê°„ ì œí•œ (ì´ˆ, ê¸°ë³¸ 20ë¶„)")
    agent_steps: int = Field(default=0, description="ì—ì´ì „íŠ¸ ë°˜ë³µ íšŸìˆ˜ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    failed_count: int = Field(default=0, description="ì‹¤íŒ¨í•œ ë¬¸í•­ ê°œìˆ˜ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    error_message: str | None = Field(default=None, description="ì—ëŸ¬ ë©”ì‹œì§€")


class ScoreAnswerRequest(BaseModel):
    """
    ìë™ ì±„ì  ìš”ì²­ (ë‹¨ì¼ ì²˜ë¦¬, Phase 1 - Tool 6 ì‹œê·¸ë‹ˆì²˜ ì¤€ìˆ˜).

    Tool 6 ì‹œê·¸ë‹ˆì²˜: score_and_explain(session_id, user_id, question_id, question_type, user_answer, ...)

    í•„ë“œ ì„¤ê³„:
    - Tool 6 í•„ìˆ˜: session_id, user_id, question_id, question_type (ëª¨ë‘ optionalë¡œ ì‹œì‘ ê°€ëŠ¥, ì—ì´ì „íŠ¸ê°€ ì œê³µ)
    - í•„ìˆ˜ (API): user_answer
    - Tool 6 ì„ íƒ: correct_answer, correct_keywords, difficulty, category
    - ë‚´ë¶€ìš©: round_id (í˜¸í™˜ì„±), item_id (í˜¸í™˜ì„±), response_time_ms
    """

    # í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    user_answer: str = Field(..., description="ì‘ì‹œìì˜ ë‹µë³€")

    # Tool 6 íŒŒë¼ë¯¸í„° (ì—ì´ì „íŠ¸ê°€ í˜¸ì¶œí•  ë•Œ ì œê³µ)
    session_id: str | None = Field(default=None, description="í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ID")
    user_id: str | None = Field(default=None, description="ì‚¬ìš©ì ID")
    question_id: str | None = Field(default=None, description="ë¬¸í•­ ID")
    question_type: str | None = Field(default=None, description="ë¬¸í•­ ìœ í˜• (multiple_choice|true_false|short_answer)")

    # Tool 6 ì„ íƒ íŒŒë¼ë¯¸í„°
    correct_answer: str | None = Field(default=None, description="ì •ë‹µ (ê°ê´€ì‹/ì°¸ê±°ì§“ìš©)")
    correct_keywords: list[str] | None = Field(default=None, description="ì •ë‹µ í‚¤ì›Œë“œ (ì£¼ê´€ì‹ìš©)")
    difficulty: int | None = Field(default=None, description="ë¬¸í•­ ë‚œì´ë„ (1-10)")
    category: str | None = Field(default=None, description="ë¬¸í•­ ì¹´í…Œê³ ë¦¬")

    # ë‚´ë¶€ìš© ë©”íƒ€ë°ì´í„° (í˜¸í™˜ì„±)
    round_id: str | None = Field(default=None, description="ë¼ìš´ë“œ ID (ë‚´ë¶€ìš©)")
    item_id: str | None = Field(default=None, description="ë¬¸í•­ ID (ë‚´ë¶€ìš©, round_idì™€ í•¨ê»˜ ì‚¬ìš©)")
    response_time_ms: int = Field(default=0, ge=0, description="ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ, ë‚´ë¶€ìš©)")


class ScoreAnswerResponse(BaseModel):
    """ìë™ ì±„ì  ì‘ë‹µ (ë‹¨ì¼ ì²˜ë¦¬, Phase 1 - Tool 6 ë°˜í™˜ê°’ ì¤€ìˆ˜)."""

    # í•„ìˆ˜ í•„ë“œ
    score: float = Field(..., ge=0, le=100, description="ì ìˆ˜ (0~100)")
    explanation: str = Field(..., description="ì •ë‹µ í•´ì„¤")
    graded_at: str = Field(..., description="ì±„ì  ì‹œê°„")

    # Tool 6 ë°˜í™˜ê°’ ë° API í˜¸í™˜ í•„ë“œ
    # is_correctì™€ correctëŠ” ê°™ì€ ì˜ë¯¸ (Tool 6ëŠ” is_correct ì‚¬ìš©)
    is_correct: bool = Field(default=False, description="ì •ë‹µ ì—¬ë¶€ (Tool 6 í˜¸í™˜)")
    correct: bool = Field(default=False, description="ì •ë‹µ ì—¬ë¶€ (API í˜¸í™˜, is_correctì™€ ë™ì¼)")
    feedback: str | None = Field(default=None, description="ë¶€ë¶„ ì •ë‹µ í”¼ë“œë°±")
    keyword_matches: list[str] = Field(default_factory=list, description="í‚¤ì›Œë“œ ë§¤ì¹­ (Tool 6)")
    extracted_keywords: list[str] = Field(default_factory=list, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ (API í˜¸í™˜)")

    # ë‚´ë¶€ ë©”íƒ€ë°ì´í„°
    item_id: str | None = Field(default=None, description="ë¬¸í•­ ID (ë‚´ë¶€ìš©, ë°°ì¹˜ ì‘ë‹µìš©)")


# ============================================================================
# Batch Scoring Models (Phase 2)
# ============================================================================


class UserAnswer(BaseModel):
    """ì‚¬ìš©ì ë‹µë³€ (ë°°ì¹˜)."""

    item_id: str = Field(..., description="ë¬¸í•­ ID")
    user_answer: str = Field(..., description="ì‚¬ìš©ì ë‹µë³€")
    response_time_ms: int = Field(default=0, ge=0, description="ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")


class SubmitAnswersRequest(BaseModel):
    """ë°°ì¹˜ ì±„ì  ìš”ì²­ (REQ: POST /api/v1/scoring/submit-answers)."""

    round_id: str = Field(..., description="ë¼ìš´ë“œ ID")
    answers: list[UserAnswer] = Field(..., description="ì‚¬ìš©ì ë‹µë³€ ë°°ì¹˜ (1-50ê°œ)")


class ItemScore(BaseModel):
    """ì±„ì ëœ ë¬¸í•­ (ë°°ì¹˜ ì‘ë‹µ)."""

    item_id: str = Field(..., description="ë¬¸í•­ ID")
    correct: bool = Field(..., description="ì •ë‹µ ì—¬ë¶€")
    score: float = Field(..., ge=0, le=100, description="ì ìˆ˜ (0~100)")
    extracted_keywords: list[str] = Field(default_factory=list, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ (ì£¼ê´€ì‹)")
    feedback: str | None = Field(default=None, description="ë¶€ë¶„ ì •ë‹µ í”¼ë“œë°±")


class RoundStats(BaseModel):
    """ë¼ìš´ë“œ í†µê³„."""

    avg_response_time: float = Field(..., ge=0, description="í‰ê·  ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")
    correct_count: int = Field(..., ge=0, description="ì •ë‹µ ê°œìˆ˜")
    total_count: int = Field(..., ge=1, description="ì „ì²´ ë¬¸í•­ ê°œìˆ˜")


class SubmitAnswersResponse(BaseModel):
    """ë°°ì¹˜ ì±„ì  ì‘ë‹µ (REQ: POST /api/v1/scoring/submit-answers)."""

    round_id: str = Field(..., description="ë¼ìš´ë“œ ID")
    per_item: list[ItemScore] = Field(..., description="ë¬¸í•­ë³„ ì±„ì  ê²°ê³¼")
    round_score: float = Field(..., ge=0, le=100, description="ë¼ìš´ë“œ ì´ì ")
    round_stats: RoundStats = Field(..., description="ë¼ìš´ë“œ í†µê³„")


# ============================================================================
# ItemGenAgent Main Class
# ============================================================================


class ItemGenAgent:
    """
    LangChain AgentExecutor ê¸°ë°˜ Item-Gen-Agent.

    ì„¤ëª…:
        - LangChainì˜ create_tool_calling_agent() API ì‚¬ìš©
        - AgentExecutorë¡œ ë„êµ¬ í˜¸ì¶œ ë° ì—ëŸ¬ ì²˜ë¦¬ ê´€ë¦¬
        - Tool Calling ë°©ì‹ (ìµœì‹  LLM ëª¨ë¸ ìµœì í™”)
        - êµ¬ì¡°í™”ëœ ì…ì¶œë ¥ (Pydantic)
        - ìƒì„¸í•œ ë¡œê¹… (ë””ë²„ê¹…)

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = ItemGenAgent()

        # Mode 1: ë¬¸í•­ ìƒì„±
        request = GenerateQuestionsRequest(
            survey_id="survey_123",
            round_idx=1,
            prev_answers=None
        )
        response = await agent.generate_questions(request)

        # Mode 2: ìë™ ì±„ì 
        score_request = ScoreAnswerRequest(
            round_id="round_123",
            item_id="item_456",
            user_answer="The answer is..."
        )
        score_response = await agent.score_and_explain(score_request)
        ```

    ì°¸ê³ :
        - LangChain ê³µì‹: https://python.langchain.com/docs/concepts/agents
        - create_tool_calling_agent: Tool Calling íŒ¨í„´ êµ¬í˜„ (ìµœì‹  LLM ìµœì í™”)
        - AgentExecutor: max_iterations, early_stopping_method, ì—ëŸ¬ ì²˜ë¦¬
    """

    def __init__(self) -> None:
        """
        Initialize ItemGenAgent with LangGraph create_react_agent.

        ë‹¨ê³„:
            1. LLM ìƒì„± (Google Gemini)
            2. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            3. FastMCP ë„êµ¬ ë“±ë¡
            4. create_react_agent()ë¡œ ì—ì´ì „íŠ¸ ìƒì„± (ìµœì‹  Tool Calling ì§€ì›)

        ì—ëŸ¬ ì²˜ë¦¬:
            - GEMINI_API_KEY ì—†ìŒ: ValueError
            - LLM ì´ˆê¸°í™” ì‹¤íŒ¨: ë¡œê·¸ + ì¬ì‹œë„
        """
        logger.info("ItemGenAgent ì´ˆê¸°í™” ì¤‘...")

        try:
            # 1. LLM ìƒì„±
            self.llm = create_llm()
            logger.info("âœ“ LLM (Google Gemini) ìƒì„± ì™„ë£Œ")

            # 2. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            self.prompt = get_react_prompt()
            logger.info("âœ“ ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")

            # 3. ë„êµ¬ ëª©ë¡ (6ê°œ)
            self.tools = TOOLS
            logger.info(f"âœ“ {len(self.tools)}ê°œ ë„êµ¬ ë“±ë¡ ì™„ë£Œ: {[t.name for t in self.tools]}")

            # 4. create_react_agent() - LangGraph ìµœì‹  Tool Calling ì§€ì›
            # LangGraphì˜ create_react_agentëŠ” ìµœì‹  LLMì˜ Tool Calling ê¸°ëŠ¥ì„ ìë™ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.
            # ReAct íŒ¨í„´: Thought â†’ Action â†’ Observationì„ ë°˜ë³µí•˜ë©° ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            # AGENT_CONFIGì˜ max_iterations, early_stopping_method, handle_parsing_errorsëŠ”
            # create_react_agentì˜ ë˜í¼ë¡œ í™œìš©ë˜ê±°ë‚˜, CompiledStateGraph ì‹¤í–‰ ì‹œ configë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
            self.executor = create_react_agent(
                model=self.llm,
                tools=self.tools,
                prompt=self.prompt,
                debug=AGENT_CONFIG.get("verbose", False),
                version="v2",  # ìµœì‹  LangGraph v2 API ì‚¬ìš©
            )
            logger.info("âœ“ ReAct ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ (Tool Calling ìµœì í™” v2)")

            logger.info("âœ… ItemGenAgent ì´ˆê¸°í™” ì„±ê³µ")

        except Exception as e:
            logger.error(f"âŒ ItemGenAgent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _extract_tool_results(self, result: dict, tool_name: str) -> list[tuple[str, str]]:
        """
        Extract tool results from agent output in both formats.

        LangGraph create_react_agent returns:
            {"messages": [AIMessage, ToolMessage, ...], ...}

        While AgentExecutor returns:
            {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...], ...}

        This helper extracts tool calls in both formats to support both actual LangGraph
        output and test mocks.

        Args:
            result: Agent output dict
            tool_name: Name of the tool to extract (e.g., "save_generated_question", "score_and_explain")

        Returns:
            List of (tool_name, tool_output_str) tuples matching the given tool_name

        Raises:
            Exception: Logs errors but continues processing for robustness

        LangChain Message Structures:
        - AIMessage: {"type": "ai", "content": "...", "tool_calls": [ToolCall(...), ...]}
        - ToolMessage: {"type": "tool", "content": "...", "tool_call_id": "call_123", "name": "tool_name"}
        - ToolCall: {id: "call_123", name: "tool_name", args: {...}}

        """
        tool_results = []

        # Format 1: AgentExecutor intermediate_steps (for backward compatibility with tests)
        intermediate_steps = result.get("intermediate_steps", [])
        if intermediate_steps:
            logger.debug("Using intermediate_steps format (test mock detected)")
            for step_tool_name, tool_output_str in intermediate_steps:
                if step_tool_name == tool_name:
                    tool_results.append((step_tool_name, tool_output_str))
            return tool_results

        # Format 2: LangGraph messages format (actual LangGraph output)
        messages = result.get("messages", [])
        if not messages:
            logger.warning("No intermediate_steps or messages found in agent result")
            return []

        logger.debug("Using messages format (actual LangGraph output)")

        # Build a map of tool_call_id â†’ ToolMessage for quick lookup
        tool_messages_by_id: dict[str, ToolMessage] = {}
        for message in messages:
            if isinstance(message, ToolMessage):
                tool_call_id = message.tool_call_id
                if tool_call_id:
                    tool_messages_by_id[tool_call_id] = message
                    logger.debug(f"Found ToolMessage for tool_call_id={tool_call_id}, tool_name={message.name}")

        # Iterate through AIMessages to find matching tool calls
        for message in messages:
            if isinstance(message, AIMessage):
                # AIMessage has tool_calls list with ToolCall objects
                tool_calls = getattr(message, "tool_calls", [])
                if not tool_calls:
                    continue

                for tool_call in tool_calls:
                    try:
                        # ToolCall is an object with .id and .name attributes
                        call_id = tool_call.id if hasattr(tool_call, "id") else tool_call.get("id")
                        call_name = tool_call.name if hasattr(tool_call, "name") else tool_call.get("name")

                        # Check if this tool call matches what we're looking for
                        if call_name == tool_name:
                            # Find the corresponding ToolMessage
                            if call_id in tool_messages_by_id:
                                tool_msg = tool_messages_by_id[call_id]
                                content = tool_msg.content if hasattr(tool_msg, "content") else str(tool_msg)
                                tool_results.append((tool_name, content))
                                logger.debug(
                                    f"Matched tool_call: tool_name={tool_name}, "
                                    f"tool_call_id={call_id}, content_preview={str(content)[:50]}..."
                                )
                            else:
                                logger.warning(f"Tool call {call_id} for {tool_name} has no matching ToolMessage")

                    except (AttributeError, KeyError, TypeError) as e:
                        logger.warning(f"Error extracting tool_call properties: {e}")
                        continue

        return tool_results

    async def generate_questions(self, request: GenerateQuestionsRequest) -> GenerateQuestionsResponse:
        """
        Mode 1: Generate questions (Tool 1-5 auto-select).

        REQ: REQ-A-Mode1-Pipeline

        ë‹¨ê³„:
            1. ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (Tool 1)
            2. í…œí”Œë¦¿ ê²€ìƒ‰ (Tool 2) - ì„ íƒì‚¬í•­
            3. ë‚œì´ë„ë³„ í‚¤ì›Œë“œ ì¡°íšŒ (Tool 3)
            4. LLMìœ¼ë¡œ ë¬¸í•­ ìƒì„±
            5. ê° ë¬¸í•­ ê²€ì¦ (Tool 4)
            6. ê²€ì¦ í†µê³¼ ë¬¸í•­ ì €ì¥ (Tool 5)

        Args:
            request: GenerateQuestionsRequest

        Returns:
            GenerateQuestionsResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool í˜¸ì¶œ ì‹¤íŒ¨: ìë™ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
            - ì—ì´ì „íŠ¸ ìµœëŒ€ ë°˜ë³µ: ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜
            - LLM ì˜¤ë¥˜: ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨

        ì°¸ê³ :
            - AgentExecutor: ë„êµ¬ í˜¸ì¶œ ë° Tool Calling ë£¨í”„ ìë™ ê´€ë¦¬
            - intermediate_steps: ì—ì´ì „íŠ¸ì˜ ê° ë„êµ¬ í˜¸ì¶œ ì¶”ì 

        """
        logger.info(f"ğŸ“ ë¬¸í•­ ìƒì„± ì‹œì‘: survey_id={request.survey_id}, round_idx={request.round_idx}")

        try:
            # ë¼ìš´ë“œ ID ìƒì„± (REQ-A-RoundID)
            # survey_idë¥¼ session_idë¡œ ì‚¬ìš©í•˜ì—¬ ë¼ìš´ë“œ ID ìƒì„±
            round_id = _round_id_gen.generate(session_id=request.survey_id, round_number=request.round_idx)

            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„±
            question_types_str = (
                ", ".join(request.question_types)
                if request.question_types
                else "multiple_choice, true_false, short_answer"
            )
            agent_input = f"""
Generate high-quality exam questions for the following survey.
Session ID: {request.session_id}
Survey ID: {request.survey_id}
Round: {request.round_idx}
Domain: {request.domain}
Previous Answers: {json.dumps(request.prev_answers) if request.prev_answers else "None (First round)"}
Question Count: {request.question_count}
Question Types: {question_types_str}

Follow these steps:
1. Get survey context and user profile (Tool 1)
2. Search question templates for similar items (Tool 2) if available
3. Get keywords for adaptive difficulty (Tool 3)
4. Generate new questions with appropriate difficulty (focused on {request.domain} domain)
5. Validate each question (Tool 4)
6. Save validated questions (Tool 5) with session_id={request.session_id} and round_id={round_id}

Important:
- Generate EXACTLY {request.question_count} questions with the specified types
- All questions should be related to {request.domain} domain/topic
- Generate questions with appropriate answer_schema (exact_match, keyword_match, or semantic_match)
- Each question must include: id, type, stem, choices (if MC), answer_schema, difficulty, category
- Return all saved questions with validation scores
- When calling Tool 5, ALWAYS pass session_id={request.session_id} to save questions with correct session reference
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (Tool Calling ë£¨í”„)
            # LangGraph v2 create_react_agentëŠ” messages í˜•ì‹ìœ¼ë¡œ ì…ë ¥ ë°›ìŒ
            # - messages: ëŒ€í™” íˆìŠ¤í† ë¦¬ (HumanMessage, AIMessage, ToolMessage ë“±)
            # - Agentê°€ ìë™ìœ¼ë¡œ Thought â†’ Action â†’ Observation ë°˜ë³µ
            #
            # ì„±ëŠ¥ ìµœì í™” (í–¥í›„ ê°œì„ ì‚¬í•­):
            # 1. Tool ë³‘ë ¬ ì‹¤í–‰: ë…ë¦½ì ì¸ Toolë“¤ (validate + save)ì€ asyncio.gatherë¡œ ë³‘ë ¬í™” ê°€ëŠ¥
            #    - ReAct íŒ¨í„´ì˜ ì œì•½: ê° Tool ê²°ê³¼ â†’ ë‹¤ìŒ Thought ê²°ì •
            #    - ê°€ëŠ¥í•œ ê²½ìš°: ê°™ì€ ì§ˆë¬¸ì˜ validate/save ë‹¨ê³„ë¥¼ ë³‘ë ¬í™”
            # 2. Tool ë¹„ë™ê¸°í™”: ëª¨ë“  Toolì„ async í•¨ìˆ˜ë¡œ ë³€ê²½ (í˜„ì¬ëŠ” ë™ê¸°)
            # 3. ìºì‹±: ìì£¼ í˜¸ì¶œë˜ëŠ” Tool (get_difficulty_keywords)ì— ìºì‹± ì ìš©
            result = await self.executor.ainvoke({"messages": [HumanMessage(content=agent_input)]})

            logger.info("âœ… ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_generate(result, round_id)
            logger.info(f"âœ… ë¬¸í•­ ìƒì„± ì„±ê³µ: {len(response.items)}ê°œ ìƒì„±")

            return response

        except Exception as e:
            logger.error(f"âŒ ë¬¸í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return GenerateQuestionsResponse(
                round_id=f"round_error_{uuid.uuid4().hex[:8]}",
                items=[],
                time_limit_seconds=1200,
                agent_steps=0,
                failed_count=0,
                error_message=str(e),
            )

    async def score_and_explain(self, request: ScoreAnswerRequest) -> ScoreAnswerResponse:
        """
        Mode 2: Auto-grade answers (Tool 6).

        REQ: REQ-A-Mode2-Pipeline

        ë‹¨ê³„:
            1. Tool 6 í˜¸ì¶œ (ìë™ ì±„ì  & í•´ì„¤ ìƒì„±)

        Args:
            request: ScoreAnswerRequest

        Returns:
            ScoreAnswerResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool 6 í˜¸ì¶œ ì‹¤íŒ¨: ì¬ì‹œë„ 3íšŒ
            - LLM ì˜¤ë¥˜: ê¸°ë³¸ ì ìˆ˜ 0 ë°˜í™˜

        ì°¸ê³ :
            - Tool 6: ê°ê´€ì‹/OX (ì •í™• ë§¤ì¹­) vs ì£¼ê´€ì‹ (LLM í‰ê°€)
            - ì±„ì  ê¸°ì¤€: >= 80 â†’ ì •ë‹µ, 70~79 â†’ ë¶€ë¶„ ì •ë‹µ, < 70 â†’ ì˜¤ë‹µ

        """
        logger.info(
            f"ğŸ“‹ ìë™ ì±„ì  ì‹œì‘: round_id={request.round_id}, item_id={request.item_id}, user_id={request.user_id}"
        )

        try:
            # Tool 6 í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
            session_id = request.session_id or "unknown_session"
            user_id = request.user_id or "unknown_user"
            question_id = request.question_id or request.item_id
            question_type = request.question_type or "short_answer"

            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„± (Tool 6 í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëª…ì‹œ)
            agent_input = f"""
Score and explain the following answer using Tool 6 (score_and_explain):

=== TEST SESSION CONTEXT ===
Session ID: {session_id}
User ID: {user_id}
Question ID: {question_id}
Question Type: {question_type}

=== ANSWER DETAILS ===
Round ID: {request.round_id}
User Answer: {request.user_answer}
Response Time (ms): {request.response_time_ms}"""

            # ì¶”ê°€ ì •ë³´ (ì„ íƒì‚¬í•­)
            if request.correct_answer:
                agent_input += f"\nCorrect Answer: {request.correct_answer}"
            if request.correct_keywords:
                agent_input += f"\nCorrect Keywords: {', '.join(request.correct_keywords)}"
            if request.difficulty:
                agent_input += f"\nDifficulty Level: {request.difficulty}"
            if request.category:
                agent_input += f"\nQuestion Category: {request.category}"

            agent_input += f"""

=== TASK ===
Call Tool 6 (score_and_explain) with the following parameters:
- session_id: {session_id}
- user_id: {user_id}
- question_id: {question_id}
- question_type: {question_type}
- user_answer: [User's response]
- correct_answer: [if available]
- correct_keywords: [if applicable]
- difficulty: [if available]
- category: [if available]

Tool 6 will return: is_correct (boolean), score (0-100), explanation, keyword_matches, feedback, graded_at
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = await self.executor.ainvoke({"input": agent_input})

            logger.info("âœ… ì±„ì  ì™„ë£Œ")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_score(result, request.item_id)
            logger.info(f"âœ… ì±„ì  ì„±ê³µ: score={response.score}, correct={response.correct}")

            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ScoreAnswerResponse(
                item_id=request.item_id,
                correct=False,
                score=0.0,
                explanation=f"ì±„ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )

    async def submit_answers(self, request: SubmitAnswersRequest) -> SubmitAnswersResponse:
        """
        Mode 2 Batch: Auto-grade multiple answers in one round (Tool 6).

        REQ: REQ-B-ItemGen-Batch

        ë‹¨ê³„:
            1. ê° ë‹µë³€ì— ëŒ€í•´ Tool 6 í˜¸ì¶œ (ìë™ ì±„ì )
            2. ì±„ì  ê²°ê³¼ ìˆ˜ì§‘
            3. ë¼ìš´ë“œ í†µê³„ ê³„ì‚° (í‰ê·  ì‘ë‹µì‹œê°„, ì •ë‹µë¥  ë“±)
            4. ë°°ì¹˜ ì‘ë‹µ ë°˜í™˜

        Args:
            request: SubmitAnswersRequest

        Returns:
            SubmitAnswersResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool 6 í˜¸ì¶œ ì‹¤íŒ¨: ê°œë³„ í•­ëª©ë³„ ì¬ì‹œë„
            - í†µê³„ ê³„ì‚° ì˜¤ë¥˜: ì•ˆì „í•œ ê¸°ë³¸ê°’ ì œê³µ

        """
        logger.info(f"ğŸ“ ë°°ì¹˜ ì±„ì  ì‹œì‘: round_id={request.round_id}, items={len(request.answers)}")

        try:
            per_item: list[ItemScore] = []
            response_times: list[int] = []

            # Round IDì—ì„œ session_id ì¶”ì¶œ (RoundIDGenerator í¬ë§·: session_id_round_number_timestamp)
            try:
                parsed_round = _round_id_gen.parse(request.round_id)
                session_id = parsed_round.session_id
                logger.debug(f"Extracted session_id from round_id: {session_id}")
            except Exception as e:
                # Round ID íŒŒì‹± ì‹¤íŒ¨ ì‹œ round_id ì „ì²´ë¥¼ session_idë¡œ ì‚¬ìš©
                session_id = request.round_id
                logger.warning(f"Failed to parse round_id: {e}, using full round_id as session_id")

            # 1. ê° ë‹µë³€ì„ ìˆœì°¨ ì±„ì  (ë³‘ë ¬í™”ëŠ” Phase 3)
            for answer in request.answers:
                try:
                    # ë‹¨ì¼ ì±„ì  ë©”ì„œë“œ í™œìš© (Tool 6 í•„ìˆ˜ íŒŒë¼ë¯¸í„° í¬í•¨)
                    single_request = ScoreAnswerRequest(
                        # Tool 6 í•„ìˆ˜ íŒŒë¼ë¯¸í„°
                        session_id=session_id,
                        user_id=None,  # ë°°ì¹˜ ìš”ì²­ì—ì„œëŠ” ì œê³µë˜ì§€ ì•ŠìŒ - Tool 6ì—ì„œ "unknown_user" ê¸°ë³¸ê°’ ì‚¬ìš©
                        question_id=answer.item_id,  # item_idë¥¼ question_idë¡œ ì‚¬ìš©
                        question_type="short_answer",  # ë°°ì¹˜ ì±„ì ì—ì„œ ê¸°ë³¸ê°’ - ì‹¤ì œ íƒ€ì…ì€ ì§ˆë¬¸ DBì—ì„œ ì¡°íšŒ í•„ìš”
                        # ì‚¬ìš©ì ë‹µë³€
                        user_answer=answer.user_answer,
                        # ë‚´ë¶€ ë©”íƒ€ë°ì´í„°
                        round_id=request.round_id,
                        item_id=answer.item_id,
                        response_time_ms=answer.response_time_ms,
                    )

                    result = await self.score_and_explain(single_request)

                    # ë°°ì¹˜ ì‘ë‹µ í¬ë§·ìœ¼ë¡œ ë³€í™˜
                    item_score = ItemScore(
                        item_id=result.item_id,
                        correct=result.correct,
                        score=result.score,
                        extracted_keywords=result.extracted_keywords,
                        feedback=result.feedback,
                    )
                    per_item.append(item_score)
                    response_times.append(answer.response_time_ms)

                    logger.info(f"âœ“ ë¬¸í•­ ì±„ì  ì™„ë£Œ: {answer.item_id}, score={result.score}, correct={result.correct}")

                except Exception as e:
                    logger.error(f"âŒ ë¬¸í•­ ì±„ì  ì‹¤íŒ¨: {answer.item_id}, {str(e)}")
                    # ì‹¤íŒ¨í•œ í•­ëª©ë„ ê²°ê³¼ì— í¬í•¨ (score=0)
                    item_score = ItemScore(
                        item_id=answer.item_id,
                        correct=False,
                        score=0.0,
                        feedback=f"ì±„ì  ì˜¤ë¥˜: {str(e)}",
                    )
                    per_item.append(item_score)
                    response_times.append(answer.response_time_ms)

            # 2. ë¼ìš´ë“œ í†µê³„ ê³„ì‚°
            correct_count = sum(1 for item in per_item if item.correct)
            total_count = len(per_item)
            round_score = sum(item.score for item in per_item) / total_count if total_count > 0 else 0.0
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0.0

            round_stats = RoundStats(
                avg_response_time=avg_response_time,
                correct_count=correct_count,
                total_count=total_count,
            )

            # 3. ë°°ì¹˜ ì‘ë‹µ ìƒì„±
            response = SubmitAnswersResponse(
                round_id=request.round_id,
                per_item=per_item,
                round_score=round_score,
                round_stats=round_stats,
            )

            logger.info(
                f"âœ… ë°°ì¹˜ ì±„ì  ì™„ë£Œ: "
                f"round_score={response.round_score:.1f}, "
                f"correct={correct_count}/{total_count}, "
                f"avg_time={avg_response_time:.0f}ms"
            )

            return response

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì±„ì  ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            # ë¶€ë¶„ ê²°ê³¼ë¼ë„ ë°˜í™˜í•˜ì§€ ì•Šê³  ì „ì²´ ì‹¤íŒ¨ í‘œì‹œ
            return SubmitAnswersResponse(
                round_id=request.round_id,
                per_item=[],
                round_score=0.0,
                round_stats=RoundStats(
                    avg_response_time=0.0,
                    correct_count=0,
                    total_count=len(request.answers),
                ),
            )

    def _parse_agent_output_generate(self, result: dict, round_id: str) -> GenerateQuestionsResponse:
        """
        Parse agent output for question generation (REQ-A-LangChain).

        Args:
            result: Agent output (supports both AgentExecutor and LangGraph formats)
            round_id: ë¼ìš´ë“œ ID

        Returns:
            GenerateQuestionsResponse

        ë¡œì§:
            1. _extract_tool_results()ë¡œ ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ (intermediate_steps ë˜ëŠ” messages ëª¨ë‘ ì§€ì›)
            2. nameì´ "save_generated_question"ì¸ í˜¸ì¶œì—ì„œ question ë°ì´í„° íŒŒì‹±
            3. ê° questionì„ GeneratedItemìœ¼ë¡œ ë³€í™˜
            4. ì„±ê³µ/ì‹¤íŒ¨ ê°œìˆ˜ ì§‘ê³„

        ì°¸ê³ :
            - AgentExecutor ì¶œë ¥: {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...]}
            - LangGraph ì¶œë ¥: {"messages": [...message objects...]}
            - ë‘ í¬ë§· ëª¨ë‘ _extract_tool_results()ë¡œ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬

        """
        logger.info(f"ë¬¸í•­ ìƒì„± ê²°ê³¼ íŒŒì‹± ì¤‘... round_id={round_id}")

        try:
            # 1. save_generated_question ë„êµ¬ ê²°ê³¼ ì¶”ì¶œ (í¬ë§· ë¬´ê´€)
            tool_results = self._extract_tool_results(result, "save_generated_question")
            agent_steps = len(result.get("intermediate_steps", [])) or len(result.get("messages", []))
            logger.info(f"ë„êµ¬ í˜¸ì¶œ {agent_steps}ê°œ ë°œê²¬, save_generated_question {len(tool_results)}ê°œ")

            # 2. save_generated_question ë„êµ¬ ê²°ê³¼ íŒŒì‹±
            items: list[GeneratedItem] = []
            failed_count = 0
            error_messages: list[str] = []

            for tool_name, tool_output_str in tool_results:
                if tool_name != "save_generated_question":
                    continue

                if not tool_output_str:
                    failed_count += 1
                    continue

                # JSON íŒŒì‹±
                try:
                    tool_output = json.loads(tool_output_str) if isinstance(tool_output_str, str) else tool_output_str
                except json.JSONDecodeError as e:
                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(tool_output_str)[:100]}")
                    failed_count += 1
                    error_messages.append(f"JSON decode error: {str(e)}")
                    continue

                # success í”Œë˜ê·¸ í™•ì¸
                has_error = "error" in tool_output
                is_success = tool_output.get("success", not has_error)

                if not is_success or has_error:
                    failed_count += 1
                    if "error" in tool_output:
                        error_messages.append(tool_output["error"])
                    continue

                # GeneratedItem ê°ì²´ ìƒì„±
                try:
                    # answer_schema êµ¬ì„± (Tool 5ê°€ ì œê³µí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
                    schema_from_tool = tool_output.get("answer_schema", {})
                    if isinstance(schema_from_tool, dict):
                        # Tool 5ì—ì„œ ë°˜í™˜í•œ answer_schema ì‚¬ìš©
                        answer_schema = AnswerSchema(
                            type=schema_from_tool.get(
                                "type", schema_from_tool.get("correct_key") and "exact_match" or "keyword_match"
                            ),
                            keywords=schema_from_tool.get("correct_keywords") or schema_from_tool.get("keywords"),
                            correct_answer=schema_from_tool.get("correct_key")
                            or schema_from_tool.get("correct_answer"),
                        )
                    else:
                        # Fallback to tool_output fields
                        answer_schema = AnswerSchema(
                            type=tool_output.get("answer_type", "exact_match"),
                            keywords=tool_output.get("correct_keywords"),
                            correct_answer=tool_output.get("correct_answer"),
                        )

                    item = GeneratedItem(
                        id=tool_output.get("question_id", f"q_{uuid.uuid4().hex[:8]}"),
                        type=tool_output.get("item_type", "multiple_choice"),
                        stem=tool_output.get("stem", ""),
                        choices=tool_output.get("choices"),
                        answer_schema=answer_schema,
                        difficulty=tool_output.get("difficulty", 5),
                        category=tool_output.get("category", "general"),
                        validation_score=tool_output.get("validation_score", 0.0),
                        saved_at=tool_output.get("saved_at", datetime.now(UTC).isoformat()),
                    )
                    items.append(item)
                    logger.info(f"âœ“ ë¬¸í•­ íŒŒì‹± ì„±ê³µ: {item.id}, stem={item.stem[:50] if item.stem else 'N/A'}")

                except Exception as e:
                    logger.error(f"GeneratedItem ìƒì„± ì‹¤íŒ¨: {e}")
                    failed_count += 1
                    error_messages.append(str(e))
                    continue

            # 3. ì‘ë‹µ ìƒì„±
            error_msg = " | ".join(error_messages) if error_messages else None

            response = GenerateQuestionsResponse(
                round_id=round_id,
                items=items,
                time_limit_seconds=1200,  # ê¸°ë³¸ 20ë¶„
                agent_steps=agent_steps,
                failed_count=failed_count,
                error_message=error_msg,
            )

            logger.info(f"âœ… íŒŒì‹± ì™„ë£Œ: ì„±ê³µ={len(items)}, ì‹¤íŒ¨={failed_count}, agent_steps={agent_steps}")
            return response

        except Exception as e:
            logger.error(f"âŒ íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return GenerateQuestionsResponse(
                round_id=round_id,
                items=[],
                time_limit_seconds=1200,
                agent_steps=0,
                failed_count=0,
                error_message=f"Parsing error: {str(e)}",
            )

    def _parse_agent_output_score(self, result: dict, item_id: str) -> ScoreAnswerResponse:
        """
        Parse agent output for auto-grading (REQ-A-LangChain).

        Args:
            result: Agent output (supports both AgentExecutor and LangGraph formats)
            item_id: ë¬¸í•­ ID

        Returns:
            ScoreAnswerResponse

        ë¡œì§:
            1. _extract_tool_results()ë¡œ score_and_explain í˜¸ì¶œ ì¶”ì¶œ (intermediate_steps ë˜ëŠ” messages ëª¨ë‘ ì§€ì›)
            2. Tool ì¶œë ¥ì„ JSONìœ¼ë¡œ íŒŒì‹±
            3. is_correct, score, explanation, feedback, keyword_matches ì¶”ì¶œ
            4. ScoreAnswerResponseë¡œ ë³€í™˜

        ì°¸ê³ :
            - AgentExecutor ì¶œë ¥: {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...]}
            - LangGraph ì¶œë ¥: {"messages": [...message objects...]}
            - Tool 6 (score_and_explain) ì¶œë ¥ êµ¬ì¡°:
              {
                "is_correct": bool,  # Tool 6 í˜¸í™˜
                "score": float (0-100),
                "explanation": str,
                "keyword_matches": list[str] (optional),  # Tool 6 í˜¸í™˜
                "feedback": str (optional)
              }

        """
        logger.info(f"ì±„ì  ê²°ê³¼ íŒŒì‹± ì¤‘... item_id={item_id}")

        try:
            # 1. score_and_explain ë„êµ¬ ê²°ê³¼ ì¶”ì¶œ (í¬ë§· ë¬´ê´€)
            tool_results = self._extract_tool_results(result, "score_and_explain")
            if not tool_results:
                logger.warning("score_and_explain ë„êµ¬ í˜¸ì¶œì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation="score_and_explain tool not executed",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 2. ì²« ë²ˆì§¸ score_and_explain ê²°ê³¼ ì‚¬ìš©
            _, score_tool_output = tool_results[0]
            if not score_tool_output:
                logger.warning("score_and_explain ë„êµ¬ ì¶œë ¥ì´ ë¹„ì–´ìˆìŒ")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation="score_and_explain output is empty",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 3. JSON íŒŒì‹±
            try:
                tool_output = json.loads(score_tool_output) if isinstance(score_tool_output, str) else score_tool_output
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(score_tool_output)[:100]}")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation=f"JSON decode error: {str(e)}",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 4. ScoreAnswerResponse ìƒì„± (Tool 6 í˜¸í™˜)
            # Tool 6ëŠ” "is_correct"ë¥¼ ë°˜í™˜í•˜ë©°, "keyword_matches"ë¡œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì œê³µ
            is_correct_from_tool = tool_output.get("is_correct", tool_output.get("correct", False))
            keyword_matches = tool_output.get("keyword_matches", tool_output.get("extracted_keywords", []))

            response = ScoreAnswerResponse(
                item_id=item_id,
                is_correct=is_correct_from_tool,  # Tool 6 í˜¸í™˜
                correct=is_correct_from_tool,  # API í˜¸í™˜ (ê°™ì€ ê°’)
                score=float(tool_output.get("score", 0)),
                explanation=tool_output.get("explanation", ""),
                feedback=tool_output.get("feedback"),
                keyword_matches=keyword_matches,  # Tool 6 í˜¸í™˜
                extracted_keywords=keyword_matches,  # API í˜¸í™˜ (ê°™ì€ ê°’)
                graded_at=tool_output.get("graded_at", datetime.now(UTC).isoformat()),
            )

            logger.info(f"âœ… ì±„ì  íŒŒì‹± ì™„ë£Œ: correct={response.correct}, score={response.score}")
            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return ScoreAnswerResponse(
                item_id=item_id,
                is_correct=False,
                correct=False,
                score=0.0,
                explanation=f"Parsing error: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )


# ============================================================================
# Factory Function
# ============================================================================


async def create_agent() -> ItemGenAgent:
    """
    Create ItemGenAgent factory function.

    Returns:
        ItemGenAgent: ì´ˆê¸°í™”ëœ ì—ì´ì „íŠ¸

    ì‚¬ìš©:
        ```python
        agent = await create_agent()
        ```

    """
    return ItemGenAgent()
