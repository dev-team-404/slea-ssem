"""
Item-Gen-Agent: LangChain ReAct ê¸°ë°˜ ììœ¨ AI ì—ì´ì „íŠ¸.

REQ: REQ-A-ItemGen

ê°œìš”:
    LangChainì˜ ìµœì‹  Agent íŒ¨í„´ (ReAct)ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒÂ·í™œìš©í•˜ëŠ”
    AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. Mode 1 (ë¬¸í•­ ìƒì„±)ê³¼ Mode 2 (ìë™ ì±„ì ) ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

ì°¸ê³ :
    - LangChain ê³µì‹ ë¬¸ì„œ: https://python.langchain.com/docs/concepts/agents
    - create_react_agent: https://python.langchain.com/api_reference/langchain/agents/
    - ìµœì‹  API ë²„ì „: LangChain 0.3.x+

í’ˆì§ˆ ê¸°ì¤€:
    - íŒ€ ë™ë£Œ ì°¸ê³  ì½”ë“œ (ë†’ì€ ìˆ˜ì¤€ì˜ ë¬¸ì„œí™”)
    - ê³µì‹ ë¬¸ì„œ ì˜ˆì‹œ ê¸°ë°˜ êµ¬í˜„
    - íƒ€ì… íŒíŠ¸ & ì—ëŸ¬ ì²˜ë¦¬ ëª…ì‹œ
"""

import json
import logging
import uuid
from datetime import UTC, datetime

from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel, Field

from src.agent.config import AGENT_CONFIG, create_llm
from src.agent.fastmcp_server import TOOLS
from src.agent.prompts.react_prompt import get_react_prompt

logger = logging.getLogger(__name__)


# ============================================================================
# Pydantic Schemas (ì…ì¶œë ¥ ë°ì´í„° ê³„ì•½)
# ============================================================================


class GenerateQuestionsRequest(BaseModel):
    """ë¬¸í•­ ìƒì„± ìš”ì²­ (REQ: POST /api/v1/items/generate)."""

    survey_id: str = Field(..., description="ì„¤ë¬¸ ID")
    round_idx: int = Field(..., ge=1, description="ë¼ìš´ë“œ ë²ˆí˜¸ (1-based)")
    prev_answers: list[dict] | None = Field(default=None, description="ì´ì „ ë¼ìš´ë“œ ë‹µë³€ (ì ì‘í˜• í…ŒìŠ¤íŠ¸ìš©)")


class AnswerSchema(BaseModel):
    """ë‹µë³€ ê²€ì¦ ìŠ¤í‚¤ë§ˆ."""

    type: str = Field(..., description="ë‹µë³€ ìœ í˜• (exact_match | keyword_match | semantic_match)")
    keywords: list[str] | None = Field(default=None, description="ì •ë‹µ í‚¤ì›Œë“œ (ì£¼ê´€ì‹ìš©)")
    correct_answer: str | None = Field(default=None, description="ì •ë‹µ (ê°ê´€ì‹/OXìš©)")


class GeneratedItem(BaseModel):
    """ìƒì„±ëœ ë¬¸í•­ ì•„ì´í…œ."""

    id: str = Field(..., description="ë¬¸í•­ ID (UUID)")
    type: str = Field(..., description="ë¬¸í•­ ìœ í˜• (multiple_choice | true_false | short_answer)")
    stem: str = Field(..., description="ë¬¸í•­ ë‚´ìš©")
    choices: list[str] | None = Field(default=None, description="ê°ê´€ì‹ ì„ íƒì§€")
    answer_schema: AnswerSchema = Field(..., description="ë‹µë³€ ê²€ì¦ ìŠ¤í‚¤ë§ˆ")
    difficulty: int = Field(..., ge=1, le=10, description="ë‚œì´ë„ (1~10)")
    category: str = Field(..., description="ë¬¸í•­ ì¹´í…Œê³ ë¦¬")
    validation_score: float = Field(default=0.0, ge=0, le=1, description="ê²€ì¦ ì ìˆ˜ (Tool 4) - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    saved_at: str | None = Field(default=None, description="ì €ì¥ ì‹œê°„ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")


class GenerateQuestionsResponse(BaseModel):
    """ë¬¸í•­ ìƒì„± ì‘ë‹µ (REQ: POST /api/v1/items/generate)."""

    round_id: str = Field(..., description="ìƒì„±ëœ ë¼ìš´ë“œ ID")
    items: list[GeneratedItem] = Field(..., description="ìƒì„±ëœ ë¬¸í•­ ëª©ë¡")
    time_limit_seconds: int = Field(default=1200, description="ì‹œê°„ ì œí•œ (ì´ˆ, ê¸°ë³¸ 20ë¶„)")
    agent_steps: int = Field(default=0, description="ì—ì´ì „íŠ¸ ë°˜ë³µ íšŸìˆ˜ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    failed_count: int = Field(default=0, description="ì‹¤íŒ¨í•œ ë¬¸í•­ ê°œìˆ˜ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    error_message: str | None = Field(default=None, description="ì—ëŸ¬ ë©”ì‹œì§€")


class ScoreAnswerRequest(BaseModel):
    """ìë™ ì±„ì  ìš”ì²­ (ë‹¨ì¼ ì²˜ë¦¬, Phase 1)."""

    round_id: str = Field(..., description="ë¼ìš´ë“œ ID")
    item_id: str = Field(..., description="ë¬¸í•­ ID")
    user_answer: str = Field(..., description="ì‘ì‹œìì˜ ë‹µë³€")
    response_time_ms: int = Field(default=0, ge=0, description="ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")


class ScoreAnswerResponse(BaseModel):
    """ìë™ ì±„ì  ì‘ë‹µ (ë‹¨ì¼ ì²˜ë¦¬, Phase 1)."""

    item_id: str = Field(..., description="ë¬¸í•­ ID")
    correct: bool = Field(..., description="ì •ë‹µ ì—¬ë¶€")
    score: float = Field(..., ge=0, le=100, description="ì ìˆ˜ (0~100)")
    explanation: str = Field(..., description="ì •ë‹µ í•´ì„¤")
    feedback: str | None = Field(default=None, description="ë¶€ë¶„ ì •ë‹µ í”¼ë“œë°±")
    extracted_keywords: list[str] = Field(default_factory=list, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ (ì£¼ê´€ì‹)")
    graded_at: str = Field(..., description="ì±„ì  ì‹œê°„")


# ============================================================================
# Batch Scoring Models (Phase 2)
# ============================================================================


class UserAnswer(BaseModel):
    """ì‚¬ìš©ì ë‹µë³€ (ë°°ì¹˜)."""

    item_id: str = Field(..., description="ë¬¸í•­ ID")
    user_answer: str = Field(..., description="ì‚¬ìš©ì ë‹µë³€")
    response_time_ms: int = Field(default=0, ge=0, description="ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")


class SubmitAnswersRequest(BaseModel):
    """ë°°ì¹˜ ì±„ì  ìš”ì²­ (REQ: POST /api/v1/scoring/submit-answers)."""

    round_id: str = Field(..., description="ë¼ìš´ë“œ ID")
    answers: list[UserAnswer] = Field(..., description="ì‚¬ìš©ì ë‹µë³€ ë°°ì¹˜ (1-50ê°œ)")


class ItemScore(BaseModel):
    """ì±„ì ëœ ë¬¸í•­ (ë°°ì¹˜ ì‘ë‹µ)."""

    item_id: str = Field(..., description="ë¬¸í•­ ID")
    correct: bool = Field(..., description="ì •ë‹µ ì—¬ë¶€")
    score: float = Field(..., ge=0, le=100, description="ì ìˆ˜ (0~100)")
    extracted_keywords: list[str] = Field(default_factory=list, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ (ì£¼ê´€ì‹)")
    feedback: str | None = Field(default=None, description="ë¶€ë¶„ ì •ë‹µ í”¼ë“œë°±")


class RoundStats(BaseModel):
    """ë¼ìš´ë“œ í†µê³„."""

    avg_response_time: float = Field(..., ge=0, description="í‰ê·  ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")
    correct_count: int = Field(..., ge=0, description="ì •ë‹µ ê°œìˆ˜")
    total_count: int = Field(..., ge=1, description="ì „ì²´ ë¬¸í•­ ê°œìˆ˜")


class SubmitAnswersResponse(BaseModel):
    """ë°°ì¹˜ ì±„ì  ì‘ë‹µ (REQ: POST /api/v1/scoring/submit-answers)."""

    round_id: str = Field(..., description="ë¼ìš´ë“œ ID")
    per_item: list[ItemScore] = Field(..., description="ë¬¸í•­ë³„ ì±„ì  ê²°ê³¼")
    round_score: float = Field(..., ge=0, le=100, description="ë¼ìš´ë“œ ì´ì ")
    round_stats: RoundStats = Field(..., description="ë¼ìš´ë“œ í†µê³„")


# ============================================================================
# ItemGenAgent Main Class
# ============================================================================


class ItemGenAgent:
    """
    LangChain AgentExecutor ê¸°ë°˜ Item-Gen-Agent.

    ì„¤ëª…:
        - LangChainì˜ create_tool_calling_agent() API ì‚¬ìš©
        - AgentExecutorë¡œ ë„êµ¬ í˜¸ì¶œ ë° ì—ëŸ¬ ì²˜ë¦¬ ê´€ë¦¬
        - Tool Calling ë°©ì‹ (ìµœì‹  LLM ëª¨ë¸ ìµœì í™”)
        - êµ¬ì¡°í™”ëœ ì…ì¶œë ¥ (Pydantic)
        - ìƒì„¸í•œ ë¡œê¹… (ë””ë²„ê¹…)

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = ItemGenAgent()

        # Mode 1: ë¬¸í•­ ìƒì„±
        request = GenerateQuestionsRequest(
            survey_id="survey_123",
            round_idx=1,
            prev_answers=None
        )
        response = await agent.generate_questions(request)

        # Mode 2: ìë™ ì±„ì 
        score_request = ScoreAnswerRequest(
            round_id="round_123",
            item_id="item_456",
            user_answer="The answer is..."
        )
        score_response = await agent.score_and_explain(score_request)
        ```

    ì°¸ê³ :
        - LangChain ê³µì‹: https://python.langchain.com/docs/concepts/agents
        - create_tool_calling_agent: Tool Calling íŒ¨í„´ êµ¬í˜„ (ìµœì‹  LLM ìµœì í™”)
        - AgentExecutor: max_iterations, early_stopping_method, ì—ëŸ¬ ì²˜ë¦¬
    """

    def __init__(self) -> None:
        """
        Initialize ItemGenAgent with LangGraph create_react_agent.

        ë‹¨ê³„:
            1. LLM ìƒì„± (Google Gemini)
            2. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            3. FastMCP ë„êµ¬ ë“±ë¡
            4. create_react_agent()ë¡œ ì—ì´ì „íŠ¸ ìƒì„± (ìµœì‹  Tool Calling ì§€ì›)

        ì—ëŸ¬ ì²˜ë¦¬:
            - GEMINI_API_KEY ì—†ìŒ: ValueError
            - LLM ì´ˆê¸°í™” ì‹¤íŒ¨: ë¡œê·¸ + ì¬ì‹œë„
        """
        logger.info("ItemGenAgent ì´ˆê¸°í™” ì¤‘...")

        try:
            # 1. LLM ìƒì„±
            self.llm = create_llm()
            logger.info("âœ“ LLM (Google Gemini) ìƒì„± ì™„ë£Œ")

            # 2. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            self.prompt = get_react_prompt()
            logger.info("âœ“ ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")

            # 3. ë„êµ¬ ëª©ë¡ (6ê°œ)
            self.tools = TOOLS
            logger.info(f"âœ“ {len(self.tools)}ê°œ ë„êµ¬ ë“±ë¡ ì™„ë£Œ: {[t.name for t in self.tools]}")

            # 4. create_react_agent() - LangGraph ìµœì‹  Tool Calling ì§€ì›
            # LangGraphì˜ create_react_agentëŠ” ìµœì‹  LLMì˜ Tool Calling ê¸°ëŠ¥ì„ ìë™ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.
            # ReAct íŒ¨í„´: Thought â†’ Action â†’ Observationì„ ë°˜ë³µí•˜ë©° ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            # AGENT_CONFIGì˜ max_iterations, early_stopping_method, handle_parsing_errorsëŠ”
            # create_react_agentì˜ ë˜í¼ë¡œ í™œìš©ë˜ê±°ë‚˜, CompiledStateGraph ì‹¤í–‰ ì‹œ configë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
            self.executor = create_react_agent(
                model=self.llm,
                tools=self.tools,
                prompt=self.prompt,
                debug=AGENT_CONFIG.get("verbose", False),
                version="v2",  # ìµœì‹  LangGraph v2 API ì‚¬ìš©
            )
            logger.info("âœ“ ReAct ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ (Tool Calling ìµœì í™” v2)")

            logger.info("âœ… ItemGenAgent ì´ˆê¸°í™” ì„±ê³µ")

        except Exception as e:
            logger.error(f"âŒ ItemGenAgent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def generate_questions(self, request: GenerateQuestionsRequest) -> GenerateQuestionsResponse:
        """
        Mode 1: Generate questions (Tool 1-5 auto-select).

        REQ: REQ-A-Mode1-Pipeline

        ë‹¨ê³„:
            1. ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (Tool 1)
            2. í…œí”Œë¦¿ ê²€ìƒ‰ (Tool 2) - ì„ íƒì‚¬í•­
            3. ë‚œì´ë„ë³„ í‚¤ì›Œë“œ ì¡°íšŒ (Tool 3)
            4. LLMìœ¼ë¡œ ë¬¸í•­ ìƒì„±
            5. ê° ë¬¸í•­ ê²€ì¦ (Tool 4)
            6. ê²€ì¦ í†µê³¼ ë¬¸í•­ ì €ì¥ (Tool 5)

        Args:
            request: GenerateQuestionsRequest

        Returns:
            GenerateQuestionsResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool í˜¸ì¶œ ì‹¤íŒ¨: ìë™ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
            - ì—ì´ì „íŠ¸ ìµœëŒ€ ë°˜ë³µ: ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜
            - LLM ì˜¤ë¥˜: ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨

        ì°¸ê³ :
            - AgentExecutor: ë„êµ¬ í˜¸ì¶œ ë° Tool Calling ë£¨í”„ ìë™ ê´€ë¦¬
            - intermediate_steps: ì—ì´ì „íŠ¸ì˜ ê° ë„êµ¬ í˜¸ì¶œ ì¶”ì 

        """
        logger.info(f"ğŸ“ ë¬¸í•­ ìƒì„± ì‹œì‘: survey_id={request.survey_id}, round_idx={request.round_idx}")

        try:
            # ë¼ìš´ë“œ ID ìƒì„±
            round_id = f"round_{request.survey_id}_{request.round_idx}_{uuid.uuid4().hex[:8]}"

            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„±
            agent_input = f"""
Generate high-quality exam questions for the following survey.
Survey ID: {request.survey_id}
Round: {request.round_idx}
Previous Answers: {json.dumps(request.prev_answers) if request.prev_answers else "None (First round)"}

Follow these steps:
1. Get survey context and user profile (Tool 1)
2. Search question templates for similar items (Tool 2) if available
3. Get keywords for adaptive difficulty (Tool 3)
4. Generate new questions with appropriate difficulty
5. Validate each question (Tool 4)
6. Save validated questions (Tool 5) with round_id={round_id}

Important:
- Generate questions with appropriate answer_schema (exact_match, keyword_match, or semantic_match)
- Each question must include: id, type, stem, choices (if MC), answer_schema, difficulty, category
- Return all saved questions with validation scores
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (Tool Calling ë£¨í”„)
            # AgentExecutorê°€ ë‹¤ìŒì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰:
            # - Agent Thought: ì—ì´ì „íŠ¸ ì¶”ë¡ 
            # - Action: ë„êµ¬ ì„ íƒ ë° í˜¸ì¶œ
            # - Observation: ë„êµ¬ ê²°ê³¼
            # - ë°˜ë³µ ë˜ëŠ” ì¢…ë£Œ
            result = await self.executor.ainvoke({"input": agent_input})

            logger.info("âœ… ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_generate(result, round_id)
            logger.info(f"âœ… ë¬¸í•­ ìƒì„± ì„±ê³µ: {len(response.items)}ê°œ ìƒì„±")

            return response

        except Exception as e:
            logger.error(f"âŒ ë¬¸í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return GenerateQuestionsResponse(
                round_id=f"round_error_{uuid.uuid4().hex[:8]}",
                items=[],
                time_limit_seconds=1200,
                agent_steps=0,
                failed_count=0,
                error_message=str(e),
            )

    async def score_and_explain(self, request: ScoreAnswerRequest) -> ScoreAnswerResponse:
        """
        Mode 2: Auto-grade answers (Tool 6).

        REQ: REQ-A-Mode2-Pipeline

        ë‹¨ê³„:
            1. Tool 6 í˜¸ì¶œ (ìë™ ì±„ì  & í•´ì„¤ ìƒì„±)

        Args:
            request: ScoreAnswerRequest

        Returns:
            ScoreAnswerResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool 6 í˜¸ì¶œ ì‹¤íŒ¨: ì¬ì‹œë„ 3íšŒ
            - LLM ì˜¤ë¥˜: ê¸°ë³¸ ì ìˆ˜ 0 ë°˜í™˜

        ì°¸ê³ :
            - Tool 6: ê°ê´€ì‹/OX (ì •í™• ë§¤ì¹­) vs ì£¼ê´€ì‹ (LLM í‰ê°€)
            - ì±„ì  ê¸°ì¤€: >= 80 â†’ ì •ë‹µ, 70~79 â†’ ë¶€ë¶„ ì •ë‹µ, < 70 â†’ ì˜¤ë‹µ

        """
        logger.info(f"ğŸ“‹ ìë™ ì±„ì  ì‹œì‘: round_id={request.round_id}, item_id={request.item_id}")

        try:
            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„±
            agent_input = f"""
Score and explain the following answer:

Round ID: {request.round_id}
Item ID: {request.item_id}
User Answer: {request.user_answer}
Response Time (ms): {request.response_time_ms}

Use Tool 6 (score_and_explain) to:
1. Score the answer (0~100)
2. Generate explanation
3. Provide feedback if needed
4. Extract keywords if applicable (for short answer)

Return: correct (boolean), score (0-100), explanation, feedback, extracted_keywords
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = await self.executor.ainvoke({"input": agent_input})

            logger.info("âœ… ì±„ì  ì™„ë£Œ")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_score(result, request.item_id)
            logger.info(f"âœ… ì±„ì  ì„±ê³µ: score={response.score}, correct={response.correct}")

            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ScoreAnswerResponse(
                item_id=request.item_id,
                correct=False,
                score=0.0,
                explanation=f"ì±„ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )

    async def submit_answers(self, request: SubmitAnswersRequest) -> SubmitAnswersResponse:
        """
        Mode 2 Batch: Auto-grade multiple answers in one round (Tool 6).

        REQ: REQ-B-ItemGen-Batch

        ë‹¨ê³„:
            1. ê° ë‹µë³€ì— ëŒ€í•´ Tool 6 í˜¸ì¶œ (ìë™ ì±„ì )
            2. ì±„ì  ê²°ê³¼ ìˆ˜ì§‘
            3. ë¼ìš´ë“œ í†µê³„ ê³„ì‚° (í‰ê·  ì‘ë‹µì‹œê°„, ì •ë‹µë¥  ë“±)
            4. ë°°ì¹˜ ì‘ë‹µ ë°˜í™˜

        Args:
            request: SubmitAnswersRequest

        Returns:
            SubmitAnswersResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool 6 í˜¸ì¶œ ì‹¤íŒ¨: ê°œë³„ í•­ëª©ë³„ ì¬ì‹œë„
            - í†µê³„ ê³„ì‚° ì˜¤ë¥˜: ì•ˆì „í•œ ê¸°ë³¸ê°’ ì œê³µ

        """
        logger.info(f"ğŸ“ ë°°ì¹˜ ì±„ì  ì‹œì‘: round_id={request.round_id}, items={len(request.answers)}")

        try:
            per_item: list[ItemScore] = []
            response_times: list[int] = []

            # 1. ê° ë‹µë³€ì„ ìˆœì°¨ ì±„ì  (ë³‘ë ¬í™”ëŠ” Phase 3)
            for answer in request.answers:
                try:
                    # ë‹¨ì¼ ì±„ì  ë©”ì„œë“œ í™œìš©
                    single_request = ScoreAnswerRequest(
                        round_id=request.round_id,
                        item_id=answer.item_id,
                        user_answer=answer.user_answer,
                        response_time_ms=answer.response_time_ms,
                    )

                    result = await self.score_and_explain(single_request)

                    # ë°°ì¹˜ ì‘ë‹µ í¬ë§·ìœ¼ë¡œ ë³€í™˜
                    item_score = ItemScore(
                        item_id=result.item_id,
                        correct=result.correct,
                        score=result.score,
                        extracted_keywords=result.extracted_keywords,
                        feedback=result.feedback,
                    )
                    per_item.append(item_score)
                    response_times.append(answer.response_time_ms)

                    logger.info(f"âœ“ ë¬¸í•­ ì±„ì  ì™„ë£Œ: {answer.item_id}, score={result.score}, correct={result.correct}")

                except Exception as e:
                    logger.error(f"âŒ ë¬¸í•­ ì±„ì  ì‹¤íŒ¨: {answer.item_id}, {str(e)}")
                    # ì‹¤íŒ¨í•œ í•­ëª©ë„ ê²°ê³¼ì— í¬í•¨ (score=0)
                    item_score = ItemScore(
                        item_id=answer.item_id,
                        correct=False,
                        score=0.0,
                        feedback=f"ì±„ì  ì˜¤ë¥˜: {str(e)}",
                    )
                    per_item.append(item_score)
                    response_times.append(answer.response_time_ms)

            # 2. ë¼ìš´ë“œ í†µê³„ ê³„ì‚°
            correct_count = sum(1 for item in per_item if item.correct)
            total_count = len(per_item)
            round_score = sum(item.score for item in per_item) / total_count if total_count > 0 else 0.0
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0.0

            round_stats = RoundStats(
                avg_response_time=avg_response_time,
                correct_count=correct_count,
                total_count=total_count,
            )

            # 3. ë°°ì¹˜ ì‘ë‹µ ìƒì„±
            response = SubmitAnswersResponse(
                round_id=request.round_id,
                per_item=per_item,
                round_score=round_score,
                round_stats=round_stats,
            )

            logger.info(
                f"âœ… ë°°ì¹˜ ì±„ì  ì™„ë£Œ: "
                f"round_score={response.round_score:.1f}, "
                f"correct={correct_count}/{total_count}, "
                f"avg_time={avg_response_time:.0f}ms"
            )

            return response

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì±„ì  ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            # ë¶€ë¶„ ê²°ê³¼ë¼ë„ ë°˜í™˜í•˜ì§€ ì•Šê³  ì „ì²´ ì‹¤íŒ¨ í‘œì‹œ
            return SubmitAnswersResponse(
                round_id=request.round_id,
                per_item=[],
                round_score=0.0,
                round_stats=RoundStats(
                    avg_response_time=0.0,
                    correct_count=0,
                    total_count=len(request.answers),
                ),
            )

    def _parse_agent_output_generate(self, result: dict, round_id: str) -> GenerateQuestionsResponse:
        """
        Parse agent output for question generation (REQ-A-LangChain).

        Args:
            result: AgentExecutorì˜ ì¶œë ¥
            round_id: ë¼ìš´ë“œ ID

        Returns:
            GenerateQuestionsResponse

        ë¡œì§:
            1. result["intermediate_steps"]ì—ì„œ ëª¨ë“  ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ
            2. nameì´ "save_generated_question"ì¸ í˜¸ì¶œì—ì„œ question ë°ì´í„° íŒŒì‹±
            3. ê° questionì„ GeneratedItemìœ¼ë¡œ ë³€í™˜
            4. ì„±ê³µ/ì‹¤íŒ¨ ê°œìˆ˜ ì§‘ê³„

        ì°¸ê³ :
            - AgentExecutor ì¶œë ¥: {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...]}
            - intermediate_stepsëŠ” (tool_name: str, tool_output: str) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
            - Tool ì¶œë ¥ì€ ëŒ€ë¶€ë¶„ JSON ë¬¸ìì—´ í˜•íƒœ

        """
        logger.info(f"ë¬¸í•­ ìƒì„± ê²°ê³¼ íŒŒì‹± ì¤‘... round_id={round_id}")

        try:
            # 1. intermediate_steps ì¶”ì¶œ (ë„êµ¬ í˜¸ì¶œ íˆìŠ¤í† ë¦¬)
            intermediate_steps = result.get("intermediate_steps", [])
            agent_steps = len(intermediate_steps)
            logger.info(f"ë„êµ¬ í˜¸ì¶œ {agent_steps}ê°œ ë°œê²¬")

            # 2. save_generated_question ë„êµ¬ ê²°ê³¼ íŒŒì‹±
            items: list[GeneratedItem] = []
            failed_count = 0
            error_messages: list[str] = []

            for tool_name, tool_output_str in intermediate_steps:
                if tool_name != "save_generated_question":
                    continue

                if not tool_output_str:
                    failed_count += 1
                    continue

                # JSON íŒŒì‹±
                try:
                    tool_output = json.loads(tool_output_str) if isinstance(tool_output_str, str) else tool_output_str
                except json.JSONDecodeError as e:
                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(tool_output_str)[:100]}")
                    failed_count += 1
                    error_messages.append(f"JSON decode error: {str(e)}")
                    continue

                # success í”Œë˜ê·¸ í™•ì¸
                has_error = "error" in tool_output
                is_success = tool_output.get("success", not has_error)

                if not is_success or has_error:
                    failed_count += 1
                    if "error" in tool_output:
                        error_messages.append(tool_output["error"])
                    continue

                # GeneratedItem ê°ì²´ ìƒì„±
                try:
                    # answer_schema êµ¬ì„±
                    answer_schema = AnswerSchema(
                        type=tool_output.get("answer_type", "exact_match"),
                        keywords=tool_output.get("correct_keywords"),
                        correct_answer=tool_output.get("correct_answer"),
                    )

                    item = GeneratedItem(
                        id=tool_output.get("question_id", f"q_{uuid.uuid4().hex[:8]}"),
                        type=tool_output.get("item_type", "multiple_choice"),
                        stem=tool_output.get("stem", ""),
                        choices=tool_output.get("choices"),
                        answer_schema=answer_schema,
                        difficulty=tool_output.get("difficulty", 5),
                        category=tool_output.get("category", "general"),
                        validation_score=tool_output.get("validation_score", 0.0),
                        saved_at=tool_output.get("saved_at", datetime.now(UTC).isoformat()),
                    )
                    items.append(item)
                    logger.info(f"âœ“ ë¬¸í•­ íŒŒì‹± ì„±ê³µ: {item.id}")

                except Exception as e:
                    logger.error(f"GeneratedItem ìƒì„± ì‹¤íŒ¨: {e}")
                    failed_count += 1
                    error_messages.append(str(e))
                    continue

            # 3. ì‘ë‹µ ìƒì„±
            error_msg = " | ".join(error_messages) if error_messages else None

            response = GenerateQuestionsResponse(
                round_id=round_id,
                items=items,
                time_limit_seconds=1200,  # ê¸°ë³¸ 20ë¶„
                agent_steps=agent_steps,
                failed_count=failed_count,
                error_message=error_msg,
            )

            logger.info(f"âœ… íŒŒì‹± ì™„ë£Œ: ì„±ê³µ={len(items)}, ì‹¤íŒ¨={failed_count}, agent_steps={agent_steps}")
            return response

        except Exception as e:
            logger.error(f"âŒ íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return GenerateQuestionsResponse(
                round_id=round_id,
                items=[],
                time_limit_seconds=1200,
                agent_steps=0,
                failed_count=0,
                error_message=f"Parsing error: {str(e)}",
            )

    def _parse_agent_output_score(self, result: dict, item_id: str) -> ScoreAnswerResponse:
        """
        Parse agent output for auto-grading (REQ-A-LangChain).

        Args:
            result: AgentExecutorì˜ ì¶œë ¥
            item_id: ë¬¸í•­ ID

        Returns:
            ScoreAnswerResponse

        ë¡œì§:
            1. result["intermediate_steps"]ì—ì„œ tool_name="score_and_explain" í˜¸ì¶œ ì°¾ê¸°
            2. Tool ì¶œë ¥ì„ JSONìœ¼ë¡œ íŒŒì‹±
            3. correct, score, explanation, feedback, extracted_keywords ì¶”ì¶œ
            4. ScoreAnswerResponseë¡œ ë³€í™˜

        ì°¸ê³ :
            - AgentExecutor ì¶œë ¥: {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...]}
            - Tool 6 (score_and_explain) ì¶œë ¥ êµ¬ì¡°:
              {
                "correct": bool,
                "score": float (0-100),
                "explanation": str,
                "extracted_keywords": list[str] (optional),
                "feedback": str (optional)
              }

        """
        logger.info(f"ì±„ì  ê²°ê³¼ íŒŒì‹± ì¤‘... item_id={item_id}")

        try:
            # 1. intermediate_stepsì—ì„œ score_and_explain í˜¸ì¶œ ì°¾ê¸°
            intermediate_steps = result.get("intermediate_steps", [])
            if not intermediate_steps:
                logger.warning("intermediate_stepsê°€ ë¹„ì–´ìˆìŒ")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    correct=False,
                    score=0.0,
                    explanation="No tool steps found",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 2. score_and_explain ë„êµ¬ í˜¸ì¶œ ì°¾ê¸°
            score_tool_output = None
            for tool_name, tool_output_str in intermediate_steps:
                if tool_name == "score_and_explain":
                    score_tool_output = tool_output_str
                    break

            if not score_tool_output:
                logger.warning("score_and_explain ë„êµ¬ í˜¸ì¶œì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    correct=False,
                    score=0.0,
                    explanation="score_and_explain tool not executed",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 3. JSON íŒŒì‹±
            try:
                tool_output = json.loads(score_tool_output) if isinstance(score_tool_output, str) else score_tool_output
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(score_tool_output)[:100]}")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    correct=False,
                    score=0.0,
                    explanation=f"JSON decode error: {str(e)}",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 4. ScoreAnswerResponse ìƒì„±
            response = ScoreAnswerResponse(
                item_id=item_id,
                correct=tool_output.get("correct", False),
                score=float(tool_output.get("score", 0)),
                explanation=tool_output.get("explanation", ""),
                feedback=tool_output.get("feedback"),
                extracted_keywords=tool_output.get("extracted_keywords", []),
                graded_at=tool_output.get("graded_at", datetime.now(UTC).isoformat()),
            )

            logger.info(f"âœ… ì±„ì  íŒŒì‹± ì™„ë£Œ: correct={response.correct}, score={response.score}")
            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return ScoreAnswerResponse(
                item_id=item_id,
                correct=False,
                score=0.0,
                explanation=f"Parsing error: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )


# ============================================================================
# Factory Function
# ============================================================================


async def create_agent() -> ItemGenAgent:
    """
    Create ItemGenAgent factory function.

    Returns:
        ItemGenAgent: ì´ˆê¸°í™”ëœ ì—ì´ì „íŠ¸

    ì‚¬ìš©:
        ```python
        agent = await create_agent()
        ```

    """
    return ItemGenAgent()
