"""
Item-Gen-Agent: LangChain ReAct ê¸°ë°˜ ììœ¨ AI ì—ì´ì „íŠ¸.

REQ: REQ-A-ItemGen

ê°œìš”:
    LangChainì˜ ìµœì‹  Agent íŒ¨í„´ (ReAct)ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒÂ·í™œìš©í•˜ëŠ”
    AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. Mode 1 (ë¬¸í•­ ìƒì„±)ê³¼ Mode 2 (ìë™ ì±„ì ) ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

ì°¸ê³ :
    - LangChain ê³µì‹ ë¬¸ì„œ: https://python.langchain.com/docs/concepts/agents
    - create_react_agent: https://python.langchain.com/api_reference/langchain/agents/
    - ìµœì‹  API ë²„ì „: LangChain 0.3.x+

í’ˆì§ˆ ê¸°ì¤€:
    - íŒ€ ë™ë£Œ ì°¸ê³  ì½”ë“œ (ë†’ì€ ìˆ˜ì¤€ì˜ ë¬¸ì„œí™”)
    - ê³µì‹ ë¬¸ì„œ ì˜ˆì‹œ ê¸°ë°˜ êµ¬í˜„
    - íƒ€ì… íŒíŠ¸ & ì—ëŸ¬ ì²˜ë¦¬ ëª…ì‹œ
"""

import json
import logging
import uuid
from datetime import UTC, datetime

from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel, Field

from src.agent.config import AGENT_CONFIG, create_llm
from src.agent.fastmcp_server import TOOLS
from src.agent.prompts.react_prompt import get_react_prompt
from src.agent.round_id_generator import RoundIDGenerator

logger = logging.getLogger(__name__)

# Round ID generator instance (singleton pattern)
_round_id_gen = RoundIDGenerator()


# ============================================================================
# Pydantic Schemas (ì…ì¶œë ¥ ë°ì´í„° ê³„ì•½)
# ============================================================================


class GenerateQuestionsRequest(BaseModel):
    """ë¬¸í•­ ìƒì„± ìš”ì²­ (REQ: POST /api/v1/items/generate)."""

    session_id: str = Field(..., description="í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ID (Backend Serviceê°€ ìƒì„±)")
    survey_id: str = Field(..., description="ì„¤ë¬¸ ID")
    round_idx: int = Field(..., ge=1, description="ë¼ìš´ë“œ ë²ˆí˜¸ (1-based)")
    prev_answers: list[dict] | None = Field(default=None, description="ì´ì „ ë¼ìš´ë“œ ë‹µë³€ (ì ì‘í˜• í…ŒìŠ¤íŠ¸ìš©)")
    question_count: int = Field(default=5, ge=1, le=20, description="ìƒì„±í•  ë¬¸í•­ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5, í…ŒìŠ¤íŠ¸: 2)")
    question_types: list[str] | None = Field(
        default=None, description="ìƒì„±í•  ë¬¸í•­ ìœ í˜• (multiple_choice | true_false | short_answer), Noneì´ë©´ ëª¨ë‘ ìƒì„±"
    )
    domain: str = Field(default="AI", description="ë¬¸í•­ ë„ë©”ì¸/ì£¼ì œ (ì˜ˆ: AI, food, science)")


class AnswerSchema(BaseModel):
    """ë‹µë³€ ê²€ì¦ ìŠ¤í‚¤ë§ˆ."""

    type: str = Field(..., description="ë‹µë³€ ìœ í˜• (exact_match | keyword_match | semantic_match)")
    keywords: list[str] | None = Field(default=None, description="ì •ë‹µ í‚¤ì›Œë“œ (ì£¼ê´€ì‹ìš©)")
    correct_answer: str | None = Field(default=None, description="ì •ë‹µ (ê°ê´€ì‹/OXìš©)")


class GeneratedItem(BaseModel):
    """ìƒì„±ëœ ë¬¸í•­ ì•„ì´í…œ."""

    id: str = Field(..., description="ë¬¸í•­ ID (UUID)")
    type: str = Field(..., description="ë¬¸í•­ ìœ í˜• (multiple_choice | true_false | short_answer)")
    stem: str = Field(..., description="ë¬¸í•­ ë‚´ìš©")
    choices: list[str] | None = Field(default=None, description="ê°ê´€ì‹ ì„ íƒì§€")
    answer_schema: AnswerSchema = Field(..., description="ë‹µë³€ ê²€ì¦ ìŠ¤í‚¤ë§ˆ")
    difficulty: int = Field(..., ge=1, le=10, description="ë‚œì´ë„ (1~10)")
    category: str = Field(..., description="ë¬¸í•­ ì¹´í…Œê³ ë¦¬")
    validation_score: float = Field(default=0.0, ge=0, le=1, description="ê²€ì¦ ì ìˆ˜ (Tool 4) - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    saved_at: str | None = Field(default=None, description="ì €ì¥ ì‹œê°„ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")


class GenerateQuestionsResponse(BaseModel):
    """ë¬¸í•­ ìƒì„± ì‘ë‹µ (REQ: POST /api/v1/items/generate)."""

    round_id: str = Field(..., description="ìƒì„±ëœ ë¼ìš´ë“œ ID")
    items: list[GeneratedItem] = Field(..., description="ìƒì„±ëœ ë¬¸í•­ ëª©ë¡")
    time_limit_seconds: int = Field(default=1200, description="ì‹œê°„ ì œí•œ (ì´ˆ, ê¸°ë³¸ 20ë¶„)")
    agent_steps: int = Field(default=0, description="ì—ì´ì „íŠ¸ ë°˜ë³µ íšŸìˆ˜ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    failed_count: int = Field(default=0, description="ì‹¤íŒ¨í•œ ë¬¸í•­ ê°œìˆ˜ - ë‚´ë¶€ ë©”íƒ€ë°ì´í„°")
    error_message: str | None = Field(default=None, description="ì—ëŸ¬ ë©”ì‹œì§€")


class ScoreAnswerRequest(BaseModel):
    """
    ìë™ ì±„ì  ìš”ì²­ (ë‹¨ì¼ ì²˜ë¦¬, Phase 1 - Tool 6 ì‹œê·¸ë‹ˆì²˜ ì¤€ìˆ˜).

    Tool 6 ì‹œê·¸ë‹ˆì²˜: score_and_explain(session_id, user_id, question_id, question_type, user_answer, ...)

    í•„ë“œ ì„¤ê³„:
    - Tool 6 í•„ìˆ˜: session_id, user_id, question_id, question_type (ëª¨ë‘ optionalë¡œ ì‹œì‘ ê°€ëŠ¥, ì—ì´ì „íŠ¸ê°€ ì œê³µ)
    - í•„ìˆ˜ (API): user_answer
    - Tool 6 ì„ íƒ: correct_answer, correct_keywords, difficulty, category
    - ë‚´ë¶€ìš©: round_id (í˜¸í™˜ì„±), item_id (í˜¸í™˜ì„±), response_time_ms
    """

    # í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    user_answer: str = Field(..., description="ì‘ì‹œìì˜ ë‹µë³€")

    # Tool 6 íŒŒë¼ë¯¸í„° (ì—ì´ì „íŠ¸ê°€ í˜¸ì¶œí•  ë•Œ ì œê³µ)
    session_id: str | None = Field(default=None, description="í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ID")
    user_id: str | None = Field(default=None, description="ì‚¬ìš©ì ID")
    question_id: str | None = Field(default=None, description="ë¬¸í•­ ID")
    question_type: str | None = Field(default=None, description="ë¬¸í•­ ìœ í˜• (multiple_choice|true_false|short_answer)")

    # Tool 6 ì„ íƒ íŒŒë¼ë¯¸í„°
    correct_answer: str | None = Field(default=None, description="ì •ë‹µ (ê°ê´€ì‹/ì°¸ê±°ì§“ìš©)")
    correct_keywords: list[str] | None = Field(default=None, description="ì •ë‹µ í‚¤ì›Œë“œ (ì£¼ê´€ì‹ìš©)")
    difficulty: int | None = Field(default=None, description="ë¬¸í•­ ë‚œì´ë„ (1-10)")
    category: str | None = Field(default=None, description="ë¬¸í•­ ì¹´í…Œê³ ë¦¬")

    # ë‚´ë¶€ìš© ë©”íƒ€ë°ì´í„° (í˜¸í™˜ì„±)
    round_id: str | None = Field(default=None, description="ë¼ìš´ë“œ ID (ë‚´ë¶€ìš©)")
    item_id: str | None = Field(default=None, description="ë¬¸í•­ ID (ë‚´ë¶€ìš©, round_idì™€ í•¨ê»˜ ì‚¬ìš©)")
    response_time_ms: int = Field(default=0, ge=0, description="ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ, ë‚´ë¶€ìš©)")


class ScoreAnswerResponse(BaseModel):
    """ìë™ ì±„ì  ì‘ë‹µ (ë‹¨ì¼ ì²˜ë¦¬, Phase 1 - Tool 6 ë°˜í™˜ê°’ ì¤€ìˆ˜)."""

    # í•„ìˆ˜ í•„ë“œ
    score: float = Field(..., ge=0, le=100, description="ì ìˆ˜ (0~100)")
    explanation: str = Field(..., description="ì •ë‹µ í•´ì„¤")
    graded_at: str = Field(..., description="ì±„ì  ì‹œê°„")

    # Tool 6 ë°˜í™˜ê°’ ë° API í˜¸í™˜ í•„ë“œ
    # is_correctì™€ correctëŠ” ê°™ì€ ì˜ë¯¸ (Tool 6ëŠ” is_correct ì‚¬ìš©)
    is_correct: bool = Field(default=False, description="ì •ë‹µ ì—¬ë¶€ (Tool 6 í˜¸í™˜)")
    correct: bool = Field(default=False, description="ì •ë‹µ ì—¬ë¶€ (API í˜¸í™˜, is_correctì™€ ë™ì¼)")
    feedback: str | None = Field(default=None, description="ë¶€ë¶„ ì •ë‹µ í”¼ë“œë°±")
    keyword_matches: list[str] = Field(default_factory=list, description="í‚¤ì›Œë“œ ë§¤ì¹­ (Tool 6)")
    extracted_keywords: list[str] = Field(default_factory=list, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ (API í˜¸í™˜)")

    # ë‚´ë¶€ ë©”íƒ€ë°ì´í„°
    item_id: str | None = Field(default=None, description="ë¬¸í•­ ID (ë‚´ë¶€ìš©, ë°°ì¹˜ ì‘ë‹µìš©)")


# ============================================================================
# Batch Scoring Models (Phase 2)
# ============================================================================


class UserAnswer(BaseModel):
    """ì‚¬ìš©ì ë‹µë³€ (ë°°ì¹˜)."""

    item_id: str = Field(..., description="ë¬¸í•­ ID")
    user_answer: str = Field(..., description="ì‚¬ìš©ì ë‹µë³€")
    response_time_ms: int = Field(default=0, ge=0, description="ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")


class SubmitAnswersRequest(BaseModel):
    """ë°°ì¹˜ ì±„ì  ìš”ì²­ (REQ: POST /api/v1/scoring/submit-answers)."""

    round_id: str = Field(..., description="ë¼ìš´ë“œ ID")
    answers: list[UserAnswer] = Field(..., description="ì‚¬ìš©ì ë‹µë³€ ë°°ì¹˜ (1-50ê°œ)")


class ItemScore(BaseModel):
    """ì±„ì ëœ ë¬¸í•­ (ë°°ì¹˜ ì‘ë‹µ)."""

    item_id: str = Field(..., description="ë¬¸í•­ ID")
    correct: bool = Field(..., description="ì •ë‹µ ì—¬ë¶€")
    score: float = Field(..., ge=0, le=100, description="ì ìˆ˜ (0~100)")
    extracted_keywords: list[str] = Field(default_factory=list, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ (ì£¼ê´€ì‹)")
    feedback: str | None = Field(default=None, description="ë¶€ë¶„ ì •ë‹µ í”¼ë“œë°±")


class RoundStats(BaseModel):
    """ë¼ìš´ë“œ í†µê³„."""

    avg_response_time: float = Field(..., ge=0, description="í‰ê·  ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")
    correct_count: int = Field(..., ge=0, description="ì •ë‹µ ê°œìˆ˜")
    total_count: int = Field(..., ge=1, description="ì „ì²´ ë¬¸í•­ ê°œìˆ˜")


class SubmitAnswersResponse(BaseModel):
    """ë°°ì¹˜ ì±„ì  ì‘ë‹µ (REQ: POST /api/v1/scoring/submit-answers)."""

    round_id: str = Field(..., description="ë¼ìš´ë“œ ID")
    per_item: list[ItemScore] = Field(..., description="ë¬¸í•­ë³„ ì±„ì  ê²°ê³¼")
    round_score: float = Field(..., ge=0, le=100, description="ë¼ìš´ë“œ ì´ì ")
    round_stats: RoundStats = Field(..., description="ë¼ìš´ë“œ í†µê³„")


# ============================================================================
# ItemGenAgent Main Class
# ============================================================================


class ItemGenAgent:
    """
    LangChain AgentExecutor ê¸°ë°˜ Item-Gen-Agent.

    ì„¤ëª…:
        - LangChainì˜ create_tool_calling_agent() API ì‚¬ìš©
        - AgentExecutorë¡œ ë„êµ¬ í˜¸ì¶œ ë° ì—ëŸ¬ ì²˜ë¦¬ ê´€ë¦¬
        - Tool Calling ë°©ì‹ (ìµœì‹  LLM ëª¨ë¸ ìµœì í™”)
        - êµ¬ì¡°í™”ëœ ì…ì¶œë ¥ (Pydantic)
        - ìƒì„¸í•œ ë¡œê¹… (ë””ë²„ê¹…)

    ì‚¬ìš© ì˜ˆì‹œ:
        ```python
        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = ItemGenAgent()

        # Mode 1: ë¬¸í•­ ìƒì„±
        request = GenerateQuestionsRequest(
            survey_id="survey_123",
            round_idx=1,
            prev_answers=None
        )
        response = await agent.generate_questions(request)

        # Mode 2: ìë™ ì±„ì 
        score_request = ScoreAnswerRequest(
            round_id="round_123",
            item_id="item_456",
            user_answer="The answer is..."
        )
        score_response = await agent.score_and_explain(score_request)
        ```

    ì°¸ê³ :
        - LangChain ê³µì‹: https://python.langchain.com/docs/concepts/agents
        - create_tool_calling_agent: Tool Calling íŒ¨í„´ êµ¬í˜„ (ìµœì‹  LLM ìµœì í™”)
        - AgentExecutor: max_iterations, early_stopping_method, ì—ëŸ¬ ì²˜ë¦¬
    """

    def __init__(self) -> None:
        """
        Initialize ItemGenAgent with LangGraph create_react_agent.

        ë‹¨ê³„:
            1. LLM ìƒì„± (Google Gemini)
            2. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            3. FastMCP ë„êµ¬ ë“±ë¡
            4. create_react_agent()ë¡œ ì—ì´ì „íŠ¸ ìƒì„± (ìµœì‹  Tool Calling ì§€ì›)

        ì—ëŸ¬ ì²˜ë¦¬:
            - GEMINI_API_KEY ì—†ìŒ: ValueError
            - LLM ì´ˆê¸°í™” ì‹¤íŒ¨: ë¡œê·¸ + ì¬ì‹œë„
        """
        logger.info("ItemGenAgent ì´ˆê¸°í™” ì¤‘...")

        try:
            # 1. LLM ìƒì„±
            self.llm = create_llm()
            logger.info("âœ“ LLM (Google Gemini) ìƒì„± ì™„ë£Œ")

            # 2. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            self.prompt = get_react_prompt()
            logger.info("âœ“ ReAct í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")

            # 3. ë„êµ¬ ëª©ë¡ (6ê°œ)
            self.tools = TOOLS
            logger.info(f"âœ“ {len(self.tools)}ê°œ ë„êµ¬ ë“±ë¡ ì™„ë£Œ: {[t.name for t in self.tools]}")

            # 4. create_react_agent() - LangGraph ìµœì‹  Tool Calling ì§€ì›
            # LangGraphì˜ create_react_agentëŠ” ìµœì‹  LLMì˜ Tool Calling ê¸°ëŠ¥ì„ ìë™ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.
            # ReAct íŒ¨í„´: Thought â†’ Action â†’ Observationì„ ë°˜ë³µí•˜ë©° ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            # AGENT_CONFIGì˜ max_iterations, early_stopping_method, handle_parsing_errorsëŠ”
            # create_react_agentì˜ ë˜í¼ë¡œ í™œìš©ë˜ê±°ë‚˜, CompiledStateGraph ì‹¤í–‰ ì‹œ configë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
            self.executor = create_react_agent(
                model=self.llm,
                tools=self.tools,
                prompt=self.prompt,
                debug=AGENT_CONFIG.get("verbose", False),
                version="v2",  # ìµœì‹  LangGraph v2 API ì‚¬ìš©
            )
            logger.info("âœ“ ReAct ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ (Tool Calling ìµœì í™” v2)")

            logger.info("âœ… ItemGenAgent ì´ˆê¸°í™” ì„±ê³µ")

        except Exception as e:
            logger.error(f"âŒ ItemGenAgent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _extract_tool_results(self, result: dict, tool_name: str) -> list[tuple[str, str]]:
        """
        Extract tool results from agent output in both formats.

        LangGraph create_react_agent returns:
            {"messages": [AIMessage, ToolMessage, ...], ...}

        While AgentExecutor returns:
            {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...], ...}

        This helper extracts tool calls in both formats to support both actual LangGraph
        output and test mocks.

        Args:
            result: Agent output dict
            tool_name: Name of the tool to extract (e.g., "save_generated_question", "score_and_explain")

        Returns:
            List of (tool_name, tool_output_str) tuples matching the given tool_name

        Raises:
            Exception: Logs errors but continues processing for robustness

        LangChain Message Structures:
        - AIMessage: {"type": "ai", "content": "...", "tool_calls": [ToolCall(...), ...]}
        - ToolMessage: {"type": "tool", "content": "...", "tool_call_id": "call_123", "name": "tool_name"}
        - ToolCall: {id: "call_123", name: "tool_name", args: {...}}

        """
        tool_results = []
        logger.info(f"\nğŸ”§ _extract_tool_results: Looking for tool_name='{tool_name}'")

        # Format 1: AgentExecutor intermediate_steps (for backward compatibility with tests)
        intermediate_steps = result.get("intermediate_steps", [])
        if intermediate_steps:
            logger.info("âœ“ Format 1: intermediate_steps detected (test mock)")
            logger.info(f"  Total steps: {len(intermediate_steps)}")
            for i, (step_tool_name, tool_output_str) in enumerate(intermediate_steps):
                logger.info(f"  [{i}] step_tool_name='{step_tool_name}', matches_target={step_tool_name == tool_name}")
                if step_tool_name == tool_name:
                    tool_results.append((step_tool_name, tool_output_str))
                    logger.info(f"       âœ“ MATCHED - output_type={type(tool_output_str).__name__}")
            logger.info(f"  Result: {len(tool_results)} matching tools found\n")
            return tool_results

        # Format 2: LangGraph messages format (actual LangGraph output)
        messages = result.get("messages", [])
        if not messages:
            logger.warning("No intermediate_steps or messages found in agent result\n")
            return []

        logger.info("âœ“ Format 2: messages detected (LangGraph output)")
        logger.info(f"  Total messages: {len(messages)}\n")

        # Step 1: Build a map of tool_call_id â†’ ToolMessage for quick lookup
        logger.info("Step 1ï¸âƒ£: Scanning for ToolMessages...")
        tool_messages_by_id: dict[str, ToolMessage] = {}
        for i, message in enumerate(messages):
            msg_type = type(message).__name__
            if isinstance(message, ToolMessage):
                tool_call_id = message.tool_call_id
                name = getattr(message, "name", "?")
                content_preview = str(getattr(message, "content", ""))[:100]
                logger.info(f"  [{i}] ToolMessage found:")
                logger.info(f"       tool_call_id={tool_call_id}, name={name}")
                logger.info(f"       content_preview={content_preview}...")
                if tool_call_id:
                    tool_messages_by_id[tool_call_id] = message
                    logger.info("       âœ“ Added to map")
                else:
                    logger.info("       âš ï¸  No tool_call_id!")
            elif msg_type == "AIMessage":
                logger.info(f"  [{i}] AIMessage (checking for tool_calls...)")
            else:
                logger.info(f"  [{i}] {msg_type}")

        logger.info(f"\nToolMessage map summary: {len(tool_messages_by_id)} items\n")

        # Step 2: Iterate through AIMessages to find matching tool calls
        logger.info("Step 2ï¸âƒ£: Scanning AIMessages for tool_calls...")
        ai_message_count = 0
        for i, message in enumerate(messages):
            if isinstance(message, AIMessage):
                ai_message_count += 1
                logger.info(f"  [{i}] AIMessage #{ai_message_count}")

                # AIMessage has tool_calls list with ToolCall objects
                tool_calls = getattr(message, "tool_calls", [])
                logger.info(f"       tool_calls: {len(tool_calls)} found")

                if not tool_calls:
                    logger.info("       âš ï¸  No tool_calls in this message")
                    continue

                for j, tool_call in enumerate(tool_calls):
                    try:
                        # ToolCall is an object with .id and .name attributes
                        call_id = tool_call.id if hasattr(tool_call, "id") else tool_call.get("id")
                        call_name = tool_call.name if hasattr(tool_call, "name") else tool_call.get("name")

                        logger.info(f"         [{j}] tool_call: name='{call_name}', id={call_id}")
                        logger.info(f"              target_tool_name='{tool_name}', matches={call_name == tool_name}")

                        # Check if this tool call matches what we're looking for
                        if call_name == tool_name:
                            logger.info("              âœ“ NAME MATCHED!")
                            # Find the corresponding ToolMessage
                            if call_id in tool_messages_by_id:
                                tool_msg = tool_messages_by_id[call_id]
                                content = tool_msg.content if hasattr(tool_msg, "content") else str(tool_msg)
                                tool_results.append((tool_name, content))
                                content_preview = str(content)[:100]
                                logger.info(f"              âœ“ FOUND ToolMessage with id={call_id}")
                                logger.info(f"              content_preview={content_preview}...")
                            else:
                                logger.warning(
                                    f"              âœ— NO ToolMessage found for id={call_id}!\n"
                                    f"                 Available IDs in map: {list(tool_messages_by_id.keys())}"
                                )
                        else:
                            logger.info(f"              âœ— Name mismatch (looking for '{tool_name}')")

                    except (AttributeError, KeyError, TypeError) as e:
                        logger.error(f"         [{j}] ERROR extracting tool_call properties: {e}")
                        continue

        logger.info(f"\nStep 2ï¸âƒ£ Result: {len(tool_results)} matching tools found\n")
        return tool_results

    async def generate_questions(self, request: GenerateQuestionsRequest) -> GenerateQuestionsResponse:
        """
        Mode 1: Generate questions (Tool 1-5 auto-select).

        REQ: REQ-A-Mode1-Pipeline

        ë‹¨ê³„:
            1. ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (Tool 1)
            2. í…œí”Œë¦¿ ê²€ìƒ‰ (Tool 2) - ì„ íƒì‚¬í•­
            3. ë‚œì´ë„ë³„ í‚¤ì›Œë“œ ì¡°íšŒ (Tool 3)
            4. LLMìœ¼ë¡œ ë¬¸í•­ ìƒì„±
            5. ê° ë¬¸í•­ ê²€ì¦ (Tool 4)
            6. ê²€ì¦ í†µê³¼ ë¬¸í•­ ì €ì¥ (Tool 5)

        Args:
            request: GenerateQuestionsRequest

        Returns:
            GenerateQuestionsResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool í˜¸ì¶œ ì‹¤íŒ¨: ìë™ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
            - ì—ì´ì „íŠ¸ ìµœëŒ€ ë°˜ë³µ: ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜
            - LLM ì˜¤ë¥˜: ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨

        ì°¸ê³ :
            - AgentExecutor: ë„êµ¬ í˜¸ì¶œ ë° Tool Calling ë£¨í”„ ìë™ ê´€ë¦¬
            - intermediate_steps: ì—ì´ì „íŠ¸ì˜ ê° ë„êµ¬ í˜¸ì¶œ ì¶”ì 

        """
        logger.info(f"ğŸ“ ë¬¸í•­ ìƒì„± ì‹œì‘: survey_id={request.survey_id}, round_idx={request.round_idx}")

        try:
            # ë¼ìš´ë“œ ID ìƒì„± (REQ-A-RoundID)
            # survey_idë¥¼ session_idë¡œ ì‚¬ìš©í•˜ì—¬ ë¼ìš´ë“œ ID ìƒì„±
            round_id = _round_id_gen.generate(session_id=request.survey_id, round_number=request.round_idx)

            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„±
            question_types_str = (
                ", ".join(request.question_types)
                if request.question_types
                else "multiple_choice, true_false, short_answer"
            )
            agent_input = f"""
Generate high-quality exam questions for the following survey.
Session ID: {request.session_id}
Survey ID: {request.survey_id}
Round: {request.round_idx}
Domain: {request.domain}
Previous Answers: {json.dumps(request.prev_answers) if request.prev_answers else "None (First round)"}
Question Count: {request.question_count}
Question Types: {question_types_str}

Follow these steps:
1. Get survey context and user profile (Tool 1)
2. Search question templates for similar items (Tool 2) if available
3. Get keywords for adaptive difficulty (Tool 3)
4. Generate new questions with appropriate difficulty (focused on {request.domain} domain)
5. Validate each question (Tool 4)
6. Save validated questions (Tool 5) with session_id={request.session_id} and round_id={round_id}

Important:
- Generate EXACTLY {request.question_count} questions with the specified types
- All questions should be related to {request.domain} domain/topic
- Generate questions with appropriate answer_schema (exact_match, keyword_match, or semantic_match)
- Each question must include: id, type, stem, choices (if MC), answer_schema, difficulty, category
- Return all saved questions with validation scores
- When calling Tool 5, ALWAYS pass session_id={request.session_id} to save questions with correct session reference
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (Tool Calling ë£¨í”„)
            # LangGraph v2 create_react_agentëŠ” messages í˜•ì‹ìœ¼ë¡œ ì…ë ¥ ë°›ìŒ
            # - messages: ëŒ€í™” íˆìŠ¤í† ë¦¬ (HumanMessage, AIMessage, ToolMessage ë“±)
            # - Agentê°€ ìë™ìœ¼ë¡œ Thought â†’ Action â†’ Observation ë°˜ë³µ
            #
            # ì„±ëŠ¥ ìµœì í™” (í–¥í›„ ê°œì„ ì‚¬í•­):
            # 1. Tool ë³‘ë ¬ ì‹¤í–‰: ë…ë¦½ì ì¸ Toolë“¤ (validate + save)ì€ asyncio.gatherë¡œ ë³‘ë ¬í™” ê°€ëŠ¥
            #    - ReAct íŒ¨í„´ì˜ ì œì•½: ê° Tool ê²°ê³¼ â†’ ë‹¤ìŒ Thought ê²°ì •
            #    - ê°€ëŠ¥í•œ ê²½ìš°: ê°™ì€ ì§ˆë¬¸ì˜ validate/save ë‹¨ê³„ë¥¼ ë³‘ë ¬í™”
            # 2. Tool ë¹„ë™ê¸°í™”: ëª¨ë“  Toolì„ async í•¨ìˆ˜ë¡œ ë³€ê²½ (í˜„ì¬ëŠ” ë™ê¸°)
            # 3. ìºì‹±: ìì£¼ í˜¸ì¶œë˜ëŠ” Tool (get_difficulty_keywords)ì— ìºì‹± ì ìš©
            result = await self.executor.ainvoke({"messages": [HumanMessage(content=agent_input)]})

            logger.info("âœ… ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_generate(result, round_id)
            logger.info(f"âœ… ë¬¸í•­ ìƒì„± ì„±ê³µ: {len(response.items)}ê°œ ìƒì„±")

            return response

        except Exception as e:
            logger.error(f"âŒ ë¬¸í•­ ìƒì„± ì‹¤íŒ¨: {e}")
            return GenerateQuestionsResponse(
                round_id=f"round_error_{uuid.uuid4().hex[:8]}",
                items=[],
                time_limit_seconds=1200,
                agent_steps=0,
                failed_count=0,
                error_message=str(e),
            )

    async def score_and_explain(self, request: ScoreAnswerRequest) -> ScoreAnswerResponse:
        """
        Mode 2: Auto-grade answers (Tool 6).

        REQ: REQ-A-Mode2-Pipeline

        ë‹¨ê³„:
            1. Tool 6 í˜¸ì¶œ (ìë™ ì±„ì  & í•´ì„¤ ìƒì„±)

        Args:
            request: ScoreAnswerRequest

        Returns:
            ScoreAnswerResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool 6 í˜¸ì¶œ ì‹¤íŒ¨: ì¬ì‹œë„ 3íšŒ
            - LLM ì˜¤ë¥˜: ê¸°ë³¸ ì ìˆ˜ 0 ë°˜í™˜

        ì°¸ê³ :
            - Tool 6: ê°ê´€ì‹/OX (ì •í™• ë§¤ì¹­) vs ì£¼ê´€ì‹ (LLM í‰ê°€)
            - ì±„ì  ê¸°ì¤€: >= 80 â†’ ì •ë‹µ, 70~79 â†’ ë¶€ë¶„ ì •ë‹µ, < 70 â†’ ì˜¤ë‹µ

        """
        logger.info(
            f"ğŸ“‹ ìë™ ì±„ì  ì‹œì‘: round_id={request.round_id}, item_id={request.item_id}, user_id={request.user_id}"
        )

        try:
            # Tool 6 í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
            session_id = request.session_id or "unknown_session"
            user_id = request.user_id or "unknown_user"
            question_id = request.question_id or request.item_id
            question_type = request.question_type or "short_answer"

            # ì—ì´ì „íŠ¸ ì…ë ¥ êµ¬ì„± (Tool 6 í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëª…ì‹œ)
            agent_input = f"""
Score and explain the following answer using Tool 6 (score_and_explain):

=== TEST SESSION CONTEXT ===
Session ID: {session_id}
User ID: {user_id}
Question ID: {question_id}
Question Type: {question_type}

=== ANSWER DETAILS ===
Round ID: {request.round_id}
User Answer: {request.user_answer}
Response Time (ms): {request.response_time_ms}"""

            # ì¶”ê°€ ì •ë³´ (ì„ íƒì‚¬í•­)
            if request.correct_answer:
                agent_input += f"\nCorrect Answer: {request.correct_answer}"
            if request.correct_keywords:
                agent_input += f"\nCorrect Keywords: {', '.join(request.correct_keywords)}"
            if request.difficulty:
                agent_input += f"\nDifficulty Level: {request.difficulty}"
            if request.category:
                agent_input += f"\nQuestion Category: {request.category}"

            agent_input += f"""

=== TASK ===
Call Tool 6 (score_and_explain) with the following parameters:
- session_id: {session_id}
- user_id: {user_id}
- question_id: {question_id}
- question_type: {question_type}
- user_answer: [User's response]
- correct_answer: [if available]
- correct_keywords: [if applicable]
- difficulty: [if available]
- category: [if available]

Tool 6 will return: is_correct (boolean), score (0-100), explanation, keyword_matches, feedback, graded_at
"""

            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = await self.executor.ainvoke({"input": agent_input})

            logger.info("âœ… ì±„ì  ì™„ë£Œ")

            # ê²°ê³¼ íŒŒì‹±
            response = self._parse_agent_output_score(result, request.item_id)
            logger.info(f"âœ… ì±„ì  ì„±ê³µ: score={response.score}, correct={response.correct}")

            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return ScoreAnswerResponse(
                item_id=request.item_id,
                correct=False,
                score=0.0,
                explanation=f"ì±„ì  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )

    async def submit_answers(self, request: SubmitAnswersRequest) -> SubmitAnswersResponse:
        """
        Mode 2 Batch: Auto-grade multiple answers in one round (Tool 6).

        REQ: REQ-B-ItemGen-Batch

        ë‹¨ê³„:
            1. ê° ë‹µë³€ì— ëŒ€í•´ Tool 6 í˜¸ì¶œ (ìë™ ì±„ì )
            2. ì±„ì  ê²°ê³¼ ìˆ˜ì§‘
            3. ë¼ìš´ë“œ í†µê³„ ê³„ì‚° (í‰ê·  ì‘ë‹µì‹œê°„, ì •ë‹µë¥  ë“±)
            4. ë°°ì¹˜ ì‘ë‹µ ë°˜í™˜

        Args:
            request: SubmitAnswersRequest

        Returns:
            SubmitAnswersResponse

        ì—ëŸ¬ ì²˜ë¦¬:
            - Tool 6 í˜¸ì¶œ ì‹¤íŒ¨: ê°œë³„ í•­ëª©ë³„ ì¬ì‹œë„
            - í†µê³„ ê³„ì‚° ì˜¤ë¥˜: ì•ˆì „í•œ ê¸°ë³¸ê°’ ì œê³µ

        """
        logger.info(f"ğŸ“ ë°°ì¹˜ ì±„ì  ì‹œì‘: round_id={request.round_id}, items={len(request.answers)}")

        try:
            per_item: list[ItemScore] = []
            response_times: list[int] = []

            # Round IDì—ì„œ session_id ì¶”ì¶œ (RoundIDGenerator í¬ë§·: session_id_round_number_timestamp)
            try:
                parsed_round = _round_id_gen.parse(request.round_id)
                session_id = parsed_round.session_id
                logger.debug(f"Extracted session_id from round_id: {session_id}")
            except Exception as e:
                # Round ID íŒŒì‹± ì‹¤íŒ¨ ì‹œ round_id ì „ì²´ë¥¼ session_idë¡œ ì‚¬ìš©
                session_id = request.round_id
                logger.warning(f"Failed to parse round_id: {e}, using full round_id as session_id")

            # 1. ê° ë‹µë³€ì„ ìˆœì°¨ ì±„ì  (ë³‘ë ¬í™”ëŠ” Phase 3)
            for answer in request.answers:
                try:
                    # ë‹¨ì¼ ì±„ì  ë©”ì„œë“œ í™œìš© (Tool 6 í•„ìˆ˜ íŒŒë¼ë¯¸í„° í¬í•¨)
                    single_request = ScoreAnswerRequest(
                        # Tool 6 í•„ìˆ˜ íŒŒë¼ë¯¸í„°
                        session_id=session_id,
                        user_id=None,  # ë°°ì¹˜ ìš”ì²­ì—ì„œëŠ” ì œê³µë˜ì§€ ì•ŠìŒ - Tool 6ì—ì„œ "unknown_user" ê¸°ë³¸ê°’ ì‚¬ìš©
                        question_id=answer.item_id,  # item_idë¥¼ question_idë¡œ ì‚¬ìš©
                        question_type="short_answer",  # ë°°ì¹˜ ì±„ì ì—ì„œ ê¸°ë³¸ê°’ - ì‹¤ì œ íƒ€ì…ì€ ì§ˆë¬¸ DBì—ì„œ ì¡°íšŒ í•„ìš”
                        # ì‚¬ìš©ì ë‹µë³€
                        user_answer=answer.user_answer,
                        # ë‚´ë¶€ ë©”íƒ€ë°ì´í„°
                        round_id=request.round_id,
                        item_id=answer.item_id,
                        response_time_ms=answer.response_time_ms,
                    )

                    result = await self.score_and_explain(single_request)

                    # ë°°ì¹˜ ì‘ë‹µ í¬ë§·ìœ¼ë¡œ ë³€í™˜
                    item_score = ItemScore(
                        item_id=result.item_id,
                        correct=result.correct,
                        score=result.score,
                        extracted_keywords=result.extracted_keywords,
                        feedback=result.feedback,
                    )
                    per_item.append(item_score)
                    response_times.append(answer.response_time_ms)

                    logger.info(f"âœ“ ë¬¸í•­ ì±„ì  ì™„ë£Œ: {answer.item_id}, score={result.score}, correct={result.correct}")

                except Exception as e:
                    logger.error(f"âŒ ë¬¸í•­ ì±„ì  ì‹¤íŒ¨: {answer.item_id}, {str(e)}")
                    # ì‹¤íŒ¨í•œ í•­ëª©ë„ ê²°ê³¼ì— í¬í•¨ (score=0)
                    item_score = ItemScore(
                        item_id=answer.item_id,
                        correct=False,
                        score=0.0,
                        feedback=f"ì±„ì  ì˜¤ë¥˜: {str(e)}",
                    )
                    per_item.append(item_score)
                    response_times.append(answer.response_time_ms)

            # 2. ë¼ìš´ë“œ í†µê³„ ê³„ì‚°
            correct_count = sum(1 for item in per_item if item.correct)
            total_count = len(per_item)
            round_score = sum(item.score for item in per_item) / total_count if total_count > 0 else 0.0
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0.0

            round_stats = RoundStats(
                avg_response_time=avg_response_time,
                correct_count=correct_count,
                total_count=total_count,
            )

            # 3. ë°°ì¹˜ ì‘ë‹µ ìƒì„±
            response = SubmitAnswersResponse(
                round_id=request.round_id,
                per_item=per_item,
                round_score=round_score,
                round_stats=round_stats,
            )

            logger.info(
                f"âœ… ë°°ì¹˜ ì±„ì  ì™„ë£Œ: "
                f"round_score={response.round_score:.1f}, "
                f"correct={correct_count}/{total_count}, "
                f"avg_time={avg_response_time:.0f}ms"
            )

            return response

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì±„ì  ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            # ë¶€ë¶„ ê²°ê³¼ë¼ë„ ë°˜í™˜í•˜ì§€ ì•Šê³  ì „ì²´ ì‹¤íŒ¨ í‘œì‹œ
            return SubmitAnswersResponse(
                round_id=request.round_id,
                per_item=[],
                round_score=0.0,
                round_stats=RoundStats(
                    avg_response_time=0.0,
                    correct_count=0,
                    total_count=len(request.answers),
                ),
            )

    def _parse_agent_output_generate(self, result: dict, round_id: str) -> GenerateQuestionsResponse:
        """
        Parse agent output for question generation (REQ-A-LangChain).

        Args:
            result: Agent output (supports both AgentExecutor and LangGraph formats)
            round_id: ë¼ìš´ë“œ ID

        Returns:
            GenerateQuestionsResponse

        ë¡œì§:
            1. _extract_tool_results()ë¡œ ë„êµ¬ í˜¸ì¶œ ì¶”ì¶œ (intermediate_steps ë˜ëŠ” messages ëª¨ë‘ ì§€ì›)
            2. nameì´ "save_generated_question"ì¸ í˜¸ì¶œì—ì„œ question ë°ì´í„° íŒŒì‹±
            3. ê° questionì„ GeneratedItemìœ¼ë¡œ ë³€í™˜
            4. ì„±ê³µ/ì‹¤íŒ¨ ê°œìˆ˜ ì§‘ê³„

        ì°¸ê³ :
            - AgentExecutor ì¶œë ¥: {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...]}
            - LangGraph ì¶œë ¥: {"messages": [...message objects...]}
            - ë‘ í¬ë§· ëª¨ë‘ _extract_tool_results()ë¡œ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬

        """
        logger.info(f"ë¬¸í•­ ìƒì„± ê²°ê³¼ íŒŒì‹± ì¤‘... round_id={round_id}")

        try:
            # DEBUG: Agent output êµ¬ì¡° ë¶„ì„
            logger.info("=" * 80)
            logger.info("ğŸ” AGENT OUTPUT STRUCTURE ANALYSIS")
            logger.info("=" * 80)

            # 1. Result dict í‚¤ í™•ì¸
            result_keys = list(result.keys())
            logger.info(f"Result ìµœìƒìœ„ í‚¤: {result_keys}")

            # 2. intermediate_steps í™•ì¸
            intermediate_steps = result.get("intermediate_steps", [])
            if intermediate_steps:
                logger.info(f"\nâœ“ intermediate_steps ë°œê²¬: {len(intermediate_steps)}ê°œ")
                for i, (tool_name, tool_output) in enumerate(intermediate_steps):
                    output_preview = str(tool_output)[:150]
                    logger.info(f"  [{i}] tool_name={tool_name}, output_type={type(tool_output).__name__}")
                    logger.info(f"      output_preview={output_preview}...")
            else:
                logger.info("\nâœ— intermediate_steps ì—†ìŒ")

            # 3. messages í™•ì¸
            messages = result.get("messages", [])
            if messages:
                logger.info(f"\nâœ“ messages ë°œê²¬: {len(messages)}ê°œ")
                for i, msg in enumerate(messages):
                    msg_type = type(msg).__name__
                    msg_attrs = {
                        "type": getattr(msg, "type", "N/A"),
                        "has_content": hasattr(msg, "content"),
                        "has_tool_calls": hasattr(msg, "tool_calls"),
                        "has_tool_call_id": hasattr(msg, "tool_call_id"),
                    }
                    logger.info(f"  [{i}] type={msg_type}, attrs={msg_attrs}")

                    # AIMessageì˜ ê²½ìš° tool_calls í™•ì¸
                    if msg_type == "AIMessage" and hasattr(msg, "tool_calls"):
                        tool_calls = getattr(msg, "tool_calls", [])
                        if tool_calls:
                            logger.info(f"      tool_calls: {len(tool_calls)}ê°œ")
                            for j, tc in enumerate(tool_calls):
                                tc_name = getattr(tc, "name", tc.get("name") if isinstance(tc, dict) else "?")
                                tc_id = getattr(tc, "id", tc.get("id") if isinstance(tc, dict) else "?")
                                logger.info(f"        [{j}] name={tc_name}, id={tc_id}")
                        # DEBUG: AIMessageì˜ content ì „ì²´ ì¶œë ¥
                        ai_content = getattr(msg, "content", "")
                        if ai_content:
                            content_preview = str(ai_content)[:500]
                            logger.info("      AIMessage.content (first 500 chars):")
                            logger.info(f"        {content_preview}...")

                    # ToolMessageì˜ ê²½ìš° ë‚´ìš© í™•ì¸
                    if msg_type == "ToolMessage":
                        content = getattr(msg, "content", "?")
                        tool_call_id = getattr(msg, "tool_call_id", "?")
                        name = getattr(msg, "name", "?")
                        logger.info(f"      tool_call_id={tool_call_id}, name={name}")
                        logger.info(f"      content_preview={str(content)[:200]}...")
            else:
                logger.info("\nâœ— messages ì—†ìŒ")

            logger.info("=" * 80)

            # 0. ReAct í…ìŠ¤íŠ¸ í˜•ì‹: Final Answer JSON íŒŒì‹± ì‹œë„
            logger.info("\nğŸ” Attempting to parse Final Answer JSON from AIMessage...")
            items: list[GeneratedItem] = []
            failed_count = 0
            error_messages: list[str] = []
            agent_steps = 0  # Initialize agent_steps early

            # AIMessageì—ì„œ Final Answer JSON ì¶”ì¶œ
            for message in result.get("messages", []):
                if isinstance(message, AIMessage):
                    content = getattr(message, "content", "")

                    # Final Answer: íŒ¨í„´ ì°¾ê¸°
                    if "Final Answer:" in content:
                        logger.info("âœ“ Found 'Final Answer:' in AIMessage content")

                        try:
                            # Final Answer ë’¤ì˜ JSON ì¶”ì¶œ
                            json_start = content.find("Final Answer:") + len("Final Answer:")
                            json_str = content[json_start:].strip()

                            # ```json ... ``` ë§ˆí¬ë‹¤ìš´ ì œê±°
                            if "```json" in json_str:
                                json_str = json_str.split("```json")[1].split("```")[0].strip()
                            elif "```" in json_str:
                                json_str = json_str.split("```")[1].split("```")[0].strip()

                            # Unescape ì²˜ë¦¬: Agentê°€ escaped quotesë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
                            # Replace escaped quotes (order matters: handle single quotes first)
                            # Remove backslash before single quotes (not valid in JSON strings)
                            json_str = json_str.replace("\\'", "'")
                            # Replace escaped double quotes with regular quotes
                            json_str = json_str.replace('\\"', '"')

                            # Convert Python None to JSON null
                            import re

                            json_str = re.sub(r"\bNone\b", "null", json_str)

                            logger.info(f"ğŸ“‹ Extracted JSON (first 300 chars): {json_str[:300]}...")

                            # JSON íŒŒì‹±
                            try:
                                questions_data = json.loads(json_str)
                            except json.JSONDecodeError as e:
                                # Additional cleaning if initial parse fails
                                logger.warning(
                                    f"âš ï¸  Initial JSON parse failed at char {e.pos}, applying additional cleanup"
                                )
                                # More aggressive cleanup for edge cases
                                json_str = re.sub(r"\\\'", "'", json_str)
                                json_str = re.sub(r'\\\\"', '"', json_str)
                                json_str = re.sub(
                                    r"\\(?![ubnftr/])", "", json_str
                                )  # Remove backslashes not part of valid escape sequences
                                # Convert Python True/False to JSON true/false
                                json_str = re.sub(r"\bTrue\b", "true", json_str)
                                json_str = re.sub(r"\bFalse\b", "false", json_str)
                                # Convert Python None to JSON null (redundant but safe)
                                json_str = re.sub(r"\bNone\b", "null", json_str)
                                logger.info(f"ğŸ“‹ Cleaned JSON (first 300 chars): {json_str[:300]}...")
                                questions_data = json.loads(json_str)

                            if not isinstance(questions_data, list):
                                questions_data = [questions_data]

                            logger.info(f"âœ… Parsed {len(questions_data)} question(s) from Final Answer JSON")

                            # ê° questionì„ GeneratedItemìœ¼ë¡œ ë³€í™˜
                            for q in questions_data:
                                try:
                                    # answer_schema êµ¬ì„±
                                    # Tool 5 ë°˜í™˜ê°’ì—ì„œ flattened í•„ë“œ ì‚¬ìš© (correct_answer, correct_keywords)
                                    answer_schema = AnswerSchema(
                                        type=q.get("answer_schema", "exact_match"),
                                        keywords=q.get("correct_keywords"),  # From Tool 5 response
                                        correct_answer=q.get("correct_answer"),  # From Tool 5 response
                                    )
                                    logger.info(
                                        f"  âœ“ answer_schema populated: type={answer_schema.type}, keywords={answer_schema.keywords is not None}, correct_answer={answer_schema.correct_answer is not None}"
                                    )

                                    item = GeneratedItem(
                                        id=q.get("question_id", f"q_{uuid.uuid4().hex[:8]}"),
                                        type=q.get("type", "multiple_choice"),
                                        stem=q.get("stem", ""),
                                        choices=q.get("choices"),
                                        answer_schema=answer_schema,
                                        difficulty=q.get("difficulty", 5),
                                        category=q.get("category", "AI"),
                                        validation_score=q.get("validation_score", 0.0),
                                        saved_at=datetime.now(UTC).isoformat(),
                                    )
                                    items.append(item)
                                    logger.info(f"  âœ“ Created GeneratedItem: {item.id} ({item.stem[:50]}...)")

                                except Exception as e:
                                    logger.error(f"  âœ— Failed to create GeneratedItem: {e}")
                                    failed_count += 1
                                    error_messages.append(f"GeneratedItem creation error: {str(e)}")
                                    continue

                            # Final Answer JSONì´ íŒŒì‹±ë˜ë©´ ë„êµ¬ ì¶”ì¶œ ìŠ¤í‚µ
                            if items:
                                logger.info(f"\nâœ… Successfully parsed {len(items)} items from Final Answer JSON")
                                logger.info("Skipping tool results extraction (using Final Answer format)")
                                # Update agent_steps when Final Answer JSON is found
                                agent_steps = max(
                                    agent_steps,
                                    len(result.get("intermediate_steps", [])) or len(result.get("messages", [])),
                                )
                                break

                        except json.JSONDecodeError as e:
                            logger.warning(f"âŒ Failed to parse Final Answer JSON: {e}")
                            error_messages.append(f"Final Answer JSON decode error: {str(e)}")
                            continue
                        except Exception as e:
                            logger.warning(f"âŒ Error processing Final Answer: {e}")
                            error_messages.append(f"Final Answer processing error: {str(e)}")
                            continue

            # 1. Final Answer JSONì´ ì—†ìœ¼ë©´ save_generated_question ë„êµ¬ ê²°ê³¼ ì¶”ì¶œ
            if not items:
                logger.info("\nğŸ“Š Extracting save_generated_question tool results (LangGraph format)...")
                tool_results = self._extract_tool_results(result, "save_generated_question")
                agent_steps = max(
                    agent_steps, len(result.get("intermediate_steps", [])) or len(result.get("messages", []))
                )
                logger.info(f"âœ“ ë„êµ¬ í˜¸ì¶œ {agent_steps}ê°œ ë°œê²¬, save_generated_question {len(tool_results)}ê°œ")

                # DEBUG: ì¶”ì¶œëœ tool_results ìƒì„¸ ì¶œë ¥
                if tool_results:
                    logger.info("\nğŸ“‹ Extracted tool results:")
                    for i, (tool_name, tool_output_str) in enumerate(tool_results):
                        logger.info(f"  [{i}] tool_name={tool_name}")
                        logger.info(f"      output_type={type(tool_output_str).__name__}")
                        output_preview = str(tool_output_str)[:300]
                        logger.info(f"      output_preview={output_preview}...")
                else:
                    logger.warning("âš ï¸  No tool results extracted!")

                # Tool results íŒŒì‹±ìœ¼ë¡œ items êµ¬ì„±
                for tool_name, tool_output_str in tool_results:
                    if tool_name != "save_generated_question":
                        continue

                    if not tool_output_str:
                        failed_count += 1
                        continue

                    # JSON íŒŒì‹±
                    try:
                        tool_output = (
                            json.loads(tool_output_str) if isinstance(tool_output_str, str) else tool_output_str
                        )
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(tool_output_str)[:100]}")
                        failed_count += 1
                        error_messages.append(f"JSON decode error: {str(e)}")
                        continue

                    # success í”Œë˜ê·¸ í™•ì¸
                    has_error = "error" in tool_output
                    is_success = tool_output.get("success", not has_error)

                    if not is_success or has_error:
                        failed_count += 1
                        if "error" in tool_output:
                            error_messages.append(tool_output["error"])
                        continue

                    # GeneratedItem ê°ì²´ ìƒì„±
                    try:
                        # answer_schema êµ¬ì„± (Tool 5ê°€ ì œê³µí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
                        schema_from_tool = tool_output.get("answer_schema", {})
                        if isinstance(schema_from_tool, dict):
                            # Tool 5ì—ì„œ ë°˜í™˜í•œ answer_schema ì‚¬ìš©
                            answer_schema = AnswerSchema(
                                type=schema_from_tool.get(
                                    "type", schema_from_tool.get("correct_key") and "exact_match" or "keyword_match"
                                ),
                                keywords=schema_from_tool.get("correct_keywords") or schema_from_tool.get("keywords"),
                                correct_answer=schema_from_tool.get("correct_key")
                                or schema_from_tool.get("correct_answer"),
                            )
                        else:
                            # Fallback to tool_output fields
                            answer_schema = AnswerSchema(
                                type=tool_output.get("answer_type", "exact_match"),
                                keywords=tool_output.get("correct_keywords"),
                                correct_answer=tool_output.get("correct_answer"),
                            )

                        item = GeneratedItem(
                            id=tool_output.get("question_id", f"q_{uuid.uuid4().hex[:8]}"),
                            type=tool_output.get("item_type", "multiple_choice"),
                            stem=tool_output.get("stem", ""),
                            choices=tool_output.get("choices"),
                            answer_schema=answer_schema,
                            difficulty=tool_output.get("difficulty", 5),
                            category=tool_output.get("category", "general"),
                            validation_score=tool_output.get("validation_score", 0.0),
                            saved_at=tool_output.get("saved_at", datetime.now(UTC).isoformat()),
                        )
                        items.append(item)
                        logger.info(f"âœ“ ë¬¸í•­ íŒŒì‹± ì„±ê³µ: {item.id}, stem={item.stem[:50] if item.stem else 'N/A'}")

                    except Exception as e:
                        logger.error(f"GeneratedItem ìƒì„± ì‹¤íŒ¨: {e}")
                        failed_count += 1
                        error_messages.append(str(e))
                        continue

            # 3. ì‘ë‹µ ìƒì„±
            error_msg = " | ".join(error_messages) if error_messages else None

            response = GenerateQuestionsResponse(
                round_id=round_id,
                items=items,
                time_limit_seconds=1200,  # ê¸°ë³¸ 20ë¶„
                agent_steps=agent_steps,
                failed_count=failed_count,
                error_message=error_msg,
            )

            logger.info(f"âœ… íŒŒì‹± ì™„ë£Œ: ì„±ê³µ={len(items)}, ì‹¤íŒ¨={failed_count}, agent_steps={agent_steps}")
            return response

        except Exception as e:
            logger.error(f"âŒ íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return GenerateQuestionsResponse(
                round_id=round_id,
                items=[],
                time_limit_seconds=1200,
                agent_steps=0,
                failed_count=0,
                error_message=f"Parsing error: {str(e)}",
            )

    def _parse_agent_output_score(self, result: dict, item_id: str) -> ScoreAnswerResponse:
        """
        Parse agent output for auto-grading (REQ-A-LangChain).

        Args:
            result: Agent output (supports both AgentExecutor and LangGraph formats)
            item_id: ë¬¸í•­ ID

        Returns:
            ScoreAnswerResponse

        ë¡œì§:
            1. _extract_tool_results()ë¡œ score_and_explain í˜¸ì¶œ ì¶”ì¶œ (intermediate_steps ë˜ëŠ” messages ëª¨ë‘ ì§€ì›)
            2. Tool ì¶œë ¥ì„ JSONìœ¼ë¡œ íŒŒì‹±
            3. is_correct, score, explanation, feedback, keyword_matches ì¶”ì¶œ
            4. ScoreAnswerResponseë¡œ ë³€í™˜

        ì°¸ê³ :
            - AgentExecutor ì¶œë ¥: {"output": "...", "intermediate_steps": [(tool_name, tool_output), ...]}
            - LangGraph ì¶œë ¥: {"messages": [...message objects...]}
            - Tool 6 (score_and_explain) ì¶œë ¥ êµ¬ì¡°:
              {
                "is_correct": bool,  # Tool 6 í˜¸í™˜
                "score": float (0-100),
                "explanation": str,
                "keyword_matches": list[str] (optional),  # Tool 6 í˜¸í™˜
                "feedback": str (optional)
              }

        """
        logger.info(f"ì±„ì  ê²°ê³¼ íŒŒì‹± ì¤‘... item_id={item_id}")

        try:
            # 1. score_and_explain ë„êµ¬ ê²°ê³¼ ì¶”ì¶œ (í¬ë§· ë¬´ê´€)
            tool_results = self._extract_tool_results(result, "score_and_explain")
            if not tool_results:
                logger.warning("score_and_explain ë„êµ¬ í˜¸ì¶œì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation="score_and_explain tool not executed",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 2. ì²« ë²ˆì§¸ score_and_explain ê²°ê³¼ ì‚¬ìš©
            _, score_tool_output = tool_results[0]
            if not score_tool_output:
                logger.warning("score_and_explain ë„êµ¬ ì¶œë ¥ì´ ë¹„ì–´ìˆìŒ")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation="score_and_explain output is empty",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 3. JSON íŒŒì‹±
            try:
                tool_output = json.loads(score_tool_output) if isinstance(score_tool_output, str) else score_tool_output
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(score_tool_output)[:100]}")
                return ScoreAnswerResponse(
                    item_id=item_id,
                    is_correct=False,
                    correct=False,
                    score=0.0,
                    explanation=f"JSON decode error: {str(e)}",
                    graded_at=datetime.now(UTC).isoformat(),
                )

            # 4. ScoreAnswerResponse ìƒì„± (Tool 6 í˜¸í™˜)
            # Tool 6ëŠ” "is_correct"ë¥¼ ë°˜í™˜í•˜ë©°, "keyword_matches"ë¡œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì œê³µ
            is_correct_from_tool = tool_output.get("is_correct", tool_output.get("correct", False))
            keyword_matches = tool_output.get("keyword_matches", tool_output.get("extracted_keywords", []))

            response = ScoreAnswerResponse(
                item_id=item_id,
                is_correct=is_correct_from_tool,  # Tool 6 í˜¸í™˜
                correct=is_correct_from_tool,  # API í˜¸í™˜ (ê°™ì€ ê°’)
                score=float(tool_output.get("score", 0)),
                explanation=tool_output.get("explanation", ""),
                feedback=tool_output.get("feedback"),
                keyword_matches=keyword_matches,  # Tool 6 í˜¸í™˜
                extracted_keywords=keyword_matches,  # API í˜¸í™˜ (ê°™ì€ ê°’)
                graded_at=tool_output.get("graded_at", datetime.now(UTC).isoformat()),
            )

            logger.info(f"âœ… ì±„ì  íŒŒì‹± ì™„ë£Œ: correct={response.correct}, score={response.score}")
            return response

        except Exception as e:
            logger.error(f"âŒ ì±„ì  íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return ScoreAnswerResponse(
                item_id=item_id,
                is_correct=False,
                correct=False,
                score=0.0,
                explanation=f"Parsing error: {str(e)}",
                graded_at=datetime.now(UTC).isoformat(),
            )


# ============================================================================
# Factory Function
# ============================================================================


async def create_agent() -> ItemGenAgent:
    """
    Create ItemGenAgent factory function.

    Returns:
        ItemGenAgent: ì´ˆê¸°í™”ëœ ì—ì´ì „íŠ¸

    ì‚¬ìš©:
        ```python
        agent = await create_agent()
        ```

    """
    return ItemGenAgent()
