"""Tests for Mode 1 Question Generation Pipeline (REQ-A-Mode1-Pipeline).

REQ: REQ-A-Mode1-Pipeline
Tests for generate_questions() orchestrator that coordinates Tools 1-5.
"""

from typing import Any
from unittest.mock import MagicMock, patch

import pytest


# ============================================================================
# Fixtures
# ============================================================================


@pytest.fixture
def valid_user_id() -> str:
    """Valid user ID."""
    return "user_550e8400_e29b_41d4_a716_446655440000"


@pytest.fixture
def valid_session_id() -> str:
    """Valid session ID."""
    return "sess_550e8400_e29b_41d4_a716_446655440000"


@pytest.fixture
def mock_user_profile() -> dict[str, Any]:
    """Mock user profile from Tool 1."""
    return {
        "user_id": "user_550e8400_e29b_41d4_a716_446655440000",
        "self_level": "intermediate",
        "years_experience": 3,
        "job_role": "AI Engineer",
        "duty": "LLM Research & Development",
        "interests": ["LLM", "RAG", "Prompt Engineering"],
        "previous_score": 75,
    }


@pytest.fixture
def mock_question_templates() -> list[dict[str, Any]]:
    """Mock question templates from Tool 2."""
    return [
        {
            "id": "tmpl_001",
            "stem": "What is RAG?",
            "type": "short_answer",
            "correct_answer": "Retrieval-Augmented Generation",
            "correct_rate": 0.85,
            "usage_count": 50,
            "avg_difficulty_score": 7.3,
        },
        {
            "id": "tmpl_002",
            "stem": "Explain prompt engineering",
            "type": "short_answer",
            "correct_answer": "Designing inputs for optimal LLM outputs",
            "correct_rate": 0.80,
            "usage_count": 45,
            "avg_difficulty_score": 6.5,
        },
    ]


@pytest.fixture
def mock_difficulty_keywords() -> dict[str, Any]:
    """Mock difficulty keywords from Tool 3."""
    return {
        "difficulty": 6,
        "category": "technical",
        "keywords": ["LLM", "Transformer", "Attention", "RAG", "Prompt"],
        "concepts": [
            {
                "name": "Large Language Model",
                "acronym": "LLM",
                "definition": "AI model trained on large text data",
                "key_points": ["Scale", "Generalization", "Few-shot"],
            }
        ],
        "example_questions": [
            {
                "stem": "What is an LLM?",
                "type": "short_answer",
                "difficulty_score": 6.0,
            }
        ],
    }


@pytest.fixture
def mock_generated_questions() -> list[dict[str, Any]]:
    """Mock questions generated by LLM."""
    return [
        {
            "stem": "What is the primary advantage of RAG?",
            "question_type": "multiple_choice",
            "choices": ["Faster training", "Better accuracy", "Lower cost", "Simpler code"],
            "correct_answer": "Better accuracy",
            "difficulty": 6,
            "category": "RAG",
        },
        {
            "stem": "Transformers use attention mechanisms.",
            "question_type": "true_false",
            "choices": ["True", "False"],
            "correct_answer": "True",
            "difficulty": 5,
            "category": "Transformer",
        },
        {
            "stem": "Explain the concept of prompt engineering.",
            "question_type": "short_answer",
            "choices": None,
            "correct_answer": None,
            "correct_keywords": ["prompt", "design", "LLM", "output"],
            "difficulty": 7,
            "category": "Prompt Engineering",
        },
        {
            "stem": "What does RAG stand for?",
            "question_type": "short_answer",
            "choices": None,
            "correct_answer": None,
            "correct_keywords": ["Retrieval", "Augmented", "Generation"],
            "difficulty": 4,
            "category": "RAG",
        },
        {
            "stem": "Which is a key component of Transformers?",
            "question_type": "multiple_choice",
            "choices": ["Convolution", "Attention", "Pooling", "Normalization"],
            "correct_answer": "Attention",
            "difficulty": 6,
            "category": "Transformer",
        },
    ]


@pytest.fixture
def mock_validation_results() -> list[dict[str, Any]]:
    """Mock validation results from Tool 4."""
    return [
        {
            "is_valid": True,
            "score": 0.92,
            "rule_score": 0.95,
            "final_score": 0.92,
            "recommendation": "pass",
            "feedback": "High quality question",
            "issues": [],
        },
        {
            "is_valid": True,
            "score": 0.88,
            "rule_score": 0.90,
            "final_score": 0.88,
            "recommendation": "pass",
            "feedback": "Good question",
            "issues": [],
        },
        {
            "is_valid": True,
            "score": 0.85,
            "rule_score": 0.87,
            "final_score": 0.85,
            "recommendation": "pass",
            "feedback": "Acceptable quality",
            "issues": [],
        },
        {
            "is_valid": True,
            "score": 0.82,
            "rule_score": 0.84,
            "final_score": 0.82,
            "recommendation": "pass",
            "feedback": "Meets minimum standards",
            "issues": [],
        },
        {
            "is_valid": True,
            "score": 0.80,
            "rule_score": 0.85,
            "final_score": 0.80,
            "recommendation": "pass",
            "feedback": "Acceptable",
            "issues": [],
        },
    ]


@pytest.fixture
def mock_save_results() -> list[dict[str, Any]]:
    """Mock save results from Tool 5."""
    results = []
    for i in range(5):
        results.append(
            {
                "question_id": f"q_{i:03d}",
                "round_id": "sess_550e8400_1_2025-11-09T10:30:00Z",
                "saved_at": "2025-11-09T10:30:00Z",
                "success": True,
            }
        )
    return results


# ============================================================================
# Tool Orchestration Tests
# ============================================================================


class TestToolOrchestration:
    """Tests for tool orchestration and sequencing."""

    def test_tool1_always_called(self, valid_user_id: str) -> None:
        """Test that Tool 1 (get_user_profile) is always called.

        REQ: REQ-A-Mode1-Pipeline, AC1

        Given: generate_questions() called
        When: Pipeline executes
        Then: Tool 1 called before other tools
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch("src.agent.pipeline.mode1_pipeline.get_user_profile") as mock_tool1:
            mock_tool1.return_value = {
                "user_id": valid_user_id,
                "interests": [],
                "self_level": "intermediate",
            }

            pipeline = Mode1Pipeline()
            pipeline._call_tool1(valid_user_id)

            mock_tool1.assert_called_once_with(valid_user_id)

    def test_tool2_conditional_with_interests(
        self, mock_user_profile: dict[str, Any], mock_question_templates: list[dict[str, Any]]
    ) -> None:
        """Test that Tool 2 is called only if user has interests.

        REQ: REQ-A-Mode1-Pipeline, AC2

        Given: User profile with interests
        When: Pipeline executes
        Then: Tool 2 called for template search
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch("src.agent.pipeline.mode1_pipeline.search_question_templates") as mock_tool2:
            mock_tool2.return_value = mock_question_templates

            pipeline = Mode1Pipeline()
            # Should call Tool 2 since interests exist
            result = pipeline._call_tool2(mock_user_profile["interests"], 6, "technical")

            assert result is not None
            assert len(result) > 0

    def test_tool2_skipped_without_interests(self) -> None:
        """Test that Tool 2 is skipped if user has no interests.

        REQ: REQ-A-Mode1-Pipeline, AC2

        Given: User profile without interests
        When: Pipeline executes
        Then: Tool 2 not called
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        pipeline = Mode1Pipeline()
        # Should return empty list without calling Tool 2
        result = pipeline._call_tool2([], 6, "technical")

        assert result == []

    def test_tool3_always_called(self) -> None:
        """Test that Tool 3 (get_difficulty_keywords) is always called.

        REQ: REQ-A-Mode1-Pipeline, AC1

        Given: generate_questions() called
        When: Pipeline executes
        Then: Tool 3 called for keyword retrieval
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch("src.agent.pipeline.mode1_pipeline.get_difficulty_keywords") as mock_tool3:
            mock_tool3.return_value = {
                "difficulty": 6,
                "keywords": ["LLM", "RAG"],
                "concepts": [],
            }

            pipeline = Mode1Pipeline()
            result = pipeline._call_tool3(6, "technical")

            assert result is not None
            mock_tool3.assert_called_once()


# ============================================================================
# Round ID Generation Tests
# ============================================================================


class TestRoundIDGeneration:
    """Tests for round ID generation and parsing."""

    def test_round_id_generated_format(self, valid_session_id: str) -> None:
        """Test that round_id is generated in correct format.

        REQ: REQ-A-Mode1-Pipeline, AC4

        Given: Session ID and round number
        When: round_id is generated
        Then: Format is "{session_id}_{round_number}_{timestamp}"
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        pipeline = Mode1Pipeline(session_id=valid_session_id)
        round_id = pipeline._generate_round_id(valid_session_id, 1)

        # Check format
        assert "_1_" in round_id  # Check round number is in middle
        assert valid_session_id in round_id  # Check session ID is included
        assert "T" in round_id  # ISO format has T

    def test_round_id_includes_timestamp(self, valid_session_id: str) -> None:
        """Test that round_id includes ISO timestamp.

        REQ: REQ-A-Mode1-Pipeline, AC4

        Given: Session ID and round number
        When: round_id is generated
        Then: Timestamp is ISO 8601 format
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        pipeline = Mode1Pipeline()
        round_id = pipeline._generate_round_id(valid_session_id, 2)

        # Timestamp should be ISO format
        assert "T" in round_id  # ISO format has T separator
        assert "Z" in round_id or "+" in round_id  # Timezone info


# ============================================================================
# Category Mapping Tests
# ============================================================================


class TestCategoryMapping:
    """Tests for category mapping (domain â†’ top-level)."""

    def test_technical_category_mapping(self) -> None:
        """Test mapping of technical domains.

        REQ: REQ-A-Mode1-Pipeline, AC3

        Given: Technical domain category
        When: get_top_category() called
        Then: Returns "technical"
        """
        from src.agent.pipeline.mode1_pipeline import get_top_category

        assert get_top_category("LLM") == "technical"
        assert get_top_category("RAG") == "technical"
        assert get_top_category("Prompt Engineering") == "technical"

    def test_business_category_mapping(self) -> None:
        """Test mapping of business domains.

        REQ: REQ-A-Mode1-Pipeline, AC3

        Given: Business domain category
        When: get_top_category() called
        Then: Returns "business"
        """
        from src.agent.pipeline.mode1_pipeline import get_top_category

        assert get_top_category("Product Strategy") == "business"
        assert get_top_category("Team Management") == "business"

    def test_general_category_mapping(self) -> None:
        """Test mapping of general domains.

        REQ: REQ-A-Mode1-Pipeline, AC3

        Given: Unknown domain or default
        When: get_top_category() called
        Then: Returns "general"
        """
        from src.agent.pipeline.mode1_pipeline import get_top_category

        assert get_top_category("Unknown Domain") == "general"
        assert get_top_category("") == "general"


# ============================================================================
# Error Handling Tests
# ============================================================================


class TestErrorHandling:
    """Tests for error handling and graceful degradation."""

    def test_tool1_failure_retry_then_default(self, valid_user_id: str) -> None:
        """Test that Tool 1 failure retries 3 times then uses default.

        REQ: REQ-A-Mode1-Pipeline, AC6

        Given: Tool 1 raises exception
        When: Pipeline executes
        Then: Retry 3x then return default profile
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch("src.agent.pipeline.mode1_pipeline.get_user_profile") as mock_tool1:
            mock_tool1.side_effect = Exception("DB connection error")

            pipeline = Mode1Pipeline()
            result = pipeline._call_tool1(valid_user_id)

            # Should have default values after retry
            assert result is not None
            assert "user_id" in result
            # Verify retry was attempted
            assert mock_tool1.call_count >= 3

    def test_tool2_no_results_skip_gracefully(self) -> None:
        """Test that Tool 2 no-result case is handled gracefully.

        REQ: REQ-A-Mode1-Pipeline, AC2

        Given: Tool 2 returns empty results
        When: Pipeline executes
        Then: Skips to Tool 3 without error
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch("src.agent.pipeline.mode1_pipeline.search_question_templates") as mock_tool2:
            mock_tool2.return_value = []

            pipeline = Mode1Pipeline()
            result = pipeline._call_tool2(["NonexistentDomain"], 5, "technical")

            assert result == []

    def test_tool3_failure_returns_cached(self) -> None:
        """Test that Tool 3 failure returns cached keywords.

        REQ: REQ-A-Mode1-Pipeline, AC6

        Given: Tool 3 raises exception
        When: Pipeline executes
        Then: Returns cached/default keywords
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch("src.agent.pipeline.mode1_pipeline.get_difficulty_keywords") as mock_tool3:
            mock_tool3.side_effect = Exception("Cache miss")

            pipeline = Mode1Pipeline()
            result = pipeline._call_tool3(5, "technical")

            # Should return something (cached or default)
            assert result is not None


# ============================================================================
# Batch Validation Tests
# ============================================================================


class TestBatchValidation:
    """Tests for batch validation (Tool 4)."""

    def test_batch_validation_pass_recommendations(
        self, mock_generated_questions: list[dict[str, Any]]
    ) -> None:
        """Test that pass recommendations are identified.

        REQ: REQ-A-Mode1-Pipeline, AC5

        Given: 5 generated questions with high validation scores
        When: Batch validation executes
        Then: Questions with score >= 0.85 marked as pass
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch("src.agent.pipeline.mode1_pipeline.validate_question_quality") as mock_tool4:
            # All pass
            mock_tool4.return_value = [
                {"is_valid": True, "final_score": 0.92, "recommendation": "pass"},
                {"is_valid": True, "final_score": 0.88, "recommendation": "pass"},
                {"is_valid": True, "final_score": 0.85, "recommendation": "pass"},
                {"is_valid": True, "final_score": 0.82, "recommendation": "pass"},
                {"is_valid": True, "final_score": 0.80, "recommendation": "pass"},
            ]

            pipeline = Mode1Pipeline()
            results = pipeline._call_tool4(mock_generated_questions)

            pass_count = sum(1 for r in results if r["recommendation"] == "pass")
            assert pass_count == 5

    def test_batch_validation_reject_recommendations(
        self, mock_generated_questions: list[dict[str, Any]]
    ) -> None:
        """Test that reject recommendations are identified.

        REQ: REQ-A-Mode1-Pipeline, AC5

        Given: Generated questions with low validation scores
        When: Batch validation executes
        Then: Questions with score < 0.70 marked as reject
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch("src.agent.pipeline.mode1_pipeline.validate_question_quality") as mock_tool4:
            mock_tool4.return_value = [
                {"is_valid": False, "final_score": 0.65, "recommendation": "reject"},
                {"is_valid": True, "final_score": 0.92, "recommendation": "pass"},
            ]

            pipeline = Mode1Pipeline()
            results = pipeline._call_tool4(mock_generated_questions[:2])

            reject_count = sum(1 for r in results if r["recommendation"] == "reject")
            assert reject_count == 1


# ============================================================================
# Complete Pipeline Tests
# ============================================================================


class TestCompletePipeline:
    """Tests for complete pipeline execution."""

    def test_generate_questions_happy_path(
        self,
        valid_user_id: str,
        valid_session_id: str,
        mock_user_profile: dict[str, Any],
        mock_question_templates: list[dict[str, Any]],
        mock_difficulty_keywords: dict[str, Any],
        mock_generated_questions: list[dict[str, Any]],
        mock_validation_results: list[dict[str, Any]],
        mock_save_results: list[dict[str, Any]],
    ) -> None:
        """Test complete pipeline execution happy path.

        REQ: REQ-A-Mode1-Pipeline, AC1-AC7

        Given: Valid user_id, round_number=1
        When: generate_questions() executed
        Then: Returns success with 5 questions
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch(
            "src.agent.pipeline.mode1_pipeline.get_user_profile"
        ) as mock_tool1, patch(
            "src.agent.pipeline.mode1_pipeline.search_question_templates"
        ) as mock_tool2, patch(
            "src.agent.pipeline.mode1_pipeline.get_difficulty_keywords"
        ) as mock_tool3, patch.object(
            Mode1Pipeline, "_generate_questions_llm"
        ) as mock_llm, patch(
            "src.agent.pipeline.mode1_pipeline.validate_question_quality"
        ) as mock_tool4, patch(
            "src.agent.pipeline.mode1_pipeline.save_generated_question"
        ) as mock_tool5:

            # Setup mocks
            mock_tool1.return_value = mock_user_profile
            mock_tool2.return_value = mock_question_templates
            mock_tool3.return_value = mock_difficulty_keywords
            mock_llm.return_value = mock_generated_questions
            mock_tool4.return_value = mock_validation_results
            mock_tool5.side_effect = mock_save_results

            pipeline = Mode1Pipeline(session_id=valid_session_id)
            result = pipeline.generate_questions(valid_user_id, round_number=1, count=5)

            # Verify result structure
            assert result["status"] == "success"
            assert result["generated_count"] == 5
            assert len(result["questions"]) == 5

            # Verify all tools called
            mock_tool1.assert_called_once()
            mock_tool2.assert_called_once()
            mock_tool3.assert_called_once()
            mock_llm.assert_called_once()
            mock_tool4.assert_called_once()
            assert mock_tool5.call_count == 5

    def test_generate_questions_response_structure(
        self,
        valid_user_id: str,
        valid_session_id: str,
        mock_user_profile: dict[str, Any],
        mock_difficulty_keywords: dict[str, Any],
        mock_generated_questions: list[dict[str, Any]],
        mock_validation_results: list[dict[str, Any]],
        mock_save_results: list[dict[str, Any]],
    ) -> None:
        """Test response structure of generate_questions.

        REQ: REQ-A-Mode1-Pipeline, AC7

        Given: Complete pipeline execution
        When: generate_questions() returns
        Then: Response has status, generated_count, questions array
        """
        from src.agent.pipeline.mode1_pipeline import Mode1Pipeline

        with patch(
            "src.agent.pipeline.mode1_pipeline.get_user_profile"
        ) as mock_tool1, patch(
            "src.agent.pipeline.mode1_pipeline.get_difficulty_keywords"
        ) as mock_tool3, patch.object(
            Mode1Pipeline, "_generate_questions_llm"
        ) as mock_llm, patch(
            "src.agent.pipeline.mode1_pipeline.validate_question_quality"
        ) as mock_tool4, patch(
            "src.agent.pipeline.mode1_pipeline.save_generated_question"
        ) as mock_tool5:

            mock_tool1.return_value = mock_user_profile
            mock_tool3.return_value = mock_difficulty_keywords
            mock_llm.return_value = mock_generated_questions
            mock_tool4.return_value = mock_validation_results
            mock_tool5.side_effect = mock_save_results

            pipeline = Mode1Pipeline(session_id=valid_session_id)
            result = pipeline.generate_questions(valid_user_id, round_number=1)

            # Check required fields
            assert "status" in result
            assert "generated_count" in result
            assert "questions" in result

            # Check field types
            assert isinstance(result["status"], str)
            assert isinstance(result["generated_count"], int)
            assert isinstance(result["questions"], list)

            # Check question structure
            for question in result["questions"]:
                assert "question_id" in question
                assert "stem" in question
                assert "type" in question
                assert "difficulty" in question
                assert "category" in question
                assert "validation_score" in question
