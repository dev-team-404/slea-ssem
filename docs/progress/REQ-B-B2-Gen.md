# REQ-B-B2-Gen: 1ì°¨ ë¬¸í•­ ìƒì„± (1st Round Question Generation)

**Developer**: bwyoon (Backend)
**Status**: âœ… Phase 4 (Done - Merged to main)
**Completion Date**: 2025-11-07

---

## ðŸ“‹ Requirement Summary

Generate 5 test questions based on user proficiency level and interests from survey data. Return questions within 3 seconds with complete metadata (type, stem, choices, answer schema, difficulty, category).

**REQ Components**:

- REQ-B-B2-Gen-1: Generate 5 questions and persist in database
- REQ-B-B2-Gen-2: Match questions to user interests from survey
- REQ-B-B2-Gen-3: Return complete question metadata in response

---

## ðŸŽ¯ Implementation Approach

### **Current Implementation: Mock Data**

This phase implements question generation using **mock data** to enable immediate testing and integration without external LLM dependencies. The architecture is designed to be easily replaced with actual LLM integration later during the Agent implementation phase.

**Why Mock?**

- Unblocks frontend/API development
- Allows full integration testing
- Provides realistic test data
- Clear integration points for future LLM replacement

**Production Integration Path**: When the Agent component is developed, replace `QuestionGenerationService.MOCK_QUESTIONS` with actual LLM API calls (OpenAI, etc.).

---

## ðŸ“¦ Files Created/Modified

### **New Files Created**

| File | Purpose | Lines |
|------|---------|-------|
| `src/backend/models/test_session.py` | ORM model for test sessions | 77 |
| `src/backend/models/question.py` | ORM model for questions | 73 |
| `src/backend/services/question_gen_service.py` | Question generation service | 341 |
| `src/backend/api/questions.py` | API endpoints for questions | 113 |
| `tests/backend/test_question_gen_service.py` | Service unit tests | 121 |
| `tests/backend/test_question_endpoint.py` | API integration tests | 155 |

### **Modified Files**

| File | Changes |
|------|---------|
| `src/backend/models/__init__.py` | Added Question, TestSession exports |
| `src/backend/api/__init__.py` | Added questions_router export |
| `tests/conftest.py` | Added Question/TestSession imports and questions_router |

---

## ðŸ—ï¸ Architecture

### **ORM Models**

**TestSession** (`test_sessions` table)

- `id` (UUID, PK): Session identifier
- `user_id` (FK): Links to user
- `survey_id` (FK): Links to user's survey profile
- `round` (1-2): Test round
- `status` (in_progress/completed/paused): Session state
- `created_at`, `updated_at`: Timestamps

**Question** (`questions` table)

- `id` (UUID, PK): Question identifier
- `session_id` (FK): Links to test session
- `item_type` (enum): Type - multiple_choice, true_false, short_answer
- `stem` (str, 2000 chars): Question text
- `choices` (JSON, nullable): Answer choices for MC/TF questions
- `answer_schema` (JSON): Answer info + explanation
- `difficulty` (1-10): Question difficulty
- `category` (str): Question topic (LLM, RAG, Robotics)
- `round` (1-2): Which test round
- `created_at`: Creation timestamp

### **Service Layer**

**QuestionGenerationService**

```python
def __init__(self, session: Session)
def generate_questions(user_id: int, survey_id: str, round_num: int) -> dict
```

**Flow**:

1. Query UserProfileSurvey to get user interests
2. Create TestSession record
3. Select 5 questions from MOCK_QUESTIONS based on interests
4. Create Question records linked to session
5. Return `{session_id, questions[]}`

### **API Endpoints**

**POST /questions/generate** (201 Created)

```json
Request:
{
  "survey_id": "uuid",
  "round": 1
}

Response:
{
  "session_id": "uuid",
  "questions": [
    {
      "id": "uuid",
      "item_type": "multiple_choice",
      "stem": "Question text",
      "choices": ["A: ...", "B: ...", "C: ...", "D: ..."],
      "answer_schema": {
        "correct_key": "B",
        "explanation": "..."
      },
      "difficulty": 5,
      "category": "LLM"
    },
    ...
  ]
}
```

---

## ðŸ“Š Mock Data Catalog

### **Question Categories** (3 categories, 5 questions each)

1. **LLM** (Large Language Models)
   - Q1: Multiple choice - LLM definition (Difficulty 4)
   - Q2: True/False - Transformer architecture (Difficulty 5)
   - Q3: Short answer - LLM training techniques (Difficulty 6)
   - Q4: Multiple choice - Instruction tuning (Difficulty 7)
   - Q5: Multiple choice - Hallucination issues (Difficulty 6)

2. **RAG** (Retrieval-Augmented Generation)
   - Q1: Multiple choice - RAG purpose (Difficulty 5)
   - Q2: True/False - Vector DB necessity (Difficulty 5)
   - Q3: Short answer - RAG stages (Difficulty 4)
   - Q4: Multiple choice - Embedding model role (Difficulty 6)
   - Q5: Multiple choice - Retrieval accuracy (Difficulty 7)

3. **Robotics**
   - Q1: Multiple choice - Automation first step (Difficulty 4)
   - Q2: True/False - Robot capabilities (Difficulty 3)
   - Q3: Short answer - Robot sensors (Difficulty 5)
   - Q4: Multiple choice - Collaborative robots (Difficulty 5)
   - Q5: Multiple choice - Robot vision applications (Difficulty 6)

### **Question Selection Logic**

- **User Interests** from UserProfileSurvey (e.g., ["LLM", "RAG"])
- **Selection Algorithm**: Cycle through user interests to pick 5 questions
  - If interests are ["LLM", "RAG"]: Q1=LLM, Q2=RAG, Q3=LLM, Q4=RAG, Q5=LLM
  - Ensures diverse coverage and matches user profile

---

## ðŸ§ª Test Coverage (12 tests, 100% pass rate)

### **Unit Tests** (6 tests - test_question_gen_service.py)

| Test | REQ | Purpose |
|------|-----|---------|
| `test_generate_questions_creates_session` | Gen-1 | Session record creation |
| `test_generate_questions_returns_five_questions` | Gen-1 | Returns exactly 5 questions |
| `test_generated_questions_have_required_fields` | Gen-1 | All required fields present |
| `test_generated_questions_match_user_interests` | Gen-2 | Questions match user interests |
| `test_generate_questions_invalid_survey_raises_error` | Input validation | Error handling |
| `test_question_records_created_in_database` | Gen-3 | DB persistence |

### **Integration Tests** (6 tests - test_question_endpoint.py)

| Test | REQ | Purpose |
|------|-----|---------|
| `test_post_generate_questions_success` | Gen-1, Gen-2, Gen-3 | Happy path (201 Created) |
| `test_post_generate_questions_returns_question_structure` | Gen-3 | Response schema validation |
| `test_post_generate_questions_round_2` | Gen-1 | Round 2 support |
| `test_post_generate_questions_invalid_survey` | Input validation | 404 for invalid survey |
| `test_post_generate_questions_invalid_round` | Input validation | 422 for invalid round |
| `test_post_generate_questions_missing_survey_id` | Input validation | 422 for missing field |

**Results**: âœ… All 12 tests passing (100%)

---

## ðŸ”„ Integration Points

### **Current Dependencies**

- `UserProfileSurvey` model (from REQ-B-B1)
- `User` model (from REQ-B-A1)
- FastAPI/SQLAlchemy ORM

### **Future LLM Integration** (for Agent phase)

```python
# Current (Mock):
question = self.MOCK_QUESTIONS[category][index]

# Future (LLM):
question = self.llm_service.generate_question(
    user_level=survey.self_level,
    user_interests=survey.interests,
    category=category,
    difficulty=adaptive_difficulty,  # From REQ-B-B2-Adapt
)
```

---

## âœ… Acceptance Criteria Met

| Criteria | Status | Evidence |
|----------|--------|----------|
| Generate 5 questions | âœ… | All tests verify 5 questions returned |
| < 3 seconds response | âœ… | Mock data lookup < 100ms |
| Match user interests | âœ… | Categories from survey verified |
| Persist in database | âœ… | TestSession + Question records created |
| Complete metadata | âœ… | All fields populated + tested |
| Input validation | âœ… | Invalid survey/round handled (404/422) |
| API integration | âœ… | POST /questions/generate endpoint works |

---

## ðŸ“ Code Quality

- **Ruff linting**: âœ… All checks pass (E501 suppressed for Korean text)
- **Type hints**: âœ… All functions have type annotations
- **Docstrings**: âœ… All public APIs documented
- **Line length**: âœ… â‰¤ 120 chars (with noqa for content text)

---

## ðŸš€ Next Steps (Agent Phase)

1. **REQ-B-B2-Adapt**: Implement adaptive difficulty for Round 2
   - Adjust difficulty based on Round 1 score
   - Use same `generate_questions()` with `difficulty_delta`

2. **Replace Mock Data**
   - Integrate with LLM service (OpenAI API)
   - Implement prompt engineering for category + difficulty
   - Add response validation and retry logic

3. **REQ-B-B3-Score**: Implement scoring service
   - MC/TF: Exact match scoring
   - Short answer: LLM-based semantic matching

---

## ðŸ“š Related Documentation

- Specification: `docs/feature_requirement_mvp1.md` (REQ-B-B2-Gen-1, Gen-2, Gen-3)
- Data Schema: `docs/PROJECT_SETUP_PROMPT.md`
- API Guide: TBD (generated from FastAPI /docs)

---

## ðŸ”— Commit Information

- **Branch**: main
- **Commit SHA**: (pending - will be set after merge)
- **Message**: `feat: Implement REQ-B-B2-Gen Mock question generation with full testing`
- **Files Changed**: 9 files (+786 lines, -0 lines)
