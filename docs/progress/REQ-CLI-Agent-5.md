# REQ-CLI-Agent-5: agent tools ëª…ë ¹ (Tool 1-6 ë””ë²„ê¹… ì¸í„°í˜ì´ìŠ¤)

**Status**: Phase 3ï¸âƒ£ - IMPLEMENTATION âœ… COMPLETE
**Priority**: ğŸŸ¡ MEDIUM
**Dependencies**: [REQ-CLI-Agent-1] âœ… DONE, [REQ-A-Mode1-Tool1~5] âœ… DONE, [REQ-A-Mode2-Tool6] âœ… DONE
**Target Completion**: Phase 4 (Design phase)

---

## ğŸ“‹ Phase 1: SPECIFICATION

### 1.1 Requirement Summary

Implement `agent tools` command suite that:
- Provides direct CLI access to invoke individual Tools 1-6
- Enables debugging and testing of tool functionality
- Supports tool-specific parameters for each tool
- Returns formatted output (Rich tables/panels) for easy interpretation
- Includes comprehensive help for each tool

### 1.2 Feature Intent

Enable developers and testers to:
- Debug individual tool behavior in isolation
- Test tool parameters without running full pipeline
- Validate tool outputs during development
- Quickly verify tool functionality changes
- Support both Mode 1 (Tools 1-5) and Mode 2 (Tool 6) tools

### 1.3 Detailed Specification

#### Location & Implementation

**Files to Create/Modify**:
- âœ… Modify: `src/cli/actions/agent.py` (replace 6 placeholders: t1-t6 functions)
- âœ… New: Comprehensive test suite in `tests/cli/test_agent_tools.py`

#### Command Signatures

```bash
agent tools [SUBCOMMAND] [OPTIONS]

Subcommands:
  t1  Get User Profile
  t2  Search Question Templates
  t3  Get Difficulty Keywords
  t4  Validate Question Quality
  t5  Save Generated Question
  t6  Score & Generate Explanation
```

#### Tool 1: Get User Profile

**Command**:
```bash
agent tools t1 --user-id USER_ID [--help]
```

**Options**:
- `--user-id TEXT` - User ID (required)
- `--help` - Show help

**Input**:
- user_id: string (required)

**Success Output**:
```
ğŸ” Tool 1: Get User Profile
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

User: user_123
Skill Level: 7/10
Experience (Years): 5
Interests:
  - Python
  - Machine Learning
  - Data Science
Job Role: Software Engineer
Category Expertise:
  - Backend Development (8/10)
  - Data Analysis (6/10)
```

**Error Cases**:
- Missing --user-id
- User not found
- API/database error

---

#### Tool 2: Search Question Templates

**Command**:
```bash
agent tools t2 --interests TEXT --difficulty INT [--category TEXT] [--help]
```

**Options**:
- `--interests TEXT` - Comma-separated interests (required)
- `--difficulty INT` - Difficulty level 1-10 (required)
- `--category TEXT` - Question category (optional)
- `--help` - Show help

**Input**:
- interests: list[str] (required, comma-separated)
- difficulty: int (1-10, required)
- category: str (optional)

**Success Output**:
```
ğŸ“š Tool 2: Search Question Templates
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Templates Found: 5
Interests: Python, Data Science
Difficulty: 7 (Â±1.5 range: 5.5-8.5)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID      â”‚ Stem                  â”‚ Type       â”‚ Difficultyâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ tmpl_01 â”‚ What is list comprehension? â”‚ MC | 6.5 |
â”‚ tmpl_02 â”‚ Explain decorators    â”‚ SA | 7.0 |
â”‚ tmpl_03 â”‚ Python async/await    â”‚ MC | 7.5 |
â”‚ tmpl_04 â”‚ Describe generators   â”‚ SA | 6.8 |
â”‚ tmpl_05 â”‚ GIL in Python?        â”‚ MC | 7.2 |
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Error Cases**:
- Missing required parameters
- Invalid difficulty (not 1-10)
- No templates found
- API error

---

#### Tool 3: Get Difficulty Keywords

**Command**:
```bash
agent tools t3 --difficulty INT [--category TEXT] [--help]
```

**Options**:
- `--difficulty INT` - Difficulty level 1-10 (required)
- `--category TEXT` - Question category (optional)
- `--help` - Show help

**Input**:
- difficulty: int (1-10, required)
- category: str (optional)

**Success Output**:
```
ğŸ“Š Tool 3: Get Difficulty Keywords
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Difficulty: 7/10 (Intermediate-Advanced)
Cache Hit: Yes (TTL: 59m 45s remaining)

Keywords (24 total):
  â€¢ Asynchronous programming
  â€¢ Context managers
  â€¢ Decorators
  â€¢ Descriptors
  â€¢ Exception handling (advanced)
  â€¢ Generator expressions
  â€¢ Metaclasses
  â€¢ Module system
  â€¢ Multithreading
  â€¢ Type hints
  [... 14 more]

Categories:
  â€¢ Backend (12 keywords)
  â€¢ Data Science (8 keywords)
  â€¢ DevOps (4 keywords)
```

**Error Cases**:
- Missing difficulty
- Invalid difficulty (not 1-10)
- Cache error (fallback to defaults)

---

#### Tool 4: Validate Question Quality

**Command**:
```bash
agent tools t4 --question TEXT --type TYPE [--help]
```

**Options**:
- `--question TEXT` - Question stem (required)
- `--type TEXT` - Question type: multiple_choice, short_answer, true_false (required)
- `--help` - Show help

**Input**:
- question_stem: str (required)
- question_type: str (required, enum)

**Success Output** (Pass):
```
âœ… Tool 4: Validate Question Quality
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Question: "Explain the difference between list and tuple in Python"
Type: short_answer
Status: âœ… PASS

Validation Score: 0.92 / 1.0
  â€¢ Clarity: 0.95 âœ…
  â€¢ Specificity: 0.90 âœ…
  â€¢ Grammar: 0.98 âœ…
  â€¢ Feasibility: 0.88 âœ…

Decision: ACCEPT
Feedback: Well-structured question with clear intent
Estimated Time: 3-5 minutes
```

**Success Output** (Revise):
```
âš ï¸  Tool 4: Validate Question Quality
Status: REVISE (Score: 0.75 / 1.0)

Feedback:
  â€¢ Question is too open-ended
  â€¢ Add constraints for scope
  â€¢ Consider adding context

Suggested Revision:
  "Explain the key differences between list and tuple in Python,
   focusing on mutability and performance implications"
```

**Error Cases**:
- Missing required parameters
- Invalid question type
- Invalid question (empty/null)

---

#### Tool 5: Save Generated Question

**Command**:
```bash
agent tools t5 --stem TEXT --type TYPE --difficulty INT --categories TEXT --round-id TEXT [--help]
```

**Options**:
- `--stem TEXT` - Question stem (required)
- `--type TEXT` - Question type (required)
- `--difficulty INT` - Difficulty 1-10 (required)
- `--categories TEXT` - Comma-separated categories (required)
- `--round-id TEXT` - Round ID (required)
- `--help` - Show help

**Input**:
- stem: str (required)
- question_type: str (required, enum)
- difficulty: int (1-10, required)
- categories: list[str] (required, comma-separated)
- round_id: str (required)

**Success Output**:
```
ğŸ’¾ Tool 5: Save Generated Question
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… Question Saved Successfully

Item ID: item_abc123def456
Round ID: round_001_python
Status: SAVED

Question Details:
  â€¢ Type: multiple_choice
  â€¢ Difficulty: 7/10
  â€¢ Categories: [Python, Functions, Intermediate]
  â€¢ Stem: "Which keyword creates an anonymous function in Python?"

Database Record:
  â€¢ Saved at: 2025-11-12T14:30:45Z
  â€¢ Table: test_questions
  â€¢ Row ID: 12847
```

**Error Cases**:
- Missing required parameters
- Invalid difficulty
- Invalid question type
- Database write error
- Duplicate question

---

#### Tool 6: Score & Generate Explanation

**Command**:
```bash
agent tools t6 --question-id ID --question TEXT --answer-type TYPE \
             --user-answer TEXT --correct-answer TEXT [--help]
```

**Options**:
- `--question-id TEXT` - Question ID (required)
- `--question TEXT` - Question stem (required)
- `--answer-type TEXT` - Question type (required)
- `--user-answer TEXT` - User's answer (required)
- `--correct-answer TEXT` - Correct answer (required)
- `--help` - Show help

**Input**:
- item_id: str (question ID, required)
- question_stem: str (required)
- question_type: str (required, enum)
- user_answer: str (required)
- correct_answer: str (required)

**Success Output** (Correct):
```
ğŸ¯ Tool 6: Score & Generate Explanation
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Question: "What is a decorator in Python?"
User Answer: "A decorator is a function that modifies another function"
Score: âœ… 100/100 (CORRECT)

Explanation:
Excellent answer! You've correctly identified that decorators are functions
that modify or enhance other functions. This is a fundamental concept in
Python that enables cleaner, more readable code through function composition.

Key Points Matched:
  âœ… Functions that modify functions
  âœ… Core concept understanding
  âœ… Relevant example understanding

Confidence: 95% (Very High)
```

**Success Output** (Partial):
```
âš ï¸  Tool 6: Score & Generate Explanation
Status: PARTIAL (Score: 65/100)

Explanation:
Your answer captures the main idea but misses some important nuances.
Decorators not only modify functions but also preserve their metadata and
can be stacked/chained for multiple modifications.

Missing Points:
  â€¢ Metadata preservation (@wraps)
  â€¢ Decorator chaining
  â€¢ Use with classes

Confidence: 78% (High)
```

**Error Cases**:
- Missing required parameters
- Invalid question type
- LLM API error
- Timeout (15 seconds)

---

#### Validation & Requirements

1. **Parameter Validation**:
   - Each tool validates required parameters
   - Type checking (str, int, list)
   - Range validation (difficulty 1-10)
   - Enum validation (question types)

2. **Error Handling**:
   - User-friendly error messages
   - Suggestion for fixes
   - Graceful fallback for cached data
   - Proper exit codes

3. **Output Formatting**:
   - Rich panels and tables
   - Color-coded status (âœ… âŒ âš ï¸)
   - Indented hierarchical data
   - Emoji indicators

4. **Performance**:
   - Timeout per tool (2-15 seconds)
   - Cache hits (Tool 3 only)
   - Async execution where applicable

#### Dependencies

- **Internal**: ItemGenAgent, FastMCP Tool implementations
- **External**: LangChain, Rich, Google Gemini API

---

## ğŸ§ª Phase 2: TEST DESIGN

### 2.1 Test Execution Strategy

Create `tests/cli/test_agent_tools.py` with:
- Help and error handling tests (9 tests: 1 help + 8 error tests)
- Success scenario tests for each tool (6 tests)
- Mock tool responses and edge cases
- Output formatting validation

### 2.2 Test Cases

#### Tool 1 Tests (3 total: 1 help + 1 success + 1 error)

**TC-1**: `agent tools t1 --help`
- Verify help displays usage and options

**TC-2**: `agent tools t1 --user-id user_123` (success)
- Mock ItemGenAgent.get_user_profile()
- Verify output contains user data

**TC-3**: `agent tools t1` (missing --user-id)
- Verify error message and usage hint

---

#### Tool 2 Tests (3 total)

**TC-4**: `agent tools t2 --help`
- Verify help displays usage

**TC-5**: `agent tools t2 --interests Python,Data --difficulty 7` (success)
- Mock search_question_templates()
- Verify table output with templates

**TC-6**: `agent tools t2 --difficulty 15` (invalid difficulty)
- Verify error for out-of-range difficulty

---

#### Tool 3 Tests (3 total)

**TC-7**: `agent tools t3 --help`
- Verify help

**TC-8**: `agent tools t3 --difficulty 7` (success)
- Mock get_difficulty_keywords()
- Verify keywords list output

**TC-9**: `agent tools t3 --difficulty abc` (invalid type)
- Verify error for non-integer difficulty

---

#### Tool 4 Tests (3 total)

**TC-10**: `agent tools t4 --help`
- Verify help

**TC-11**: `agent tools t4 --question "What is X?" --type multiple_choice` (success)
- Mock validate_question_quality()
- Verify scoring display

**TC-12**: `agent tools t4 --question "What?" --type invalid_type` (invalid type)
- Verify error for invalid question type

---

#### Tool 5 Tests (3 total)

**TC-13**: `agent tools t5 --help`
- Verify help

**TC-14**: `agent tools t5 --stem "Q?" --type mc --difficulty 5 --categories Python --round-id r1` (success)
- Mock save_generated_question()
- Verify success message with item ID

**TC-15**: `agent tools t5 --stem "Q?" --difficulty 15` (invalid difficulty)
- Verify error

---

#### Tool 6 Tests (3 total)

**TC-16**: `agent tools t6 --help`
- Verify help

**TC-17**: `agent tools t6 --question-id q1 --question "Q?" --answer-type mc --user-answer "A" --correct-answer "A"` (success)
- Mock score_and_explain()
- Verify scoring and explanation

**TC-18**: `agent tools t6 --question-id q1 --question "Q?"` (missing answers)
- Verify error for missing parameters

---

#### Additional Tests (3 total)

**TC-19**: Main `agent tools --help`
- Verify tools menu displays all 6 tools

**TC-20**: `agent tools invalid_tool`
- Verify error for invalid subcommand

**TC-21**: Agent initialization failure
- Mock ItemGenAgent() to raise exception
- Verify error handling

---

### 2.3 Test Implementation Structure

**File**: `tests/cli/test_agent_tools.py` (~600 LOC)

**Test Classes**:
- `TestToolsHelp` (2 tests: main help + individual help)
- `TestTool1GetUserProfile` (3 tests)
- `TestTool2SearchTemplates` (3 tests)
- `TestTool3GetKeywords` (3 tests)
- `TestTool4ValidateQuality` (3 tests)
- `TestTool5SaveQuestion` (3 tests)
- `TestTool6ScoreAnswer` (3 tests)
- `TestToolsErrors` (2 tests: invalid subcommand, agent init failure)

**Mocking Strategy**:
- Mock `src.cli.actions.agent.ItemGenAgent` constructor
- Mock each tool method: get_user_profile(), search_question_templates(), etc.
- Return realistic mock responses for each tool
- Use AsyncMock for async methods

### 2.4 Acceptance Criteria (Phase 2) âœ… COMPLETE

- [x] 21 test cases designed with clear assertions
- [x] Mock strategy defined for all tools
- [x] Test file location and class structure identified
- [x] Fixture setup documented
- [x] **All 21 tests passing** (2.48s execution)

**Phase 2 Status**: âœ… COMPLETE

Test Classes:
- TestToolsHelp (1 test)
- TestTool1GetUserProfile (3 tests)
- TestTool2SearchTemplates (3 tests)
- TestTool3GetKeywords (3 tests)
- TestTool4ValidateQuality (3 tests)
- TestTool5SaveQuestion (3 tests)
- TestTool6ScoreAnswer (3 tests)
- TestToolsErrors (2 tests)

Total: 21 tests (100% passing)

---

## ğŸ’» Phase 3: IMPLEMENTATION âœ… COMPLETE

### 3.1 Implementation Summary

Implemented all 6 tool functions with complete functionality:

1. **src/cli/actions/agent.py**:
   - Replaced `t1_get_user_profile()` (52 lines) with full implementation
   - Replaced `t2_search_question_templates()` (58 lines) with full implementation
   - Replaced `t3_get_difficulty_keywords()` (56 lines) with full implementation
   - Replaced `t4_validate_question_quality()` (52 lines) with full implementation
   - Replaced `t5_save_generated_question()` (68 lines) with full implementation
   - Replaced `t6_score_and_explain()` (72 lines) with full implementation
   - Added 6 help functions (_print_t1_help through _print_t6_help, ~52 lines total)

2. **tests/cli/test_agent_tools.py** âœ… ALL 21 TESTS PASSING:
   - TestToolsHelp (1 test)
   - TestTool1GetUserProfile (3 tests)
   - TestTool2SearchTemplates (3 tests)
   - TestTool3GetKeywords (3 tests)
   - TestTool4ValidateQuality (3 tests)
   - TestTool5SaveQuestion (3 tests)
   - TestTool6ScoreAnswer (3 tests)
   - TestToolsErrors (2 tests)

### 3.2 Key Implementation Details

**Argument Parsing Pattern**:
```python
i = 0
while i < len(args):
    arg = args[i]
    if arg == "--option" and i + 1 < len(args):
        value = args[i + 1]
        i += 2
    elif arg == "--help":
        _print_help(context)
        return
    else:
        i += 1
```

**Parameter Validation**:
- Type checking (str, int conversions)
- Range validation (difficulty 1-10)
- Enum validation (question types)
- Required field checking with helpful error messages

**Agent Initialization**:
- Try-except wrapping for graceful error handling
- asyncio.run() for executing async agent methods from sync context
- Proper error messaging for missing GEMINI_API_KEY

**Output Formatting**:
- Rich console panels with emoji headers (ğŸ“š ğŸ” âœ… ğŸ’¾ ğŸ¯)
- Divider lines (â”â”â”) for visual separation
- Hasattr() checks for optional response fields
- Color-coded error messages ([bold red]âŒ Error:[/bold red])

**Test Assertions**:
- Uses strip_ansi() helper to remove ANSI color codes from output
- Validates actual implementation output instead of placeholder messages
- Checks for expected tool names, titles, and error messages

### 3.3 Code Quality

**Docstrings**: All functions properly documented per PEP 257
**Type Hints**: All parameters and return types annotated
**Error Handling**: Comprehensive error cases with user-friendly messages
**Testing**: 100% test passing rate (21/21 tests)

---

## ğŸ“Š Implementation Roadmap

| Phase | Task | Status |
|-------|------|--------|
| 1 | Specification âœ“ | âœ… Complete |
| 2 | Test Design âœ“ | âœ… Complete (21/21 passing) |
| 3 | Implementation âœ“ | âœ… Complete (21/21 tests passing) |
| 4 | Summary & Commit | **In Progress** |

---

## âœ… Phase 1-3 Complete

- Phase 1: âœ“ Comprehensive specification for all 6 tools with parameters and outputs
- Phase 2: âœ“ 21 detailed test cases with full mocking strategy - **ALL 21 TESTS PASSING** âœ…
- Phase 3: âœ“ Full implementation of all 6 tool functions - **ALL 21 TESTS PASSING** âœ…

**Ready for Phase 4 (Summary & Commit).**
