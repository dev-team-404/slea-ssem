â— ì¢‹ìŠµë‹ˆë‹¤. ì¶©ë¶„í•œ ì½”ë“œë¥¼ ì½ì—ˆìœ¼ë‹ˆ ì´ì œ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤. Langchain ê³µì‹ ì˜ˆì œì™€ ë¹„êµí•˜ë©´ì„œ ReAct êµ¬ì¡°,
  Prompt, JSON parsing, í€´ì¦ˆ ìƒì„± í’ˆì§ˆì„ ì¤‘ì‹¬ìœ¼ë¡œ ê²€í† í•˜ê² ìŠµë‹ˆë‹¤.

  ğŸ“Š ì¢…í•© ì½”ë“œ ë¦¬ë·° ê²°ê³¼

  âœ… ë§¤ìš° ì˜ êµ¬í˜„ëœ ë¶€ë¶„ë“¤

  1. Prompt Architecture (SOLID ì›ì¹™ ì ìš©) â­â­â­â­â­

  # src/agent/prompts/ êµ¬ì¡° - íƒì›”í•œ ì„¤ê³„
  prompt_content.py    # ìˆœìˆ˜ í…ìŠ¤íŠ¸ (no escaping!)
  prompt_builder.py    # Builder + Factory patterns
  react_prompt.py      # Simple API

  ì¥ì :
  - âœ… Contentì™€ template logic ì™„ë²½ ë¶„ë¦¬ â†’ JSON escaping ë¬¸ì œ ì™„ì „ í•´ê²°
  - âœ… SystemMessage(content=...) ì‚¬ìš© â†’ from_template() íšŒí”¼
  - âœ… ìœ ì§€ë³´ìˆ˜ì„± ìµœê³  (contentë§Œ ìˆ˜ì • ê°€ëŠ¥)
  - âœ… Langchain ê³µì‹ ê¶Œì¥ íŒ¨í„´ ì¤€ìˆ˜

  2. ReAct Format Enforcement (ê²¬ê³ í•¨) â­â­â­â­â­

  # src/agent/prompts/prompt_content.py:19-37
  REACT_FORMAT_RULES = """========== CRITICAL: MANDATORY ReAct Format Rules ==========

  âœ“ EVERY "Action:" MUST have a corresponding "Action Input:"
  âœ“ EVERY "Action Input:" MUST have a corresponding "Observation:"
  âœ“ NEVER output just "Action:" without "Action Input:"

  ì¥ì :
  - âœ… ëª…ì‹œì  format rules (Langchain ê³µì‹ë³´ë‹¤ ë” êµ¬ì²´ì )
  - âœ… _is_complete_react_response() ë©”ì„œë“œë¡œ ì™„ì„±ë„ ê²€ì¦ (llm_agent.py:385-418)
  - âœ… Temperature 0.3 ì„¤ì • (consistency í–¥ìƒ)

  3. JSON Parsing Robustness â­â­â­â­â­

  # src/agent/output_converter.py:67-190
  # 5-stage cleanup strategies
  1. No cleanup (raw parsing)
  2. Fix Python literals (True/False â†’ true/false)
  3. Fix trailing commas
  4. Fix escapes
  5. Remove control characters

  ì¥ì :
  - âœ… Multi-layer fallback strategy
  - âœ… ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±°
  - âœ… Escaped ë¬¸ì ì²˜ë¦¬
  - âœ… answer_schema ì •ê·œí™” (7ê°€ì§€ ì…ë ¥ í˜•ì‹ ì§€ì›)

  4. Error Handling Framework â­â­â­â­

  # src/agent/error_handler.py
  - Retry with exponential backoff
  - Tool-specific strategies (Tool 1-6 ê°ê°)
  - Circuit breaker pattern
  - Graceful degradation

  ì¥ì :
  - âœ… ê° Toolë³„ ë§ì¶¤í˜• ì „ëµ
  - âœ… Queue-based retry (Tool 5)
  - âœ… LLM timeout fallback (Tool 6)

  ---
  âš ï¸ ê°œì„  ê°€ëŠ¥í•œ ë¶€ë¶„ë“¤

  1. Prompt - Tool Selection ëª…í™•ì„± ë¶€ì¡± (ì¤‘ìš”ë„: ë†’ìŒ)

  í˜„ì¬ ë¬¸ì œ:
  # src/agent/prompts/prompt_content.py:67-80
  TOOL_SELECTION_STRATEGY = """1. Tool Selection Strategy:
     - For question generation (Mode 1):
       * Always call get_user_profile first
       * Call search_question_templates if interests are available
       * Always call get_difficulty_keywords
       * Call validate_question_quality for each generated question
       * Call save_generated_question if validation passes

  ë¬¸ì œì :
  - âŒ "Call validate_question_quality for each generated question" â†’ ëª¨í˜¸í•¨
  - âŒ Agentê°€ ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œ ë²ˆì— ìƒì„± í›„ ê²€ì¦í• ì§€, í•˜ë‚˜ì”© ìƒì„±í• ì§€ ë¶ˆëª…í™•
  - âŒ Langchain ê³µì‹ ì˜ˆì œëŠ” êµ¬ì²´ì  ì‹¤í–‰ ìˆœì„œ ëª…ì‹œ

  ê°œì„ ì•ˆ:
  TOOL_SELECTION_STRATEGY = """Tool Calling Sequence (STRICT ORDER):

  Mode 1 (Question Generation) - Follow this EXACT sequence:
  Step 1: Call get_user_profile(user_id) â†’ Get proficiency level
  Step 2: Call get_difficulty_keywords(difficulty, domain) â†’ Get topic keywords
  Step 3: [Optional] Call search_question_templates() if user has specific interests
  Step 4: Generate ONE question at a time using the context from Steps 1-3
  Step 5: Call validate_question_quality(stem, type, choices, correct_answer) â†’ MUST validate BEFORE saving
  Step 6: IF validation passes (final_score >= 0.70):
             Call save_generated_question(...)
          ELSE:
             Regenerate the question (max 2 retries)
  Step 7: Repeat Steps 4-6 until {question_count} questions are successfully saved

  CRITICAL RULES:
  âœ“ NEVER save a question without validating first
  âœ“ NEVER batch-validate multiple questions (validate ONE at a time)
  âœ“ ALWAYS check validation.final_score >= 0.70 before calling save_generated_question
  âœ“ If a question fails validation twice, skip it and generate a new one
  """

  ì´ìœ :
  - Langchain ê³µì‹ì€ "step-by-step explicit instructions"ì„ ê¶Œì¥
  - Agentê°€ Tool ìˆœì„œë¥¼ ì˜ëª» ì´í•´í•˜ë©´ validation ì—†ì´ ì €ì¥ ê°€ëŠ¥ì„±

  2. Prompt - Final Answer Format ë¶ˆëª…í™• (ì¤‘ìš”ë„: ë†’ìŒ)

  í˜„ì¬ ë¬¸ì œ:
  # src/agent/prompts/prompt_content.py:87-118
  RESPONSE_FORMAT_RULES = """3. Response Format - CRITICAL:
     - IMPORTANT: When returning Final Answer for Mode 1:
       * Use JSON array format with proper array brackets [...]
       * Return Tool 5 response directly in Final Answer

  ë¬¸ì œì :
  - âŒ "Return Tool 5 response directly" â†’ Agentê°€ Tool 5 ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•  ìˆ˜ë„ ìˆìŒ
  - âŒ JSON structure ì˜ˆì œ ë¶€ì¡± (Langchainì€ í•­ìƒ êµ¬ì²´ì  ì˜ˆì œ ì œê³µ)
  - âŒ answer_schemaê°€ stringì´ì–´ì•¼ í•œë‹¤ëŠ” ê·œì¹™ì´ ìˆì§€ë§Œ, ì‹¤ì œ Tool 5ëŠ” dictë¥¼ ë°˜í™˜ (line 310)

  ê°œì„ ì•ˆ:
  RESPONSE_FORMAT_RULES = """Final Answer Format (Mode 1 - Question Generation):

  Return a JSON array with ALL generated questions in this EXACT format:

  Final Answer: [
    {
      "question_id": "uuid-string",
      "type": "multiple_choice" | "true_false" | "short_answer",
      "stem": "Question text here",
      "choices": ["A", "B", "C", "D"] or null for short_answer,
      "answer_schema": {
        "type": "exact_match" | "keyword_match",
        "correct_answer": "B" (for MC/TF) or null,
        "keywords": ["keyword1", "keyword2"] (for SA) or null
      },
      "difficulty": 5,
      "category": "AI",
      "validation_score": 0.85
    },
    ...
  ]

  CRITICAL FORMATTING RULES:
  âœ“ answer_schema is an OBJECT (dict), NOT a string
  âœ“ choices must be an array for MC/TF, null for short_answer
  âœ“ Do NOT include "saved_at", "round_id" in Final Answer (internal metadata)
  âœ“ Only include successfully validated and saved questions
  âœ“ Ensure valid JSON syntax (no trailing commas, proper escaping)

  WRONG EXAMPLES:
  âŒ "answer_schema": "exact_match"  (string instead of object)
  âŒ "choices": "A,B,C,D"            (string instead of array)
  âŒ [..., }]                        (trailing comma before closing bracket)

  ì´ìœ :
  - Output parserê°€ ëª…í™•í•œ êµ¬ì¡°ë¥¼ ê¸°ëŒ€í•¨
  - answer_schema normalization ë¡œì§ì´ ë³µì¡í•´ì§€ëŠ” ì´ìœ ê°€ ì—¬ê¸°ì— ìˆìŒ

  3. Agent - Tool Result Extraction ë³µì¡ë„ (ì¤‘ìš”ë„: ì¤‘ê°„)

  í˜„ì¬ ë¬¸ì œ:
  # src/agent/llm_agent.py:420-547 (128ì¤„)
  def _extract_tool_results(self, result: dict, tool_name: str):
      # Format 1: intermediate_steps (test mockìš©)
      # Format 2: LangGraph messages (ì‹¤ì œ ìš´ì˜)
      # ë³µì¡í•œ message ìˆœíšŒ ë° tool_call_id ë§¤ì¹­

  ë¬¸ì œì :
  - âŒ 128ì¤„ì§œë¦¬ ë©”ì„œë“œ (ë„ˆë¬´ ê¸´ ë©”ì„œë“œ, SRP ìœ„ë°˜)
  - âŒ Format 1/2 ëª¨ë‘ ì§€ì› â†’ í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•œ legacy ì½”ë“œ
  - âŒ ë¡œê¹…ì´ ë§ì•„ ì‹¤ì œ ë¡œì§ íŒŒì•… ì–´ë ¤ì›€

  ê°œì„ ì•ˆ:
  def _extract_tool_results(self, result: dict, tool_name: str):
      """Extract tool results from agent output."""
      # Delegate to format-specific handlers
      if "intermediate_steps" in result:
          return self._extract_from_intermediate_steps(result, tool_name)
      elif "messages" in result:
          return self._extract_from_messages(result, tool_name)
      else:
          logger.warning("No tool results found in agent output")
          return []

  def _extract_from_intermediate_steps(self, result: dict, tool_name: str):
      """Extract from intermediate_steps (test mock format)."""
      # ê¸°ì¡´ Format 1 ë¡œì§ (ê°„ê²°í™”)
      ...

  def _extract_from_messages(self, result: dict, tool_name: str):
      """Extract from LangGraph messages (production format)."""
      # ê¸°ì¡´ Format 2 ë¡œì§ (ê°„ê²°í™”)
      # Step 1: Build tool_messages_by_id map
      # Step 2: Scan AIMessages for matching tool_calls
      ...

  ì´ìœ :
  - ë©”ì„œë“œ ë¶„ë¦¬ë¡œ ê°€ë…ì„± í–¥ìƒ
  - ê° formatë³„ ë¡œì§ì„ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
  - Langchain ê³µì‹: "Small, focused methods"

  4. LLM Config - Temperature ì „ëµ ê²€ì¦ í•„ìš” (ì¤‘ìš”ë„: ì¤‘ê°„)

  í˜„ì¬ ì„¤ì •:
  # src/agent/config.py:72-73
  temperature=0.3,  # ê²°ì •ì  ë„êµ¬ í˜¸ì¶œ (0.7 â†’ 0.3ìœ¼ë¡œ ê°ì†Œ)
  max_output_tokens=8192,

  ê²€í†  í•„ìš” ì‚¬í•­:
  - âœ… Temperature 0.3ì€ ReAct consistencyì— ì¢‹ìŒ (ê³µì‹ ê¶Œì¥ 0.0-0.3)
  - âš ï¸ ê·¸ëŸ¬ë‚˜ question generationì—ëŠ” ì°½ì˜ì„±ì´ í•„ìš”í•  ìˆ˜ë„ ìˆìŒ
  - âš ï¸ Langchain ì˜ˆì œëŠ” tool calling (0.0-0.3), generation (0.7-0.9)ë¥¼ ë¶„ë¦¬

  ì œì•ˆ:
  # config.pyì— modeë³„ temperature ì¶”ê°€
  def create_llm(mode: str = "default"):
      """
      Create LLM with mode-specific temperature.
      
      Args:
          mode: "tool_calling" (0.3) | "generation" (0.7)
      """
      provider = LLMFactory.get_provider()
      llm = provider.create()

      if mode == "generation":
          # For creative question generation
          llm.temperature = 0.7
      elif mode == "tool_calling":
          # For deterministic tool selection
          llm.temperature = 0.3

      return llm

  ë˜ëŠ” ë” ê°„ë‹¨í•˜ê²Œ:
  # Agentì—ì„œ ë‘ ê°œì˜ LLM ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
  self.llm_tool_calling = create_llm()  # temperature 0.3
  self.llm_generation = create_llm()    # temperature 0.7 (question generationìš©)

  ì´ìœ :
  - Tool callingì—ëŠ” ë‚®ì€ temperature í•„ìš” (consistency)
  - Question generationì—ëŠ” ë†’ì€ temperature í•„ìš” (diversity)
  - í˜„ì¬ëŠ” ëª¨ë“  ì‘ì—…ì— 0.3 ì‚¬ìš© â†’ ë¬¸í•­ ë‹¤ì–‘ì„± ì €í•˜ ê°€ëŠ¥ì„±

  5. Validation Tool - LLM Prompt ê°œì„  (ì¤‘ìš”ë„: ë‚®ìŒ)

  í˜„ì¬:
  # src/agent/tools/validate_question_tool.py:176-192
  prompt = f"""Evaluate the quality of this question on a scale of 0.0 to 1.0.

  Question Stem: {stem}
  ...
  Respond with ONLY a number between 0.0 and 1.0, like: 0.85

  Do not include any explanation, just the score."""

  ë¬¸ì œì :
  - âŒ "ONLY a number" â†’ LLMì´ ì„¤ëª…ì„ ì¶”ê°€í•  ê°€ëŠ¥ì„± (parsing ì‹¤íŒ¨)
  - âŒ Criteriaê°€ ë„ˆë¬´ í¬ê´„ì  (5ê°œ í•­ëª©ì„ í•˜ë‚˜ì˜ ì ìˆ˜ë¡œ ì••ì¶•)

  ê°œì„ ì•ˆ (Option 1 - Structured Output):
  from langchain_core.pydantic_v1 import BaseModel, Field

  class ValidationScore(BaseModel):
      """Structured validation score output."""
      clarity: float = Field(..., ge=0.0, le=1.0, description="Question clarity")
      appropriateness: float = Field(..., ge=0.0, le=1.0)
      correctness: float = Field(..., ge=0.0, le=1.0)
      bias_free: float = Field(..., ge=0.0, le=1.0)
      format_valid: float = Field(..., ge=0.0, le=1.0)

  def _call_llm_validation(stem, question_type, choices, correct_answer):
      llm = create_llm().with_structured_output(ValidationScore)
      result = llm.invoke(prompt)
      # Calculate weighted average
      score = (result.clarity * 0.3 + result.appropriateness * 0.2 +
               result.correctness * 0.3 + result.bias_free * 0.1 +
               result.format_valid * 0.1)
      return score

  ê°œì„ ì•ˆ (Option 2 - More Robust Parsing):
  prompt = f"""Score this question (0.0-1.0 scale).

  <question>
  Stem: {stem}
  Type: {question_type}
  Choices: {choices_str}
  Correct: {correct_answer}
  </question>

  Criteria:
  1. Clarity (30%): Unambiguous and easy to understand?
  2. Correctness (30%): Objectively verifiable answer?
  3. Appropriateness (20%): Suitable difficulty level?
  4. Bias (10%): Free from offensive language?
  5. Format (10%): Valid structure?

  Output format (STRICT):
  SCORE: 0.85
  REASON: <1-sentence justification>

  Example:
  SCORE: 0.92
  REASON: Clear question with objective answer and appropriate difficulty.
  """

  # Parsing
  match = re.search(r"SCORE:\s*(0\.\d+|1\.0)", response.content)
  if match:
      score = float(match.group(1))
  else:
      logger.warning("Could not parse SCORE, using default")
      score = DEFAULT_LLM_SCORE

  ì´ìœ :
  - Structured outputì€ parsing error ë°©ì§€ (Langchain ê¶Œì¥)
  - Multi-criteria scoringì€ íˆ¬ëª…ì„± í–¥ìƒ

  6. fastmcp_server.py - Mock Tool êµ¬í˜„ (ì¤‘ìš”ë„: ë‚®ìŒ)

  í˜„ì¬ ë¬¸ì œ:
  # src/agent/fastmcp_server.py
  # Tool 1-6 ëª¨ë‘ hard-coded mock data ë°˜í™˜
  @tool
  def get_user_profile(user_id: str):
      # ...
      profile = {
          "user_id": user_id,
          "self_level": "intermediate",  # â† Hard-coded
          "years_experience": 5,
          ...
      }

  ë¬¸ì œì :
  - âŒ ì‹¤ì œ DB/API ì—°ê²° ì—†ìŒ â†’ í”„ë¡œë•ì…˜ ë°°í¬ ë¶ˆê°€
  - âŒ Tool ë¬¸ì„œí™”ì— "REQ: REQ-A-FastMCP"ë¼ê³  ë˜ì–´ ìˆì§€ë§Œ ì‹¤ì œëŠ” mock

  ì œì•ˆ:
  # ì‹¤ì œ tool êµ¬í˜„ìœ¼ë¡œ êµì²´ (ì´ë¯¸ tools/ ë””ë ‰í† ë¦¬ì— ìˆìŒ)
  from src.agent.tools.user_profile_tool import get_user_profile as get_profile_impl
  from src.agent.tools.save_question_tool import save_generated_question as save_impl
  # ...

  TOOLS = [
      get_profile_impl,      # Real implementation
      search_templates_impl,
      get_keywords_impl,
      validate_impl,
      save_impl,
      score_impl,
  ]

  í™•ì¸ í•„ìš”:
  - src/agent/tools/ ë””ë ‰í† ë¦¬ì— ì‹¤ì œ êµ¬í˜„ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
  - ë§Œì•½ ìˆë‹¤ë©´ fastmcp_server.pyì˜ mockì„ ì œê±°í•˜ê³  ì‹¤ì œ toolë¡œ êµì²´

  ---
  ğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ì‘ì—…

  High Priority (ë°°í¬ ì „ í•„ìˆ˜)

  1. Prompt - Tool Selection Strategy ëª…í™•í™” (30ë¶„)
    - êµ¬ì²´ì  step-by-step ìˆœì„œ ì¶”ê°€
    - Validation í•„ìˆ˜ ì¡°ê±´ ê°•ì¡°
  2. Prompt - Final Answer Format ì˜ˆì œ ì¶”ê°€ (20ë¶„)
    - JSON structure ëª…í™•í™”
    - answer_schema object êµ¬ì¡° ëª…ì‹œ
  3. fastmcp_server.py Mock ì œê±° (1ì‹œê°„)
    - ì‹¤ì œ tool êµ¬í˜„ìœ¼ë¡œ êµì²´
    - í†µí•© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰

  Medium Priority (í’ˆì§ˆ í–¥ìƒ)

  4. _extract_tool_results() ë¦¬íŒ©í† ë§ (1ì‹œê°„)
    - Formatë³„ ë©”ì„œë“œ ë¶„ë¦¬
    - ë¡œê¹… ê°„ì†Œí™”
  5. Temperature ì „ëµ ê²€í†  (30ë¶„)
    - Question generationìš© LLM ë¶„ë¦¬ ê³ ë ¤
    - A/B í…ŒìŠ¤íŠ¸ë¡œ í’ˆì§ˆ ë¹„êµ

  Low Priority (ì„ íƒì‚¬í•­)

  6. Validation LLM Prompt ê°œì„  (1ì‹œê°„)
    - Structured output ì ìš©
    - Multi-criteria scoring

  ---
  ğŸ“ Langchain ê³µì‹ ì˜ˆì œì™€ì˜ ë¹„êµ

  | í•­ëª©               | í˜„ì¬ êµ¬í˜„                             | Langchain ê³µì‹                  | í‰ê°€
      |
  |------------------|-----------------------------------|-------------------------------|-------------------|
  | ReAct Agent      | create_react_agent (LangGraph v2) | âœ… ë™ì¼                          | â­â­â­â­â­
    |
  | Prompt Structure | SOLID íŒ¨í„´ (content ë¶„ë¦¬)             | ê¶Œì¥í•˜ì§€ë§Œ ì˜ˆì œ ì—†ìŒ                   | â­â­â­â­â­
  (ë” ì¢‹ìŒ)      |
  | Tool Selection   | ëª…ì‹œì  strategy (ê°œì„  í•„ìš”)              | Step-by-step examples         | â­â­â­ (ê°œì„ 
  í•„ìš”)       |
  | Error Handling   | Circuit breaker, retry ë“±          | ê¸°ë³¸ ì˜ˆì œë§Œ ì œê³µ                     | â­â­â­â­â­ (ë”
  ì¢‹ìŒ)      |
  | JSON Parsing     | Multi-layer fallback              | ê¸°ë³¸ json.loads()               | â­â­â­â­â­ (ë” ì¢‹ìŒ)
      |
  | Temperature      | 0.3 (ê³ ì •)                          | 0.0-0.3 (tool), 0.7-0.9 (gen) | â­â­â­ (modeë³„ ë¶„ë¦¬
  ê¶Œì¥) |

  ---
  âœ… ìµœì¢… í‰ê°€

  ì „ì²´ì ìœ¼ë¡œ ë§¤ìš° ìš°ìˆ˜í•œ êµ¬í˜„ì…ë‹ˆë‹¤. íŠ¹íˆ:
  - âœ… SOLID ì›ì¹™ ì ìš© (Prompt êµ¬ì¡°)
  - âœ… Robust JSON parsing
  - âœ… í¬ê´„ì  error handling
  - âœ… ìµœì‹  Langchain API ì‚¬ìš©

  í•µì‹¬ ê°œì„ ì‚¬í•­:
  1. Promptì— êµ¬ì²´ì  tool calling sequence ì¶”ê°€
  2. Final Answer format ëª…í™•í™”
  3. Mock toolì„ ì‹¤ì œ êµ¬í˜„ìœ¼ë¡œ êµì²´
  4. (Optional) Temperature ì „ëµ ê²€í† 

  ì´ ê°œì„ ì‚¬í•­ë“¤ì„ ì ìš©í•˜ë©´ í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ê°€ ì™„ë£Œë©ë‹ˆë‹¤.